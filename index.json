[{"categories":["Mythology"],"content":"1 WHAT IS REALITY? WHAT IS MAGIC? REALITY IS EVERYTHING that exists. That sounds straightforward, doesn’t it? Actually, it isn’t. There are various problems. What about dinosaurs, which once existed but exist no longer? What about stars, which are so far away that, by the time their light reaches us and we can see them, they may have fizzled out? We’ll come to dinosaurs and stars in a moment. But in any case, how do we know things exist, even in the present? Well, our five senses – sight, smell, touch, hearing and taste – do a pretty good job of convincing us that many things are real: rocks and camels, newly mown grass and freshly ground coffee, sandpaper and velvet, waterfalls and doorbells, sugar and salt. But are we only going to call something ‘real’ if we can detect it directly with one of our five senses? What about a distant galaxy, too far away to be seen with the naked eye? What about a bacterium, too small to be seen without a powerful microscope? Must we say that these do not exist because we can’t see them? No. Obviously we can enhance our senses through the use of special instruments: telescopes for the galaxy, microscopes for bacteria. Because we understand telescopes and microscopes, and how they work, we can use them to extend the reach of our senses – in this case, the sense of sight – and what they enable us to see convinces us that galaxies and bacteria exist. How about radio waves? Do they exist? Our eyes can’t detect them, nor can our ears, but again special instruments – television sets, for example – convert them into signals that we can see and hear. So, although we can’t see or hear radio waves, we know they are a part of reality. As with telescopes and microscopes, we understand how radios and televisions work. So they help our senses to build a picture of what exists: the real world – reality. Radio telescopes (and X-ray telescopes) show us stars and galaxies through what seem like different eyes: another way to expand our view of reality. Back to those dinosaurs. How do we know that they once roamed the Earth? We have never seen them or heard them or had to run away from them. Alas, we don’t have a time machine to show them to us directly. But here we have a different kind of aid to our senses: we have fossils, and we can see them with the naked eye. Fossils don’t run and jump but, because we understand how fossils are formed, they can tell us something of what happened millions of years ago. We understand how water, with minerals dissolved in it, seeps into corpses buried in layers of mud and rock. We understand how the minerals crystallize out of the water and replace the materials of the corpse, atom by atom, leaving some trace of the original animal’s form imprinted on the stone. So, although we can’t see dinosaurs directly with our senses, we can work out that they must have existed, using indirect evidence that still ultimately reaches us through our senses: we see and touch the stony traces of ancient life. In a different sense, a telescope can work like a kind of time machine. What we see when we look at anything is actually light, and light takes time to travel. Even when you look at a friend’s face you are seeing them in the past, because the light from their face takes a tiny fraction of a second to travel to your eye. Sound travels much more slowly, which is why you see a firework burst in the sky noticeably earlier than you hear the bang. When you watch a man chopping down a tree in the distance, there is an odd delay in the sound of his axe hitting the tree. Light travels so fast that we normally assume anything we see happens at the instant we see it. But stars are another matter. Even the sun is eight light-minutes away. If the sun blew up, this catastrophic event wouldn’t become a part of our reality until eight minutes later. And that would be the end of us! As for the next nearest star, Proxima Centauri, if you look at it in 2012, what you are seeing is happening in 2008. Galaxie","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:1:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Models: testing our imagination There is a less familiar way in which a scientist can work out what is real when our five senses cannot detect it directly. This is through the use of a ‘model’ of what might be going on, which can then be tested. We imagine – you might say we guess – what might be there. That is called the model. We then work out (often by doing a mathematical calculation) what we ought to see, or hear, etc. (often with the help of measuring instruments) if the model were true. We then check whether that is what we actually do see. The model might literally be a replica made out of wood or plastic, or it might be a piece of mathematics on paper, or it might be a simulation in a computer. We look carefully at the model and predict what we ought to see or hear, etc. if the model were correct. Then we look to see whether the predictions are right or wrong. If they are right, this increases our confidence that the model really does represent reality; we then go on to devise further experiments, perhaps refining the model, to test the findings further and confirm them. If our predictions are wrong, we reject the model, or modify it and try again. Here’s an example. Nowadays, we know that genes – the units of heredity – are made of stuff called DNA. We know a great deal about DNA and how it works. But you can’t see the details of what DNA looks like, even with a powerful microscope. Almost everything we know about DNA comes indirectly from dreaming up models and then testing them. Actually, long before anyone had even heard of DNA, scientists already knew lots about genes from testing the predictions of models. Back in the nineteenth century, an Austrian monk called Gregor Mendel did experiments in his monastery garden, breeding peas in large quantities. He counted the numbers of plants that had flowers of various colours, or that had peas that were wrinkly or smooth, as the generations went by. Mendel never saw or touched a gene. All he saw were peas and flowers, and he could use his eyes to count different types. He invented a model, which involved what we would now call genes (though Mendel didn’t call them that), and he calculated that, if his model were correct, in a particular breeding experiment there ought to be three times as many smooth peas as wrinkly ones. And that is what he found when he counted them. Leaving aside the details, the point is that Mendel’s ‘genes’ were an invention of his imagination: he couldn’t see them with his eyes, not even with a microscope. But he could see smooth and wrinkled peas, and by counting them he found indirect evidence that his model of heredity was a good representation of something in the real world. Later scientists used a modification of Mendel’s method, working with other living things such as fruit flies instead of peas, to show that genes are strung out in a definite order, along threads called chromosomes (we humans have forty-six chromosomes, fruit flies have eight). It was even possible to work out, by testing models, the exact order in which genes were arranged along chromosomes. All this was done long before we knew that genes were made of DNA. Nowadays we know this, and we know exactly how DNA works, thanks to James Watson and Francis Crick, plus a lot of other scientists who came after them. Watson and Crick could not see DNA with their own eyes. Once again, they made their discoveries by imagining models and testing them. In their case, they literally built metal and cardboard models of what DNA might look like, and they calculated what certain measurements ought to be if those models were correct. The predictions of one model, the so-called double helix model, exactly fitted the measurements made by Rosalind Franklin and Maurice Wilkins, using special instruments involving X-rays beamed into crystals of purified DNA. Watson and Crick also immediately realized that their model of the structure of DNA would produce exactly the kind of results seen by Gregor","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:1:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Science and the supernatural: explanation and its enemy So that is reality, and that is how we can know whether something is real or not. Each chapter of this book is going to be about one particular aspect of reality – the sun, for instance, or earthquakes, or rainbows, or the many different kinds of animals. I want now to turn to the other key word of my title: magic. Magic is a slippery word: it is commonly used in three different ways, and the first thing I must do is distinguish between them. I’ll call the first one ‘supernatural magic’, the second one ‘stage magic’ and the third one (which is my favourite meaning, and the one I intend in my title) ‘poetic magic’. Supernatural magic is the kind of magic we find in myths and fairy tales. (In ‘miracles’, too, though I shall leave those to one side for now and return to them in the final chapter.) It’s the magic of Aladdin’s lamp, of wizards’ spells, of the Brothers Grimm, of Hans Christian Andersen and of J. K. Rowling. It’s the fictional magic of a witch casting a spell and turning a prince into a frog, or a fairy godmother changing a pumpkin into a gleaming coach. These are the stories we all remember with fondness from our childhood, and many of us still enjoy when served up in a traditional Christmas pantomime – but we all know this kind of magic is just fiction and does not happen in reality. Stage magic, by contrast, really does happen, and it can be great fun. Or at least, something really happens, though it isn’t what the audience thinks it is. A man on a stage (it usually is a man, for some reason) deceives us into thinking that something astonishing has happened (it may even seem supernatural) when what really happened was something quite different. Silk handkerchiefs cannot turn into rabbits, any more than frogs can turn into princes. What we have seen on the stage is only a trick. Our eyes have deceived us – or rather, the conjuror has gone to great pains to deceive our eyes, perhaps by cleverly using words to distract us from what he is really doing with his hands. Some conjurors are honest and go out of their way to make sure their audiences know that they have simply performed a trick. I am thinking of people like James ‘The Amazing’ Randi, or Penn and Teller, or Derren Brown. Even though these admirable performers don’t usually tell the audience exactly how they did the trick – they could be thrown out of the Magic Circle (the conjurors’ club) if they did that – they do make sure the audience knows that there was no supernatural magic involved. Others don’t actively spell out that it was just a trick, but they don’t make exaggerated claims about what they have done either – they just leave the audience with the rather enjoyable sensation that something mysterious has happened, without actively lying about it. But unfortunately there are some conjurors who are deliberately dishonest, and who pretend they really do have ‘supernatural’ or ‘paranormal’ powers: perhaps they claim that they really can bend metal or stop clocks by the power of thought alone. Some of these dishonest fakes (‘charlatans’ is a good word for them) earn large fees from mining or oil companies by claiming that they can tell, using ‘psychic powers’, where would be a good place to drill. Other charlatans exploit people who are grieving, by claiming to be able to make contact with the dead. When this happens it is no longer just fun or entertainment, but preying on people’s gullibility and distress. To be fair, it may be that not all of these people are charlatans. Some of them may sincerely believe they are talking to the dead. The third meaning of magic is the one I mean in my title: poetic magic. We are moved to tears by a beautiful piece of music and we describe the performance as ‘magical’. We gaze up at the stars on a dark night with no moon and no city lights and, breathless with joy, we say the sight is ‘pure magic’. We might use the same word to describe a gorgeous sunset, or an alp","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:1:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"The slow magic of evolution To turn one complex organism into another complex organism in a single step – as in a fairytale – would indeed be beyond the realms of realistic possibility. And yet complex organisms do exist. So how did they arise? How, in reality, did complicated things like frogs and lions, baboons and banyan trees, princes and pumpkins, you and me come into existence? For most of history that was a baffling question, which no one could answer properly. People therefore invented stories to try to explain it. But then the question was answered – and answered brilliantly – in the nineteenth century, by one of the greatest scientists who ever lived, Charles Darwin. I’ll use the rest of this chapter to explain his answer, briefly, and in different words from Darwin’s own. The answer is that complex organisms – like humans, crocodiles and Brussels sprouts – did not come about suddenly, in one fell swoop, but gradually, step by tiny step, so that what was there after each step was only a little bit different from what was already there before. Imagine you wanted to create a frog with long legs. You could give yourself a good start by beginning with something that was already a bit like what you wanted to achieve: a frog with short legs, say. You would look over your short-legged frogs and measure their legs. You’d pick a few males and a few females that had slightly longer legs than most, and you’d let them mate together, while preventing their shorter-legged friends from mating at all. The longer-legged males and females would make tadpoles together, and these would eventually grow legs and become frogs. Then you’d measure this new generation of frogs, and once again pick out those males and females that had longer-than-average legs, and put them together to mate. After doing this for about 10 generations, you might start to notice something interesting. The average leg length of your population of frogs would now be noticeably longer than the average leg length of the starting population. You might even find that all the frogs of the 10th generation had longer legs than any of the frogs of the first generation. Or 10 generations might not be enough to achieve this: you might need to go on for 20 generations or even more. But eventually you could proudly say, ‘I have made a new kind of frog with longer legs than the old type.’ No wand was needed. No magic of any kind was required. What we have here is the process called selective breeding. It makes use of the fact that frogs vary among themselves and those variations tend to be inherited – that is, passed on from parent to child via the genes. Simply by choosing which frogs breed and which do not, we can make a new kind of frog. Simple, isn’t it? But just making legs longer is not very impressive. After all, we started with frogs – they were just short-legged frogs. Suppose you started, not with a shorter-legged form of frog, but with something that wasn’t a frog at all, say something more like a newt. Newts have very short legs compared with frogs’ legs (compared with frogs’ hind legs, at least), and they use them not for jumping but for walking. Newts also have long tails, whereas frogs don’t have tails at all, and newts are altogether longer and narrower than most frogs. But you can see that, given enough thousands of generations, you could change a population of newts into a population of frogs, simply by patiently choosing, in each of those millions of generations, male and female newts that were slightly more frog-like and letting them mate together, while preventing their less frog-like friends from doing so. At no stage during the process would you see any dramatic change. Every generation would look pretty much like the previous generation, but nevertheless, once enough generations had gone by, you’d start to notice that the average tail length was slightly shorter and the average pair of hind legs was slightly longer. After a very large number of generation","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:1:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"2 WHO WAS THE FIRST PERSON? MOST CHAPTERS in this book are headed by a question. My purpose is to answer the question, or at least give the best possible answer, which is the answer of science. But I shall usually begin with some mythical answers because they are colourful and interesting, and real people have believed them. Some people still do. All peoples around the world have origin myths, to account for where they came from. Many tribal origin myths talk only about that one particular tribe – as though other tribes don’t count! In the same way, many tribes have a rule that they mustn’t kill people – but ‘people’ turns out to mean only others of your own tribe. Killing members of other tribes is just fine! Here’s a typical origin myth, from a group of Tasmanian aborigines. A god called Moinee was defeated by a rival god called Dromerdeener in a terrible battle up in the stars. Moinee fell out of the stars down to Tasmania to die. Before he died, he wanted to give a last blessing to his final resting place, so he decided to create humans. But he was in such a hurry, knowing he was dying, that he forgot to give them knees; and (no doubt distracted by his plight) he absent-mindedly gave them big tails like kangaroos, which meant they couldn’t sit down. Then he died. The people hated having kangaroo tails and no knees, and they cried out to the heavens for help. The mighty Dromerdeener, who was still roaring around the sky on his victory parade, heard their cry and came down to Tasmania to see what the matter was. He took pity on the people, gave them bendable knees and cut off their inconvenient kangaroo tails so they could all sit down at last; and they lived happily ever after. Quite often we meet different versions of the same myth. That’s not surprising, because people often change details while telling tales around the camp fire, so local versions of the stories drift apart. In a different telling of this Tasmanian myth, Moinee created the first man, called Parlevar, up in the sky. Parlevar couldn’t sit down because he had a tail like a kangaroo and unbendable knees. As before, the rival star god Dromerdeener came to the rescue. He gave Parlevar proper knees and cut off his tail, healing the wound with grease. Parlevar then came down to Tasmania, walking along the sky road (the Milky Way). The Hebrew tribes of the Middle East had only a single god, whom they regarded as superior to the gods of rival tribes. He had various names, none of which they were allowed to say. He made the first man out of dust and called him Adam (which just means ‘man’). He deliberately made Adam like himself. Indeed, most of the gods of history were portrayed as men (or sometimes women), often of giant size and always with supernatural powers. The god placed Adam in a beautiful garden called Eden, filled with trees whose fruit Adam was encouraged to eat – with one exception. This forbidden tree was the ‘tree of knowledge of good and evil’, and the god left Adam in no doubt that he must never eat its fruit. The god then realized that Adam might be lonely all by himself, and wanted to do something about it. At this point – as with the story of Dromerdeener and Moinee – there are two versions of the myth, both found in the biblical book of Genesis. In the more colourful version, the god made all the animals as Adam’s helpers, then decided that there was still something missing: a woman! So he gave Adam a general anaesthetic, cut him open, removed one rib and stitched him up again. Then he grew a woman from the rib, rather as you grow a flower from a cutting. He named her Eve and presented her to Adam as his wife. Unfortunately, there was a wicked snake in the garden, who approached Eve and persuaded her to give Adam the forbidden fruit from the tree of knowledge of good and evil. Adam and Eve ate the fruit and promptly acquired the knowledge that they were naked. This embarrassed them, and they made themselves aprons out of fig leaves. When the go","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:2:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Who was the first person *really*? This may surprise you, but there never was a first person – because every person had to have parents, and those parents had to be people too! Same with rabbits. There never was a first rabbit, never was a first crocodile, never a first dragonfly. Every creature ever born belonged to the same species as its parents (with perhaps a very small number of exceptions, which I shall ignore here). So that must mean that every creature ever born belonged to the same species as its grandparents. And its great-grandparents. And its great-great-grandparents. And so on for ever. For ever? Well, no, it’s not as simple as that. This is going to need a bit of explaining, and I’ll begin with a thought experiment. A thought experiment is an experiment in your imagination. What we are going to imagine is not literally possible because it takes us way, way back in time, long before we were born. But imagining it teaches us something important. So, here is our thought experiment. All you have to do is imagine yourself following these instructions. Find a picture of yourself. Now take a picture of your father and place it on top. Then find a picture of his father, your grandfather. Then place on top of that a picture of your grandfather’s father, your great-grandfather. You may not have ever met any of your great-grandfathers. I never met any of mine, but I know that one was a country schoolmaster, one a country doctor, one a forester in British India, and one a lawyer, greedy for cream, who died rock-climbing in old age. Still, even if you don’t know what your father’s father’s father looked like, you can imagine him as a sort of shadowy figure, perhaps a fading brown photograph in a leather frame. Now do the same thing with his father, your great-great-grandfather. And just carry on piling the pictures on top of each other, going back through more and more and more great-great-greats. You can go on doing this even before photography was invented: this is a thought experiment, after all. How many greats do we need for our thought experiment? Oh, a mere 185 million or so will do nicely! Mere? MERE? It isn’t easy to imagine a pile of 185 million pictures. How high would it be? Well, if each picture was printed as a normal picture postcard, 185 million pictures would form a tower about 220,000 feet high: that’s more than 180 New York skyscrapers standing on top of each other. Too tall to climb, even if it didn’t fall over (which it would). So let’s tip it safely on its side, and pack the pictures along the length of a single bookshelf. How long is the bookshelf? About forty miles. The near end of the bookshelf has the picture of you. The far end has a picture of your 185-million-greats-grandfather. What did he look like? An old man with wispy hair and white sidewhiskers? A caveman in a leopard skin? Forget any such thought. We don’t know exactly what he looked like, but fossils give us a pretty good idea. Believe it or not, your 185-million-greats-grandfather was – a fish. So was your 185-million-greats-grandmother, which is just as well or they couldn’t have mated with each other and you wouldn’t be here. Let’s now walk along our forty-mile bookshelf, pulling pictures off it one by one to have a look at them. Every picture shows a creature belonging to the same species as the picture on either side of it. Every one looks just like its neighbours in the line – or at least as much alike as any man looks like his father and his son. Yet if you walk steadily from one end of the bookshelf to the other, you’ll see a human at one end and a fish at the other. And lots of other interesting great- . . . great-grandparents in between, which, as we shall soon see, include some animals that look like apes, others that look like monkeys, others that look like shrews, and so on. Each one is like its neighbours in the line, yet if you pick any two pictures far apart in the line they are very different – and if you follow the line ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:2:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Turned to stone Now, how do we know what our distant ancestors looked like, and how do we know when they lived? Mostly from fossils. Fossils are made of stone. They are stones that have picked up the shapes of dead animals or plants. The great majority of animals die with no hope of turning into a fossil. The trick, if you want to be a fossil, is to get yourself buried in the right kind of mud or silt, the kind that might eventually harden to form ‘sedimentary rock’. What does that mean? Rocks are of three kinds: igneous, sedimentary and metamorphic. I shall ignore metamorphic rocks, as they were originally one of the other two kinds, igneous or sedimentary, and have been changed by pressure and/or heat. Igneous rocks (from the Latin for ‘fire’, ignis) were once molten, like the hot lava that comes out of erupting volcanoes now, and solidified into hard rock when they cooled. Hard rocks, of any kind, get worn down (‘eroded’) by wind or water to make smaller rocks, pebbles, sand and dust. Sand or dust gets suspended in water and can then settle in layers of sediment or mud at the bottom of a sea, lake or river. Over a very long time, sediments can harden to make layers (or ‘strata’) of sedimentary rock. Although all strata start off flat and horizontal, they have often got tilted, upended or warped by the time we see them, millions of years later (we will see how this happens in Chapter 10 on earthquakes). Now, suppose a dead animal happens to get washed into the mud, in an estuary perhaps. If the mud later hardens to become sedimentary rock, the animal’s body may rot away, leaving in the hardening rock a hollow imprint of its form which we eventually find. That is one kind of fossil – a kind of ‘negative’ picture of the animal. Or the hollow imprint may act as a mould into which new sediments fall, later hardening to form a ‘positive’ replica of the outside of the animal’s body. That’s a second kind of fossil. And there’s a third kind of fossil in which the atoms and molecules of the animal’s body are, one by one, replaced by atoms and molecules of minerals from the water, which later crystallize to form rock. This is the best kind of fossil because, with luck, tiny details of the animal’s insides are permanently reproduced, right through the middle of the fossil. Fossils can even be dated. We can tell how old they are, mostly by measuring radioactive isotopes in the rocks. We’ll learn what isotopes are, and atoms, in Chapter 4. Briefly, a radioactive isotope is a kind of atom which decays into a different kind of atom: for example, one called uranium-238 turns into one called lead-206. Because we know how long this takes to happen, we can think of the isotope as a radioactive clock. Radioactive clocks are rather like the water clocks and candle clocks that people used in the days before pendulum clocks were invented. A tank of water with a hole in the bottom will drain at a measurable rate. If the tank was filled at dawn, you can tell how much of the day has passed by measuring the present level of water. Same with a candle clock. The candle burns at a fixed rate, so you can tell how long it has been burning by measuring how much candle is left. In the case of a uranium-238 clock, we know that it takes 4.5 billion years for half the uranium-238 to decay to lead-206. This is called the ‘half-life’ of uranium-238. So, by measuring how much lead-206 there is in a rock, compared with the amount of uranium-238, you can calculate how long it is since there was no lead-206 and only uranium-238: how long, in other words, since the clock was ‘zeroed’. And when is the clock zeroed? Well, it only happens with igneous rocks, whose clocks are all zeroed at the moment when the molten rock hardens to become solid. It doesn’t work with sedimentary rock, which has no such ‘zero moment’, and this is a pity because fossils are found only in sedimentary rocks. So we have to find igneous rocks close by sedimentary layers and use them as our clock","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:2:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"A ride back in time Let’s do another thought experiment. Take a few companions and get in a time machine. Fire up the engine and zoom back ten thousand years. Open the door and have a look at the people you meet. If you happen to land in what is now Iraq, they’ll be in the process of inventing agriculture. In most other places they’ll be ‘hunter-gatherers’, moving from place to place, hunting wild animals and gathering wild berries, nuts and roots. You won’t be able to understand what they say and they will be wearing very different clothes (if any). Nevertheless, if you dress them in modern clothes and give them modern haircuts, they will be indistinguishable from modern people (or no more different from some modern people than people are different from one another today). And they will be fully capable of breeding with any of the modern people on board your time machine. Now, take one volunteer from among them (perhaps your 400-greats-grandfather, because this is approximately the time when he might have lived) and set off again in your time machine, back another ten thousand years: to twenty thousand years ago, where you have a chance to meet your 800-greats-grandparents. This time the people you see will all be hunter-gatherers but, once again, their bodies will be those of fully modern humans and, once again, they will be perfectly capable of interbreeding with modern people and producing fertile offspring. Take one of them with you in the time machine, and set off another ten thousand years into the past. Keep on doing this, hopping back in steps of ten thousand years, at each stop picking up a new passenger and taking him or her back to the past. The point is that eventually, after a lot of ten-thousand-year hops, perhaps when you’ve gone a million years into the past, you’ll begin to notice that the people you meet when you emerge from the time machine are definitely different from us, and can’t interbreed with the people who boarded with you at the start of its journey. But they will be capable of breeding with the latest additions to the passenger list, who are almost as ancient as they are themselves. I’m just making the same point as I made before – about gradual change being imperceptible, like the moving hour hand of a watch – but using a different thought experiment. It’s worth saying in two different ways, because it is so important and yet – quite understandably – so hard for some people to appreciate. Let’s resume our journey into the past, and look at some of the stations on the way back to that fish. Suppose we have just arrived in our time machine at the station labelled ‘Six Million Years Ago’. What shall we find there? So long as we make a point of being in Africa, we’ll find our 250,000-greats-grandparents (give or take some generations). They’ll be apes, and they might look a bit like chimpanzees. But they won’t be chimpanzees. Instead, they’ll be the ancestors that we share with chimpanzees. They’ll be too different from us to mate with us, and too different from chimpanzees to mate with chimpanzees. But they will be able to mate with the passengers we took on board at Station Five Million Nine Hundred and Ninety Thousand Years Ago. And probably those from Station Five Million Nine Hundred Thousand Years Ago, too. But probably not those who joined us at Station Four Million Years Ago. Let’s now resume our ten-thousand-year hops, all the way back to Station Twenty-Five Million Years Ago. There we shall find your (and my) one-and-a-half-million-greats-grandparents – at an approximate estimate. They will not be apes, for they will have tails. We would call them monkeys if we met them today, although they are no more closely related to modern monkeys than they are to us. Although very different from us, and incapable of breeding with us or with modern monkeys, they will breed happily with the all-but-identical passengers who joined us at Station Twenty-Four Million Nine Hundred and Ninety Thousand Years A","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:2:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"DNA tells us we are all cousins Although we may lack the fossils to tell us exactly what our very ancient ancestors looked like, we are in no doubt at all that all living creatures are our cousins, and cousins of each other. And we also know which modern animals are close cousins of each other (like humans and chimpanzees, or rats and mice), and which are distant cousins of each other (like humans and cuckoos, or mice and alligators). How do we know? By systematically comparing them. Nowadays, the most powerful evidence comes from comparing their DNA. DNA is the genetic information that all living creatures carry in each of their cells. The DNA is spelled out along massively coiled ‘tapes’ of data, called ‘chromosomes’. These chromosomes really are very like the kind of data tapes you’d feed into an old-fashioned computer, because the information they carry is digital and is strung along them in order. They consist of long strings of code ‘letters’, which you can read and count: each letter is either there or it isn’t – there are no half measures. That’s what makes it digital, and why we say DNA is ‘spelled out’. All genes, in every animal, plant and bacterium that has ever been looked at, are coded messages for how to build the creature, written in a standard alphabet. The alphabet has only four letters to choose from (as opposed to the 26 letters of the English alphabet). We write the DNA letters as A, T, C and G. The same genes occur in many different creatures, with a few revealing differences. For example, there’s a gene called FoxP2, which is shared by all mammals and lots more creatures besides. The gene is a string of more than 2,000 letters. You can tell that FoxP2 is the same gene in all mammals because the great majority of the code letters are the same. Not quite all the chimpanzee letters are the same as ours, and somewhat fewer of the mouse ones are. Of the total of 2,076 letters in FoxP2, the chimpanzee has nine letters different from ours, while the mouse has 139 letters different. And that pattern holds for other genes too. That explains why chimpanzees are very like us, while mice are less so. Chimpanzees are our close cousins, mice are our more distant cousins. ‘Distant cousins’ means that the most recent ancestor we share with them lived a long time ago. Monkeys are closer to us than mice but further from us than chimpanzees. Baboons and rhesus macaques are both monkeys, close cousins of each other, and with almost identical FoxP2 genes. They are exactly as distant from chimps as they are from us; and the number of DNA letters in FoxP2 that separate baboons from chimps is almost exactly the same (24) as the number of letters that separate baboons from us (23). It all fits. And, just to finish off this little thought, frogs are much more distant cousins of all mammals. All mammals have approximately the same number of letter differences from a frog (about 140), for the simple reason that they are all exactly equally close cousins: all mammals share a more recent ancestor with each other (about 180 million years ago) than they do with the frog (about 340 million years ago). But of course not all humans are the same as all other humans, and not all baboons are the same as all other baboons and not all mice are the same as all other mice. We could compare your genes with mine, letter by letter. And the result? We’d turn out to have even more letters in common than either of us does with a chimpanzee. But we’d still find some letters that are different. Not many, and there’s no particular reason to single out the FoxP2 gene. But if you counted up the number of letters all humans share in all our genes, it would be more than any of us shares with a chimpanzee. And you share more letters with your cousin than you share with me. And you share even more letters with your mother and your father, and (if you have one) with your sister or brother. In fact, you can work out how closely related any two people are to each","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:2:4","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"3 WHY ARE THERE SO MANY DIFFERENT KINDS OF ANIMALS? THERE ARE LOTS of myths that attempt to explain why particular kinds of animals are the way that they are – myths that ‘explain’ things like why leopards have spots, and why rabbits have white tails. But there don’t seem to be many myths about the sheer range and variety of different kinds of animals. I can find nothing akin to the Jewish myth of the Tower of Babel, which accounts for the great variety of languages. Once upon a time, according to this myth, all the people in the world spoke the same language. They could therefore work harmoniously together to build a great tower, which they hoped would reach the sky. God noticed this and took a very dim view of everybody being able to understand everybody else. Whatever might they get up to next, if they could talk to each other and work together? So he decided to ‘confound their language’ so that ‘they may not understand one another’s speech’. This, the myth tells us, is why there are so many different languages, and why, when people try to talk to people from another tribe or country, their speech often sounds like meaningless babble. Oddly enough, there is no connection between the word ‘babble’ and the Tower of Babel. I was hoping to find a similar myth about the great diversity of animals, because there is a resemblance between language evolution and animal evolution, as we shall see. But there doesn’t seem to be any myth that specifically tackles the sheer number of different kinds of animals. This is surprising, because there is indirect evidence that tribal peoples can be well aware of the fact there are many different kinds of animals. In the 1920s a now famous German scientist called Ernst Mayr did a pioneering study of the birds of the New Guinea highlands. He compiled a list of 137 species, then discovered, to his amazement, that the local Papuan tribesmen had separate names for 136 of them. Back to the myths. The Hopi tribe of North America had a goddess called Spider Woman. In their creation myth she teamed up with Tawa the sun god, and they sang the First Magic Song as a duet. This song brought the Earth, and life, into being. Spider Woman then took the threads of Tawa’s thoughts and wove them into solid form, creating fish, birds, and all other animals. Other North American tribes, the Pueblo and Navajo peoples, have a myth of life that is a tiny bit like the idea of evolution: life emerges from the Earth like a sprouting plant growing up through a sequence of stages. The insects climbed from their world, the First or Red World, up into the Second World, the Blue World, where the birds lived. The Second World then became too crowded, so the birds and insects flew up into the Third or Yellow World, where the people and other mammals lived. The Yellow World in turn became crowded and food became scarce, so they all, insects, birds and everybody, went up to the Fourth World, the Black and White World of day and night. Here the gods had already created cleverer people who knew how to farm the Fourth World and who taught the newcomers how to do it too. The Jewish creation myth comes closer to doing justice to diversity, but it doesn’t really attempt to explain it. Actually, the Jewish holy book has two different creation myths, as we saw in the previous chapter. In the first one, the Jewish god created everything in six days. On the fifth day he created fish, whales and all sea creatures, and the birds of the air. On the sixth day he made the rest of the land animals, including man. The language of the myth pays some attention to the number and variety of living creatures – for example, ‘God created great whales, and every living creature that moveth, which the waters brought forth abundantly after their kind, and every winged fowl after his kind,’ and made every ‘beast of the earth’ and ‘every thing that creepeth upon the earth after his kind’. But why was there such variety? We are not told. In the second myth we","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:3:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Why are there *really* so many different kinds of animals? Adam’s task of naming all the animals was a tough one – tougher than the ancient Hebrews could possibly have realized. It’s been estimated that about 2 million species have so far been given scientific names, and even these are just a small fraction of the number of species yet to be named. How do we even decide whether two animals belong in the same species or in two different species? Where animals reproduce sexually, we can come up with a sort of definition. Animals belong to different species if they don’t breed together. There are borderline cases like horses and donkeys, which can breed together but produce offspring (called mules or hinnies) that are infertile – that is, that cannot have offspring themselves. We therefore place a horse and a donkey in different species. More obviously, horses and dogs belong to different species because they don’t even try to interbreed, and couldn’t produce offspring if they did, even infertile ones. But spaniels and poodles belong to the same species because they happily interbreed, and the puppies that they produce are fertile. Every scientific name of an animal or plant consists of two Latin words, usually printed in italics. The first word refers to the ‘genus’ or group of species and the second to the individual species within the genus. Homo sapiens (‘wise man’) and Elephas maximus (‘very big elephant’) are examples. Every species is a member of a genus. Homo is a genus. So is Elephas. The lion is Panthera leo and the genus Panthera also includes Panthera tigris (tiger), Panthera pardus (leopard or ‘panther’) and Panthera onca (jaguar). Homo sapiens is the only surviving species of our genus, but fossils have been given names like Homo erectus and Homo habilis. Other human-like fossils are sufficiently different from Homo to be placed in a different genus, for example Australopithecus africanus and Australopithecus afarensis (nothing to do with Australia, by the way: australo- just means ‘southern’, which is where Australia’s name also comes from). Each genus belongs to a family, usually printed in ordinary ‘roman’ type with a capital initial. Cats (including lions, leopards, cheetahs, lynxes and lots of smaller cats) make up the family Felidae. Every family belongs to an order. Cats, dogs, bears, weasels and hyenas belong to different families within the order Carnivora. Monkeys, apes (including us) and lemurs all belong to different families within the order Primates. And every order belongs to a class. All mammals are in the class Mammalia. Can you see the shape of a tree developing in your mind as you read this description of the sequence of groupings? It is a family tree: a tree with many branches, each branch having sub-branches, and each sub-branch having sub-sub-branches. The tips of the twigs are species. The other groupings – class, order, family, genus – are the branches and sub-branches. The whole tree is all of life on Earth. Think about why trees have so many twigs. Branches branch. When we have enough branches of branches of branches, the total number of twigs can be very large. That’s what happens in evolution. Charles Darwin himself drew a branching tree as the only picture in his most famous book, On the Origin of Species. He sketched an early version in one of his notebooks some years earlier. At the top of the page he wrote a mysterious little message to himself: ‘I think’. What do you think he meant? Maybe he started to write a sentence and one of his children interrupted him so he never finished it. Maybe he found it easier to represent quickly what he was thinking in this diagram than in words. Perhaps we shall never know. There is other handwriting on the page, but it is hard to decipher. It is tantalizing to read the actual notes of a great scientist, written on a particular day and never meant for publication. The following isn’t exactly how the tree of animals branched, but it gives you an idea ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:3:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Pulling apart: how languages, and species, divide Although the legend of the Tower of Babel is, of course, not really true, it does raise the interesting question of why there are so many different languages. Just as some species are more similar than others and are placed in the same family, so there are also families of languages. Spanish, Italian, Portuguese, French and many European languages and dialects such as Romansch, Galician, Occitan and Catalan are all pretty similar to each other; together they’re called ‘Romance’ languages. The name actually comes from their common origin in Latin, the language of Rome, not from any association with romance, but let’s use an expression of love as our example. Depending on which country you are in, you might declare your feelings in one of the following ways: ‘Ti amo’, ‘Amote’, ‘T’aimi’ or ‘Je t’aime’. In Latin it would be ‘Te amo’ – exactly like modern Spanish. To swear your love to someone in Kenya, Tanzania or Uganda you could say, in Swahili, ‘Nakupenda’. A bit further south, in Mozambique, Zambia, or Malawi where I was brought up, you might say, in the Chinyanja language, ‘Ndimakukonda’. In other so-called Bantu languages in southern Africa you might say ‘Ndinokuda’, ‘Ndiyakuthanda’ or, to a Zulu, ‘Ngiyakuthanda’. This Bantu family of languages is quite distinct from the Romance family of languages, and both are distinct from the Germanic family which includes Dutch, German and the Scandinavian languages. See how we use the word ‘family’ for languages, just as we do for species (the cat family, the dog family) and also, of course, for our own families (the Jones family, the Robinson family, the Dawkins family). It isn’t hard to work out how families of related languages arise over the centuries. Listen to the way you and your friends speak to each other, and compare it to the way your grandparents speak. Their speech is only slightly different and you can easily understand them, but they are only two generations away. Now imagine talking, not to your grandparents but to your 25-greats-grandparents. If you happen to be English, that might take you back to the late fourteenth century – the lifetime of the poet Geoffrey Chaucer, who wrote descriptions like this: He was a lord ful fat and in good poynt; His eyen stepe, and rollynge in his heed, That stemed as a forneys of a leed; His bootes souple, his hors in greet estaat. Now certeinly he was a fair prelaat; He was nat pale as a forpyned goost. A fat swan loved he best of any roost. His palfrey was as broun as is a berye. Well, it is recognizably English, isn’t it? But I bet you’d have a hard time understanding it if you heard it spoken. And if it was any more different you’d probably consider it a separate language, as different as Spanish is from Italian. So, the language in any one place changes century by century. We could say it ‘drifts’ into something different. Now add the fact that people speaking the same language in different places don’t often have the opportunity to hear each other (or at least they didn’t before telephones and radios were invented); and the fact that language drifts in different directions in different places. This applies to the way it is spoken as well as to the words themselves: think how different English sounds in a Scottish, Welsh, Geordie, Cornish, Australian or American accent. And Scottish people can easily distinguish an Edinburgh accent from a Glasgow accent or a Hebridean accent. Over time, both the way the language is spoken and the words used become characteristic of a region; when two ways of speaking a language have drifted sufficiently far apart, we call them different ‘dialects’. After enough centuries of drift, different regional dialects eventually become so different that people in one region can no longer understand people in another. At this point we call them separate languages. That is what happened when German and Dutch drifted, in separate directions, from a now extinct a","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:3:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Islands and isolation: the power of separation So the DNA of species, like the words of languages, drifts apart when separated. Why might this happen? What might start the separation? An obvious possibility is the sea. Populations on separate islands don’t meet each other – not often, anyway – so their two sets of genes have the opportunity to drift away from one another. This makes islands extremely important in the origins of new species. But we can think of an island as more than just a piece of land surrounded by water. To a frog, an oasis is an ‘island’ where it can live, surrounded by a desert where it can’t. To a fish, a lake is an island. Islands matter, both for species and for languages, because the population of an island is cut off from contact with other populations (preventing gene flow in the case of species, just as it prevents language drift) and so is free to begin to evolve in its own direction. The next important point is that the population of an island need not be totally isolated for ever: genes can occasionally cross the barrier surrounding it, whether this be water or uninhabitable land. On 4 October 1995 a mat of logs and uprooted trees was blown onto a beach on the Caribbean island of Anguilla. On the mat were 15 green iguanas, alive after what must have been a perilous journey from another island, probably Guadeloupe, 160 miles away. Two hurricanes, called Luis and Marilyn, had roared through the Caribbean during the previous month, uprooting trees and flinging them into the sea. It seems that one of these hurricanes must have torn down the trees in which the iguanas were climbing (they love sitting up in trees, as I have seen in Panama) and blown them out to sea. Eventually reaching Anguilla, the iguanas crawled off their unorthodox means of transport onto the beach and began a new life, feeding and reproducing and passing on their DNA, on a brand new island home. We know this happened because the iguanas were seen arriving on Anguilla by local fishermen. Centuries earlier, although nobody was there to witness it, something similar is almost certainly what brought the iguanas’ ancestors to Guadeloupe in the first place. And something like the same story almost certainly accounts for the presence of iguanas on the Galapagos islands, which is where we turn for the next step in our story. The Galapagos islands are historically important because they probably inspired Charles Darwin’s first thoughts on evolution when, as a member of the expedition on HMS Beagle, he visited them in 1835. They are a collection of volcanic islands in the Pacific Ocean near the equator, about 600 miles west of South America. They are all young (just a few million years old), formed by volcanoes punching up from the bottom of the sea. This means that all the species of animals and plants on the islands must have arrived from elsewhere – presumably the mainland of South America – and recently, by evolutionary standards. Once arrived, species could make the shorter crossings from island to island, sufficiently often to reach all the islands (maybe once or twice every century or so) but sufficiently seldom that they were able to evolve separately – ‘drift apart’ as we have been saying in this chapter – during the intervals between the rare crossings. Nobody knows when the first iguanas arrived in the Galapagos. They probably rafted across from the mainland just like the ones that arrived in Anguilla in 1995. Nowadays the nearest island to the mainland is San Cristobal (Darwin knew it by the English name of Chatham), but millions of years ago there were other islands too, which have now sunk beneath the sea. The iguanas could have arrived first on one of the now sunken islands, and then crossed to other islands, including those still above water today. Once there, they had the opportunity to flourish in a new place, just like the ones that arrived in Anguilla in 1995. The first iguanas on Galapagos would have evolved to become ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:3:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Stirring, selection and survival I want to round off the chapter by telling the story again in slightly different language. I’ve already briefly mentioned gene flow; scientists also talk of something called the gene pool, and I now want to spell out more fully what that means. Of course there can’t literally be a pool of genes. The word ‘pool’ suggests a liquid, in which genes might be stirred around. But genes are found only in the cells of living bodies. So what does it mean to talk of a gene pool? In every generation, sexual reproduction sees to it that genes are shuffled. You were born with the shuffled genes of your father and your mother, which means the shuffled genes of your four grandparents. The same applies to every individual in the population over the long, long reach of evolutionary time: thousands of years, tens of thousands, hundreds of thousands of years. During that time, this process of sexual shuffling sees to it that the genes within the whole population are so thoroughly shuffled, indeed stirred, that it makes sense to talk of a great, swirling pool of genes: the ‘gene pool’. You remember our definition of a species as a group of animals or plants that can breed with each other? Now you can see why this definition matters. If two animals are members of the same species in the same population, that means their genes are being stirred about in the same gene pool. If two animals are members of different species they cannot be members of the same gene pool because their DNA cannot mix in sexual reproduction, even if they live in the same country and meet each other frequently. If populations of the same species are geographically separated, their gene pools have the opportunity to drift apart – so far apart, eventually, that if they happen to meet again they can no longer breed together. Now that their gene pools have moved beyond mixing they have become different species and can go on moving further apart for millions of years to the point where they might become as different from one another as humans are from cockroaches. Evolution means change in a gene pool. Change in a gene pool means that some genes become more numerous, others less. Genes that used to be common become rare, or disappear altogether. Genes that used to be rare become common. And the result is that the shape, or size, or colour, or behaviour of typical members of the species changes: it evolves, because of changes in the numbers of genes in the gene pool. That is what evolution is. Why should the numbers of different genes change as the generations go by? Well, you might say it would be surprising if they didn’t, given such immensities of time. Think of the way language changes over the centuries. Words like ‘thee’ and ‘thou’, ‘zounds’ and ‘avast’, phrases like ‘stap me vitals’, have now more or less dropped out of English. On the other hand, the phrase ‘I was like’ (meaning ‘I said’), which would have been incomprehensible as recently as 20 years ago, is now commonplace. So is ‘cool’ as a term of approval. So far in this chapter, I haven’t needed to go much further than the idea that gene pools in separate populations can drift apart, like languages. But actually, in the case of species, there is much more to it than drifting. This ‘much more’ is natural selection, the supremely important process that was Charles Darwin’s greatest discovery. Even without natural selection, we’d expect gene pools that happen to be separated to drift apart. But they’d drift in a rather aimless fashion. Natural selection nudges evolution in a purposeful direction: namely, the direction of survival. The genes that survive in a gene pool are the genes that are good at surviving. And what makes a gene good at surviving? It helps other genes to build bodies that are good at surviving and reproducing: bodies that survive long enough to pass on the genes that helped them to survive. Exactly how they do it varies from species to species. Genes survive in bird or ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:3:4","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"4 WHAT ARE THINGS MADE OF? IN VICTORIAN TIMES, a favourite book for children was Edward Lear’s Book of Nonsense. As well as the poems about the Owl and the Pussycat (which you may know because it is still famous), The Jumblies and The Pobble Who Has No Toes, I love the Recipes at the end of the book. The one for Crumboblious Cutlets begins like this: ‘Procure some strips of beef, and having cut them into the smallest possible slices, proceed to cut them still smaller, eight or perhaps nine times.’ What do you get if you keep on cutting stuff into smaller and smaller pieces? Suppose you take a piece of anything and cut it in half, using the thinnest and sharpest razor blade you can find. Then you cut that in half, then cut that half in half, and so on, over and over again. Do the pieces eventually get so small that they can’t get any smaller? How thin is the edge of a razor blade? How small is the sharp end of a needle? What are the smallest bits that things are made of? The ancient civilizations of Greece, China and India all seem to have arrived at the same idea that everything is made from four ‘elements’: air, water, fire and earth. But one ancient Greek, Democritus, came a bit closer to the truth. Democritus thought that, if you cut anything up into sufficiently small pieces, you would eventually reach a piece so small that it couldn’t be cut any further. The Greek for ‘cut’ is tomos, and if you stick an ‘a’ in front of a Greek word it means ‘not’ or ‘you can’t’. So ‘a-tomic’ means something too small to be cut any smaller, and that is where our word ‘atom’ comes from. An atom of gold is the smallest possible bit of gold. Even if it were possible to cut it any smaller, it would cease to be gold. An atom of iron is the smallest possible bit of iron. And so on. We now know that there are about 100 different kinds of atoms, of which only about 90 occur in nature. The few others have been concocted by scientists in the lab, but only in tiny quantities. Pure substances that consist of one kind of atom only are called elements (same word as was once used for earth, air, fire and water, but with a very different meaning). Examples of elements are hydrogen, oxygen, iron, chlorine, copper, sodium, gold, carbon, mercury and nitrogen. Some elements, such as molybdenum, are rare on Earth (which is why you may not have heard of molybdenum) but commoner elsewhere in the universe (if you wonder how we know this, wait for Chapter 8). Metals such as iron, lead, copper, zinc, tin and mercury are elements. So are gases such as oxygen, hydrogen, nitrogen and neon. But most of the substances that we see around us are not elements but compounds. A compound is what you get when two or more different atoms join together in a particular way. You’ve probably heard water referred to as ‘H2O’. This is its chemical formula, and means it is a compound of one oxygen atom joined to two hydrogen atoms. A group of atoms joined together to make a compound is called a molecule. Some molecules are very simple: a molecule of water, for example, has just those three atoms. Other molecules, especially those in living bodies, have hundreds of atoms, all joined together in a very particular way. Indeed, it is the way they are joined together, as well as the type and number of atoms, that makes any particular molecule one compound and not another. You can also use the word ‘molecule’ to describe what you get when two or more of the same kind of atom join together. A molecule of oxygen, the gas we need in order to breathe, consists of two oxygen atoms joined together. Sometimes three oxygen atoms join together to form a different kind of molecule called ozone. The number of atoms in a molecule really makes a difference, even if the atoms are all the same. Ozone is harmful to breathe, but we benefit from a layer of it in the Earth’s upper atmosphere, which protects us from the most damaging of the sun’s rays. One of the reasons Australians have to be especially caref","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:4:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Crystals – atoms on parade A diamond crystal is a huge molecule, of no fixed size, consisting of millions of atoms of the element carbon stuck together, all lined up in a very particular way. They are so regularly spaced inside the crystal, you could think of them as being like soldiers on parade, except that they are parading in three dimensions, like a shoal of fish. But the number of ‘fish’ in the shoal – the number of carbon atoms in even the smallest diamond crystal – is gigantic, more than all the fish (plus all the people) in the world. And ‘stuck together’ is a misleading way to describe them if it makes you think of the atoms as solid lumps of carbon closely packed with no space in between. In fact, as we shall see, most ‘solid’ matter consists of empty space. That will take some explaining! I’ll come back to it. All crystals are built up in the same ‘soldiers-on-parade’ way, with atoms regularly spaced in a fixed pattern that gives the whole crystal its shape. Indeed, that is what we mean by a crystal. Some ‘soldiers’ are capable of ‘parading’ in more than one way, producing very different crystals. Carbon atoms, if they parade in one way, make the legendarily hard diamond crystals. But if they adopt a different formation they make crystals of graphite, so soft it is used as a lubricant. We think of crystals as beautiful transparent objects, and we even describe other things like pure water as ‘crystal clear’. But actually, most solid stuff is made of crystals, and most solid stuff is not transparent. A lump of iron is made of lots of tiny crystals packed together, each crystal consisting of millions of iron atoms, spaced out ‘on parade’ like the carbon atoms in a diamond crystal. Lead, aluminium, gold, copper – all are made of crystals of their different kinds of atoms. So are rocks, like granite or sandstone – but they are often mixtures of lots of different kinds of tiny crystals all packed together. Sand is crystalline, too. In fact, many sand grains are just little bits of rock, ground down by water and wind. The same is true of mud, with the addition of water or other liquids. Often, sand grains and mud grains get packed together again to make new rocks, called ‘sedimentary’ rocks because they are hardened sediments of sand and mud. (A ‘sediment’ is the bits of solid stuff that settle in the bottom of a liquid, for example in a river or lake or sea.) The sand in sandstone is mostly made of quartz and feldspar, two common crystals in the Earth’s crust. Limestone is different. Like chalk it is calcium carbonate, and it comes from ground-down coral skeletons and sea shells, including the shells of tiny single-celled creatures called forams. If you see a very white beach, the sand is most likely calcium carbonate from the same shelly source. Sometimes crystals are made entirely of the same kind of atoms ‘on parade’ – all of the same element. Diamond, gold, copper and iron are examples. But other crystals are made of two different kinds of atoms, again on parade in strict order: alternating, for example. Salt (common salt, table salt) is not an element but a compound of two elements, sodium and chlorine. In a crystal of salt, the sodium and chlorine atoms parade together alternately. Actually, in this case they are called not atoms but ‘ions’, but I’m not going to go into why that is. Every sodium ion has six chlorines for neighbours, at right angles to each other: in front, behind, to left, to right, above, and below. And every chlorine ion is surrounded by sodiums, in just the same way. The whole arrangement is composed of squares, and this is why salt crystals, if you look at them carefully with a strong lens, are cubic – the three-dimensional form of a square – or at least have squared-off edges. Lots of other crystals are made of more than one kind of atom ‘on parade’, and many of them are found in rocks, sand and soil. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:4:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Solid, liquid, gas – how molecules move Crystals are solid, but not everything is solid. We also have liquids and gases. In a gas, the molecules don’t stick together as they do in a crystal, but rush freely about within whatever space is available, travelling in straight lines like billiard balls (but in three dimensions, not two as on a flat table). They rush about until they hit something, such as another molecule or the walls of a container, in which case they bounce off, again like billiard balls. Gases can be compressed, which shows there is a lot of space between the atoms and molecules. When you compress a gas, it feels ‘springy’. Put your finger over the end of a bicycle pump and feel the springiness as you push the plunger in. If you keep your finger there, when you let the plunger go it shoots back out. The springiness that you are feeling is called ‘pressure’. The pressure is the effect of all the millions of molecules of air (a mixture of nitrogen and oxygen and a few other gases) in the pump bombarding the plunger (and everything else, but the plunger is the only part that can move in response). At high pressure the bombardment happens at a higher rate. This will happen if the same number of gas molecules are confined in a smaller volume (for instance, when you push the plunger of a bicycle pump). Or it will happen if you raise the temperature, which makes the gas molecules charge about faster. A liquid is like a gas in that its molecules move around or ‘flow’ (that’s why both are called ‘fluids’, while solids aren’t). But the molecules in a liquid are much closer to each other than the molecules in a gas. If you put a gas into a sealed tank, it fills every nook and cranny of the tank up to the top. The volume of gas rapidly expands to fill the whole tank. A liquid also fills every nook and cranny, but only up to a certain level. A given amount of liquid, unlike the same amount of gas, keeps a fixed volume, and gravity pulls it downwards, so it fills only as much as it needs of the tank, from the bottom upwards. That’s because the molecules of a liquid stay close to each other. But, unlike those of a solid, they do slide around over each other, which is why a liquid behaves as a fluid. A solid doesn’t even try to fill the tank – it just retains its shape. That’s because the molecules of a solid don’t slide around over each other like those of a liquid, but stay in (roughly) the same positions relative to their neighbours. ‘Roughly’ because even in a solid the molecules do sort of jiggle about (faster at higher temperatures): they just don’t move far enough from their position in the crystal ‘parade’ to affect its shape. Sometimes a liquid is ‘viscous’, like treacle. A viscous liquid flows, but so slowly that, although a very viscous liquid eventually fills the bottom part of the tank, it takes a long time to do so. Some liquids are so viscous – flow so slowly – that they might as well be solid. Substances of this kind behave like solids, even though they’re not made of crystals. Solid, liquid and gas are the names we give to the three common ‘phases’ of matter. Many substances are capable of being all three, at different temperatures. On Earth, methane is a gas (it’s often called ‘marsh gas’, because it bubbles up from marshes, and sometimes it catches fire and we see it lit up as eerie ‘will o’ the wisps’). But on a large, very cold moon of the planet Saturn called Titan there are lakes of liquid methane. If a planet were colder still, it might have ‘rocks’ of frozen methane. We think of mercury as a liquid, but that just means it’s liquid at ordinary temperatures on Earth. Mercury is a solid metal if you leave it outside in the Arctic winter. Iron is a liquid if you heat it to a high enough temperature. Indeed, around the deep centre of the Earth is a sea of liquid iron mixed with liquid nickel. For all I know, there may be very hot planets with oceans of liquid iron at the surface, and perhaps strange creatures","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:4:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Inside the atom When we were imagining cutting matter into the smallest possible pieces at the beginning of this chapter, we stopped at the atom. An atom of lead is the smallest object that still deserves to be called lead. But can you really not cut an atom any further? And would an atom of lead actually look like a tiny little chip of lead? No, it wouldn’t look like a tiny piece of lead. It wouldn’t look like anything. That’s because an atom is too small to be seen, even with a powerful microscope. And yes, you can cut an atom into even smaller pieces – but what you then get is no longer the same element, for reasons we shall soon see. What is more, this is very difficult to do, and it releases an alarming quantity of energy. That is why, for some people, the phrase ‘splitting the atom’ has such an ominous ring to it. It was first done by the great New Zealand scientist Ernest Rutherford in 1919. Although we can’t see an atom, and although we can’t split it without turning it into something else, that doesn’t mean we can’t work out what it is like inside. As I explained in Chapter 1, when scientists can’t see something directly, they propose a ‘model’ of what it might be like, and then they test that model. A scientific model is a way of thinking about how things might be. So a model of the atom is a kind of mental picture of what the inside of an atom might be like. A scientific model can seem like a flight of fancy, but it is not just a flight of fancy. Scientists don’t stop at proposing a model: they then go on to test it. They say, ‘If this model that I am imagining were true, we would expect to see such-and-such in the real world.’ They predict what you’ll find if you do a particular experiment and make certain measurements. A successful model is one whose predictions come out right, especially if they survive the test of experiment. And if the predictions come out right, we hope it means that the model probably represents the truth, or at least a part of the truth. Sometimes the predictions don’t come out right, and so scientists go back and adjust the model, or think up a new one, and then go on to test that. Either way, this process of proposing a model and then testing it – what we call the ‘scientific method’ – has a much better chance of getting at the way things really are than even the most imaginative and beautiful myth invented to explain what people didn’t – and often, at the time, couldn’t – understand. An early model of the atom was the so called ‘currant bun’ model proposed by the great English physicist J. J. Thomson at the end of the nineteenth century. I won’t describe it because it was replaced by the more successful Rutherford model, first proposed by the same Ernest Rutherford who split the atom, who came from New Zealand to England to work as Thomson’s pupil and who succeeded Thomson as Cambridge’s Professor of Physics. The Rutherford model, later refined in turn by Rutherford’s pupil, the celebrated Danish physicist Niels Bohr, treats the atom as a tiny, miniaturized solar system. There is a nucleus in the middle of the atom, which contains the bulk of its material. And there are tiny particles called electrons whizzing around the nucleus in ‘orbit’ (though ‘orbit’ may be misleading if you think of it as just like a planet orbiting the sun, because an electron is not a little round thing in a definite place). One surprising thing about the Rutherford / Bohr model, which probably reflects a real truth, is that the distance between each nucleus and the next is very large compared with the size of the nuclei, even in a hard chunk of solid matter like a diamond. The nuclei are hugely spaced out. This is the point I promised to return to. Remember I said that a diamond crystal is a giant molecule made of carbon atoms like soldiers on parade, but a parade in three dimensions? Well, we can now improve our ‘model’ of the diamond crystal by giving it a scale – that is, a sense of how sizes and distances in ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:4:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"The tiniest things of all The nucleus isn’t really like a football. That was just a crude model. It certainly isn’t round like a football. It isn’t even clear whether we should speak of it as having a ‘shape’ at all. Maybe the very word ‘shape’, like the word ‘solid’, loses all meaning down at these very tiny sizes. And we are talking very very tiny indeed: the full stop at the end of this sentence contains about a million million atoms of printing ink. Each nucleus contains smaller particles called protons and neutrons. You can think of them as balls too, if you wish, but like the nuclei they are not really balls. Protons and neutrons are approximately the same size as each other. They are very very tiny indeed, but even so they are still 1,000 times bigger than the electrons (‘gnats’) in orbit around the nucleus. The main difference between a proton and a neutron is that the proton has an electric charge. Electrons, too, have an electric charge, opposite to that of protons. We needn’t bother with exactly what ‘electric charge’ means here. Neutrons have no charge. Because electrons are so very very very tiny (while protons and neutrons are only very very tiny!) the mass of an atom is, to all intents and purposes, just its protons and neutrons. What does ‘mass’ mean? Well, you can think of mass as rather like weight, and you can measure it using the same units as weight (grams or pounds). Weight is not the same as mass, however, and I’ll need to explain the difference, but I’m postponing that to the next chapter. For the moment just think of ‘mass’ as something like ‘weight’. The mass of an object depends almost entirely on how many protons and neutrons it has in all its atoms added together. The number of protons in the nucleus of any atom of a particular element is always the same, and is equal to the number of electrons in orbit around the nucleus, although the electrons don’t contribute noticeably to the mass because they are too small. A hydrogen atom has only one proton (and one electron). A uranium atom has 92 protons. Lead has 82. Carbon has 6. For every possible number from 1 to 100 (and a few more), there is one and only one element that has that number of protons (and the same number of electrons). I won’t list them all, but it would be easy to do so. The number of protons (or electrons) that an element possesses is called the ‘atomic number’ of that element. So you can define an element not just by its name but by its own unique atomic number. For example, element number 6 is carbon; element number 82 is lead. The elements are conveniently set out in a table called the periodic table – I won’t go into why it’s called that, although it is interesting. But now is the moment to return, as I promised I would, to the question of why, when you cut a piece of, say, lead into smaller and smaller pieces, you eventually reach a point where, if you cut it again, it is no longer lead. An atom of lead has 82 protons. If you split the atom so that it no longer has 82 protons it ceases to be lead. The number of neutrons in an atom’s nucleus is less fixed than the number of protons: many elements have different versions, called isotopes, with different numbers of neutrons. For example, there are three isotopes of carbon, called Carbon-12, Carbon-13 and Carbon-14. The numbers refer to the mass of the atom, which is the sum of the protons and neutrons. Each of the three has six protons. Carbon-12 has six neutrons, Carbon-13 has seven neutrons and Carbon-14 has eight neutrons. Some isotopes, for example Carbon-14, are radioactive, which means they change into other elements at a predictable rate, although at unpredictable moments. Scientists can use this feature to help them calculate the age of fossils. Carbon-14 is used to date things younger than most fossils, for example ancient wooden ships. Well then, does our quest to cut things ever smaller and smaller end with these three particles: electrons, protons and neutrons? No – even","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:4:4","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Carbon – the scaffolding of life All the elements are special in their different ways. But one element, carbon, is so special that we will end the chapter by talking briefly about that. Carbon chemistry even has its own name, separating it from the whole of the rest of chemistry: ‘organic’ chemistry. All the rest of chemistry is ‘inorganic’ chemistry. So what is so special about carbon? The answer is that carbon atoms link up with other carbon atoms to form chains. The chemical compound octane, which, as you may know, is an ingredient of petrol (gasoline), is a rather short chain of eight carbon atoms with hydrogen atoms sticking out to the sides. The wonderful thing about carbon is that it can make chains of any length, some literally hundreds of carbon atoms long. Sometimes the chains come around in a loop. For example, molecules of naphthalene (the substance that mothballs are made of) are also made of carbon with hydrogen attached, this time in two loops. Carbon chemistry is rather like the toy construction kit called Tinkertoy. In the laboratory, chemists have succeeded in making carbon atoms join up with each other, not just in simple loops but in wonderfully shaped Tinkertoy-like molecules nicknamed Buckyballs and Buckytubes. ‘Bucky’ was the nickname of Buckminster Fuller, the great American architect who invented the geodesic dome. The Buckyballs and Buckytubes scientists have made are artificial molecules. But they show the Tinkertoyish way in which carbon atoms can be joined together into scaffolding-like structures that can be indefinitely large. (Just recently the exciting news was announced that Buckyballs have been detected in outer space, in the dust drifting near to a distant star.) Carbon chemistry offers a near-infinite number of possible molecules, all of different shapes, and thousands of different ones are found in living bodies. One very large molecule called myoglobin, for example, is found, in millions of copies, in all our muscles. Not all the atoms in myoglobin are carbon atoms, but it is the carbon atoms that join together in these fascinating Tinkertoy-like scaffolding structures. And that is really what makes life possible. When you think that myoglobin is only one example among thousands of equally complicated molecules in living cells, you can perhaps imagine that, just as you can build pretty much anything you like if you have a large enough Tinkertoy set, so the chemistry of carbon provides the vast range of possible forms required to put together anything so complicated as a living organism. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:4:5","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"What, no myths? This chapter has been unusual in that it didn’t begin with a list of myths. This was only because it was so hard to find any myths on this subject. Unlike, say, the sun, or the rainbow, or earthquakes, the fascinating world of the very small never came to the notice of primitive peoples. If you think about this for a minute, it’s not really surprising. They had no way of even knowing it was there, and so of course they didn’t invent any myths to explain it! It wasn’t until the microscope was invented in the sixteenth century that people discovered that ponds and lakes, soil and dust, even our own bodies, teem with tiny living creatures, too small to see, yet complicated and, in their own way, beautiful – or perhaps frightening, depending on how you think about them. Dust mites are distantly related to spiders but too small to see except as tiny specks. There are thousands of them in every home, crawling through every carpet and every bed, quite probably including yours. If primitive peoples had known about them, you can imagine what myths and legends they might have invented to explain them! But before the invention of the microscope, their existence was not even dreamed of – and so there are no myths about them. And, small as it is, even a dust mite contains more than a hundred trillion atoms. Dust mites are too small for us to see, but the cells of which they are made are smaller still. The bacteria that live inside them – and us – in vast numbers are smaller even than that. And atoms are far far smaller even than bacteria. The whole world is made of incredibly tiny things, much too small to be visible to the naked eye – and yet none of the myths or so-called holy books that some people, even now, think were given to us by an all-knowing god, mentions them at all! In fact, when you look at those myths and stories, you can see that they don’t contain any of the knowledge that science has patiently worked out. They don’t tell us how big or how old the universe is; they don’t tell us how to treat cancer; they don’t explain gravity or the internal combustion engine; they don’t tell us about germs, or nuclear fusion, or electricity, or anaesthetics. In fact, unsurprisingly, the stories in holy books don’t contain any more information about the world than was known to the primitive peoples who first started telling them! If these ‘holy books’ really were written, or dictated, or inspired, by all-knowing gods, don’t you think it’s odd that those gods said nothing about any of these important and useful things? ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:4:6","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"5 WHY DO WE HAVE NIGHT AND DAY, WINTER AND SUMMER? OUR LIVES are dominated by two great rhythms, one much slower than the other. The fast one is the daily alternation between dark and light, which repeats every 24 hours, and the slow one is the yearly alternation between winter and summer, which has a repeat time of a little over 365 days. Not surprisingly, both rhythms have spawned myths. The day–night cycle especially is rich in myth because of the dramatic way the sun seems to move from east to west. Several peoples even saw the sun as a golden chariot, driven by a god across the sky. The aboriginal peoples of Australia were isolated on their island continent for at least 40,000 years, and they have some of the oldest myths in the world. These are mostly set in a mysterious age called the Dreamtime, when the world began and was peopled by animals and a race of giant ancestors. Different tribes of aborigines have different myths of the Dreamtime. This first one comes from a tribe who live in the Flinders Ranges of southern Australia. During the Dreamtime, two lizards were friends. One was a goanna (the Australian name for a large monitor lizard) and the other a gecko (a delightful little lizard with suction pads on its feet, with which it climbs up vertical surfaces). The friends discovered that some other friends of theirs had been massacred by the ‘sun-woman’ and her pack of yellow dingo dogs. Furious with the sun-woman, the big goanna hurled his boomerang at her and knocked her out of the sky. The sun vanished over the western horizon and the world was plunged into darkness. The two lizards panicked and tried desperately to knock the sun back into the sky, to restore the light. The goanna took another boomerang and hurled it westwards, to where the sun had disappeared. As you may know, boomerangs are remarkable weapons that come back to the thrower, so the lizards hoped that the boomerang would hook the sun back up into the sky. It didn’t. They then tried throwing boomerangs in all directions, in a vague hope of retrieving the sun. Finally, goanna lizard had only one boomerang left, and in desperation he threw it to the east, the opposite direction from where the sun had disappeared. This time, when it returned, it brought the sun with it. Ever since then, the sun has repeated the same pattern of disappearing in the west and reappearing in the east. Many myths and legends from all around the world have the same odd feature: a particular incident happens once, and then, for reasons never explained, the same thing goes on happening again and again for ever. Here’s another aboriginal myth, this time from southeastern Australia. Someone threw the egg of an emu (a sort of Australian ostrich) up into the sky. The sun hatched out of the egg and set fire to a pile of kindling wood which happened (for some reason) to be up there. The sky god noticed that the light was useful to men, and he told his servants to go out every night from then on, to put enough firewood in the sky to light up the next day. The longer cycle of the seasons is also the subject of myths all around the world. Native North American myths, like many others, often have animal characters. In this one, from the Tahltan people of western Canada, there was a quarrel between Porcupine and Beaver over how long the seasons ought to be. Porcupine wanted winter to last five months, so he held up his five fingers. But Beaver wanted winter to last for more months than that – the number of grooves in his tail. Porcupine was angry and insisted on an even shorter winter. He dramatically bit off his thumb and held up the remaining four fingers. And ever since then winter has lasted four months. I find this a rather disappointing myth, because it already assumes that there will be a winter and summer, and explains only how many months each will last. The Greek myth of Persephone is better in this respect at least. Persephone was the daughter of the chief god Zeus. Her mother ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:5:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"What *really* changes day to night, winter to summer? Whenever things change rhythmically with great precision, scientists suspect that either something is swinging like a pendulum or something is rotating: going round and round. In the case of our daily and seasonal rhythms, it’s the second. The seasonal rhythm is explained by the yearly orbiting of the Earth around the sun, at a distance of about 93 million miles. And the daily rhythm is explained by the Earth’s spinning round and round like a top. The illusion that the sun moves across the sky is just that – an illusion. It’s the illusion of relative movement. You will have met the same kind of illusion often enough. You are in a train, standing at a station next to another train. Suddenly you seem to start ‘moving’. But then you realize that you aren’t actually moving at all. It is the second train that is moving, in the opposite direction. I remember being intrigued by the illusion the first time I travelled in a train. (I must have been very young, because I also remember another thing I got wrong on that first train journey. While we were waiting on the platform, my parents kept saying things like ‘Our train will be coming soon’ and ‘Here comes our train’, and then ‘This is our train now’. I was thrilled to get on it because this was our train. I walked up and down the corridor, marvelling at everything, and very proud because I thought we owned every bit of it.) The illusion of relative movement works the other way, too. You think the other train has moved, only to discover that it is your own train that is moving. It can be hard to tell the difference between apparent movement and real movement. It’s easy if your train starts with a jolt, of course, but not if your train moves very smoothly. When your train overtakes a slightly slower train, you can sometimes fool yourself into thinking your train is still and the other train is moving slowly backwards. It’s the same with the sun and the Earth. The sun is not really moving across our sky from east to west. What is really happening is that the Earth, like almost everything in the universe (including the sun itself, by the way, but we can ignore that), is spinning round and round. Technically we say the Earth is spinning on its ‘axis’: you can think of the axis as a bit like an axle running right through the globe from North Pole to South Pole. The sun stays almost still relative to the Earth (not relative to other things in the universe, but I am just going to write about how it seems to us here, on Earth). We spin too smoothly to feel the movement, and the air we breathe spins with us. If it didn’t, we would feel it as a mighty rushing wind, because we spin at a thousand miles an hour. At least, that is the spin speed at the equator; obviously we spin more slowly as we approach the North or South Pole because the ground we’re standing on has less far to go to complete a circuit round the axis. Since we can’t feel the spinning of the planet, and the air spins with us, it’s like the case of the two trains. The only way we can tell we are moving is to look at objects that are not spinning with us: objects like the stars and the sun. What we see is the relative movement, and – just as with the trains – it looks as though we are standing still and the stars and the sun are moving across our sky. A famous thinker called Wittgenstein once asked a friend and pupil called Elizabeth Anscombe, ‘Why do people say it was natural to think that the sun went round the Earth rather than that the Earth turned on its axis?’ Miss Anscombe answered, ‘I suppose because it looked as if the sun went round the Earth.’ ‘Well,’ Wittgenstein replied, ‘what would it have looked like if it had looked as if the Earth turned on its axis?’ You try and answer that! If the Earth is spinning at a thousand miles an hour, why, when we jump straight up in the air, don’t we come down in a different place? Well, when you are on a train travelling at 100 mph,","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:5:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Working round the clock – and the calendar Night gives way to day, and day gives way to night, as the part of the world we happen to be standing on spins to face the sun, or spins into the shade. But almost as dramatic, at least for those of us who live far from the equator, is the seasonal change from short nights and long, hot days in summer to long nights and short, cold days in winter. The difference between night and day is dramatic – so dramatic that most species of animal can thrive either in the day or in the night but not both. They usually sleep during their ‘off’ period. Humans and most birds sleep by night and work at the business of living during the day. Hedgehogs and jaguars and many other mammals work by night and sleep by day. In the same way, animals have different ways of coping with the change between winter and summer. Lots of mammals grow a thick, shaggy coat for the winter, then shed it in spring. Many birds, and mammals too, migrate, sometimes huge distances, to spend the winter closer to the equator, then migrate back to the high latitudes (the far north or far south) for the summer, where the long days and short nights provide bumper feeding. A seabird called the Arctic tern carries this to an extreme. Arctic terns spend the northern summer in the Arctic. Then, in the northern autumn, they migrate south – but they don’t stop in the tropics, they go all the way to the Antarctic. Books sometimes describe the Antarctic as the ‘wintering grounds’ of the Arctic tern, but of course that’s nonsense: by the time they get to the Antarctic it is the southern summer. The Arctic tern migrates so far that it gets two summers: it has no ‘wintering grounds’ because it has no winter. I’m reminded of the joking remark of a friend of mine who lived in England during the summer, and went to tropical Africa to ‘tough out the winter’! Another way some animals avoid the winter is to sleep through it. It’s called ‘hibernation’, from hibernus, the Latin word for ‘wintry’. Bears and ground squirrels are among the many mammals, and quite a lot of other kinds of animals, that hibernate. Some animals sleep continuously through the whole winter; some sleep for most of the time, occasionally stirring into sluggish activity and then sleeping again. Usually their body temperature drops dramatically during hibernation and everything inside them slows down almost to a stop: their internal engines just barely tick over. There’s even a frog in Alaska which goes so far as to freeze solid in a block of ice, thawing out and coming to life again in the spring. Even those animals, like us, that don’t hibernate or migrate to avoid the winter have to adapt to the changing seasons. Leaves sprout in spring and fall in autumn (which is why it’s called the ‘fall’ in America), so trees that are a lush green in summer become gaunt and bare in winter. Lambs are born in spring, so they get the benefit of warm temperatures and new grass as they are growing. We may not grow long, woolly coats in winter, but we often wear them. So we can’t ignore the changing seasons, but do we understand them? Many people don’t. There are even some people who don’t understand that the Earth takes a year to orbit the sun – indeed, that’s what a year is! According to a poll, 19 per cent of British people think it takes a month, and similar percentages have been found in other European countries. Even among those who understand what a year means, there are many who think the Earth is closer to the sun in summer, more distant in winter. Tell that to an Australian, barbecuing Christmas dinner in a bikini on a baking hot beach! The moment you remember that in the southern hemisphere December is midsummer and June is midwinter, you realize that the seasons can’t be caused by changes in how close the Earth is to the sun. There has to be another explanation. We can’t get very far with that explanation until we have looked at what makes heavenly bodies orbit other heavenly bodies","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:5:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Into orbit Why do the planets stay in orbit around the sun? Why does anything stay in orbit around anything else? This was first understood in the seventeenth century by Sir Isaac Newton, one of the greatest scientists who ever lived. Newton showed that all orbits were controlled by gravity – the same force of gravity that pulls falling apples towards the ground, but on a larger scale. (Alas, the story that Newton got the idea when an apple bounced off his head is probably not really true.) Newton imagined a cannon on top of a very high mountain, with its barrel pointing horizontally out to sea (the mountain is on the coast). Each ball it fires seems to start off moving horizontally, but at the same time it is falling towards the sea. The combination of motion out over the sea and falling towards the sea results in a graceful downward curve, culminating in a splash. It is important to understand that the ball is falling all the time, even on the earlier, flatter part of the curve. It’s not that it travels flat horizontally for a while, then suddenly changes its mind like a cartoon character who realizes he ought to be falling and therefore starts doing so! The cannonball starts falling the moment it leaves the gun, but you don’t see the falling as downward motion because the ball is moving (nearly) horizontally as well, and quite fast. Now let’s make our cannon bigger and stronger, so that the cannonball travels many miles before it finally splashes into the sea. There is still a downward curve, but it’s a very gradual, very ‘flat’ curve. The direction of travel is pretty nearly horizontal for quite a lot of the way, but nevertheless it is still falling the whole time. Let’s carry on imagining a bigger and bigger cannon, more and more powerful: so powerful that the ball travels a really long way before it goes into the sea. Now the curvature of the Earth starts to make itself felt. The ball is still ‘falling’ the whole time, but because the planet’s surface is curved, ‘horizontal’ now starts to mean something a bit odd. The cannonball still follows a graceful curve, as before. But as it slowly curves towards the sea, the sea curves away from it because the planet is round. So it takes even longer for the cannonball finally to splash down into the sea. It is still falling all the time, but it is falling around the planet. You can see the way the argument is going. We now imagine a cannon so powerful that the ball keeps going all the way around the Earth till it arrives back where it started. It is still ‘falling’, but the curve of its fall is matched by the curvature of the Earth so that it goes right round the planet without getting any closer to the sea. It is now in orbit and it will keep on orbiting the Earth for an indefinite time, assuming that there is no air resistance to slow it down (which in reality there would be). It will still be ‘falling’, but the graceful curve of its prolonged fall will go all around the Earth, and around again and again. It will behave just like a miniature moon. In fact, that is what satellites are – artificial ‘moons’. They are all ‘falling’ but they never actually come down. The ones that are used for relaying long-distance telephone calls or television signals are in a special orbit called a geostationary orbit. This means that the rate at which they go around the Earth has been cunningly arranged so that it is exactly the same as the rate at which the Earth spins on its own axis: that is, they orbit the Earth once every 24 hours. This means, if you think about it, that they are always hovering above exactly the same spot on the Earth’s surface. That is why you can aim your satellite dish precisely at the particular satellite that is beaming down the television signal. When an object, such as a space station, is in orbit, it is ‘falling’ the whole time, and all the objects in the space station, whether we think of them as light or heavy, are falling at the same rate. This is a good place t","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:5:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Eggs, ellipses and escaping gravity Let’s go back to our cannon on the mountain-top, and make it more powerful still. What will happen? Well, now we need to acquaint ourselves with the discovery of the great German scientist Johannes Kepler, who lived just before Newton. Kepler showed that the graceful curve by which things orbit other things in space is not really a circle but something known to mathematicians since ancient Greek times as an ‘ellipse’. An ellipse is sort of egg-shaped (only ‘sort of’: eggs are not perfect ellipses). A circle is a special case of an ellipse; think of a very blunt egg, an egg so short and squat that it looks like a ping-pong ball. There’s an easy way to draw an ellipse, while at the same time convincing yourself that a circle is a special case of an ellipse. Take a piece of string and make it into a loop by tying the ends together, in as neat and small a knot as you can. Now stick a pin in a pad of paper, loop the string around the pin, stick a pencil through the other end of the loop, pull it tight and draw all around the pin with the string loop at full stretch. You’ll draw a circle, of course. Next, take a second pin and stick it in the pad, right next to the first pin so that they are touching. You’ll still draw a circle because the two pins are so close together that they count as a single pin. But now here’s the interesting part. Move the pins apart a few inches. Now when you draw with the string at full stretch, the shape you produce will not be a circle, it will be an ‘egg-shaped’ ellipse. The further apart you place the two pins, the narrower the ellipse will be. The closer you place the two pins to each other, the wider – the more circular – the ellipse will be until, when the two pins become one pin, the ellipse will be a circle – the special case. Now that we have met the ellipse we can go back to our super-powerful cannon. It has already fired a cannonball into an orbit which we assumed to be nearly circular. If we now make it more powerful still, what happens is that the orbit becomes a more ‘stretched’, less circular ellipse. This is called an ‘eccentric’ orbit. Our cannon ball zooms quite a long way from the Earth, then turns around and falls back. Earth is one of the two ‘pins’. The other ‘pin’ doesn’t really exist as a solid object, but you can think of it as an imaginary pin out there in space. The imaginary pin helps to make the mathematics understandable for some people but if it confuses you just forget about it. The important thing to realize is that the Earth is not in the centre of the ‘egg’. The orbit stretches much further away from the Earth on one side (the side of the ‘imaginary pin’) than on the other (the side where the Earth itself is the ‘pin’). We go on making our cannon more and more powerful. The cannonball is now travelling a long, long way from the Earth and is only just pulled back around to fall back towards Earth. The ellipse is now very long and stretched indeed. And there will eventually come a point where it ceases to be an ellipse altogether: we fire the cannonball even faster, and now the extra speed just pushes it beyond the point of no return, where the Earth’s gravity can’t summon it back. It has reached ‘escape velocity’ and disappears for ever (or until captured by the gravity of another body, such as the sun). Our increasingly powerful cannon has illustrated all the stages towards and beyond the establishment of an orbit. First the ball just flops into the sea. Then, as we fire successive balls with increasing force, the curve of their travel becomes increasingly horizontal until the ball reaches the necessary speed to go into a near-circular orbit (remember that a circle is a special case of an ellipse). Then, as the speed of firing increases more and more, the orbit becomes less circular and more elongated, more obviously elliptical. Finally, the ‘ellipse’ becomes so elongated that it ceases to be an ellipse at all: the ball reaches escape ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:5:4","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"A sideways look at summer Now that we understand orbits, we can go back to the question of why we have winter and summer. Some people, you’ll remember, wrongly think it is because we are closer to the sun in summer and further away in winter. That would be a good explanation if Earth had an orbit like Pluto’s. In fact Pluto’s winter and summer (both very much colder than anything we experience here) are caused in exactly that way. The Earth’s orbit, however, is almost circular, so the planet’s closeness to the sun cannot be what causes the changing seasons. For what it is worth, the Earth is actually closest to the sun (perihelion) in January and furthest (aphelion) in July, but the elliptical orbit is so close to circular that it makes no noticeable difference. Well then, what does cause the change from winter to summer? Something quite different. The Earth spins on an axis, and the axis is tilted. This tilting is the true reason why we have seasons. Let’s see how it works. As I said before, we could think of the axis as an axle, a rod running right through the globe and sticking out at the North Pole and the South Pole. Now think of the orbit of the Earth around the sun as a much larger wheel, with its own axle, this time running through the sun, and sticking out at the sun’s ‘north pole’ and the sun’s ‘south pole’. Those two axles could have been exactly parallel to each other, so that the Earth did not have a ‘tilt’ – in which case the noonday sun would always seem to be directly overhead at the equator, and day and night would be of equal length everywhere. There would be no seasons. The equator would be perpetually hot, and it would become colder and colder the further you moved away from the equator and towards either of the poles. You could get cool by moving away from the equator, but not by waiting for winter because there would be no winter to wait for. No summer, no seasons of any kind. In fact, however, the two axles are not parallel. The axle (axis) of the Earth’s own spinning is tilted relative to the axle (axis) of our orbit around the sun. The tilt is not particularly great – about 23.5 degrees. If it were 90 degrees (which is about the tilt of the planet Uranus) the North Pole would be pointing straight towards the sun at one time of year (which we can call the northern midsummer) and straight away from the sun at the northern midwinter. If Earth were like Uranus, in midsummer the sun would be overhead all the time at the North Pole (there’d be no night there), while it would be icy cold and dark at the South Pole, with no suggestion of day. And vice versa six months later. Since our planet is actually tilted at only 23.5 degrees instead of 90 degrees, we are about a quarter of the way from the no seasons extreme of no tilt at all towards the Uranus extreme of near total tilt. This is enough to mean that, as on Uranus, the sun never sets at the Earth’s North Pole in midsummer. It is perpetual day; but, unlike on Uranus, the sun is not overhead. It seems to loop around the sky as the Earth rotates, but it never quite dips below the horizon. That is true throughout the Arctic Circle. If you stood right on the Arctic Circle, say on the north-west tip of Iceland, on midsummer day, you’d see the sun skim along the northern horizon at midnight, but never actually set. Then it would loop around to its highest position (not very high) at midday. In northern Scotland, which is a little way outside the Arctic Circle, the midsummer sun dips below the horizon far enough to make a sort of night – but not a very dark night, because the sun is never very far below the horizon. So, the tilt of the Earth’s axis explains why we have winter (when the bit of the planet where we are is tilted away from the sun) and summer (when it’s tilted towards the sun), and why we have short days in winter and long days in summer. But does that explain why it is so cold in winter and so hot in summer? Why does the sun feel hotter when it is d","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:5:5","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"6 WHAT IS THE SUN? THE SUN IS so dazzlingly bright, so comforting in cold climates, so mercilessly scorching in hot ones, it is no wonder many peoples have worshipped it as a god. Sun worship often goes together with moon worship, and the sun and the moon are frequently regarded as being of opposite sex. The Tiv tribe of Nigeria and other parts of west Africa believe the sun is the son of their high god Awondo, and the moon is Awondo’s daughter. The Barotse tribe of south-east Africa think the sun is the moon’s husband rather than her brother. Myths often treat the sun as male and the moon as female, but it can be the other way around. In the Japanese Shinto religion the sun is the goddess Amaterasu, and the moon is her brother Ogetsuno. Those great civilizations that flourished in South and Central America before the Spaniards arrived in the sixteenth century worshipped the sun. The Inca of the Andes believed that the sun and the moon were their ancestors. The Aztecs of Mexico shared many of their gods with older civilizations in the area, such as the Maya. Several of these gods had a connection with the sun, or in some cases were the sun. The Aztec ‘Myth of the Five Suns’ held that there had been four worlds before the present one, each with its own sun. The earlier four worlds were destroyed, one after the other, by catastrophes, often engineered by the gods. The first sun was the god called Black Tezcatlipoca; he fought with his brother, Quetzalcoatl, who knocked him out of the sky with his club. After a period of darkness, with no sun, Quetzalcoatl became the second sun. In his anger, Tezcatlipoca turned all the people into monkeys, whereupon Quetzalcoatl blew all the monkeys away, and then resigned as the second sun. The god Tlaloc then became the third sun. Annoyed when Tezcatlipoca stole his wife Xochiquetzal, he sulked and refused to allow any rain to fall, so there was a terrible drought. The people begged and begged for rain, and Tlaloc became so fed up with their begging that he sent down a rain of fire instead. This burned up the world, and the gods had to start all over again. The fourth sun was Tlaloc’s new wife, Chalchiuhtlicue. She started out well, but then Tezcatlipoca so upset her that she cried tears of blood for 52 years without stopping. This completely flooded the world, and yet again the gods had to start from scratch. Isn’t it strange, by the way, how exactly myths specify little details? How did the Aztecs decide that she cried for 52 years, not 51 or 53? The fifth sun, which the Aztecs believed is the present one that we still see in the sky, was the god Tonatiuh, sometimes known as Huitzilopochtli. His mother, Coatlicue, gave birth to him after being accidentally impregnated by a bundle of feathers. This might sound odd, but such things would have seemed quite normal to people brought up with traditional myths (another Aztec goddess was impregnated by a gourd, which is the dried skin of a fruit like a pumpkin). Coatlicue’s 400 sons were so enraged to find their mother pregnant yet again that they tried to behead her. However, in the nick of time she gave birth to Huitzilopochtli. He was born fully armed and lost no time in killing all of his 400 half-brothers, except a few who escaped ‘to the south’. Huitzilopochtli then assumed his duties as the fifth sun. The Aztecs believed that they had to sacrifice human victims to appease the sun god, otherwise he would not rise in the east each morning. Apparently it didn’t occur to them to try the experiment of not making sacrifices, to see whether the sun might, just possibly, rise anyway. The sacrifices themselves were famously gruesome. By the end of the Aztecs’ heyday, when the Spaniards arrived (bringing their own brand of gruesomeness), the sun cult had escalated to a gory climax. It is estimated that between 20,000 and 80,000 humans were sacrificed for the rededication of the Great Temple of Tenochititlan in 1487. Various gifts could be offered to ap","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:6:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"What is the sun, really? The sun is a star. It’s no different from lots of other stars, except that we happen to be near it so it looks much bigger and brighter than the others. For the same reason, the sun, unlike any other star, feels hot, damages our eyes if we look straight at it, and burns our skin red if we stay out in it too long. It is not just a little bit nearer than any other star; it is vastly nearer. It is hard to grasp how far away the stars are, how big space is. Actually, it’s more than hard, it’s almost impossible. There’s a lovely book called Earthsearch by John Cassidy, which makes an attempt to grasp it, using a scale model. 1 Go out into a big field with a football and plonk it down to represent the sun. 2 Then walk 25 metres away and drop a peppercorn to represent the Earth’s size and its distance from the sun. 3 The moon, to the same scale, would be a pinhead, and it would be only 5 centimetres away from the peppercorn. 4 But the nearest other star, Proxima Centauri, to the same scale, would be another (slightly smaller) football located about . . . wait for it . . . six and a half thousand kilometres away! There may or may not be planets orbiting Proxima Centauri, but there certainly are planets orbiting other stars, maybe most stars. And the distance between each star and its planets is usually small compared to the distance between the stars themselves. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:6:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"How stars work The difference between a star (like the sun) and a planet (like Mars or Jupiter) is that stars are bright and hot, and we see them by their own light, whereas planets are relatively cold and we see them only by reflected light from a nearby star, which they are orbiting. And that difference, in turn, results from the difference in size. Here’s how. The larger any object is, the stronger the gravitational pull towards its centre. Everything pulls everything by gravity. Even you and I exert a gravitational pull on each other. But the pull is too weak to notice unless at least one of the bodies concerned is large. The Earth is large, so we feel a strong pull towards it, and when we drop something it falls ‘downwards’ – that is, towards the centre of the Earth. A star is much larger than a planet like Earth, so its gravitational pull is much stronger. The middle of a large star is under huge pressure because a gigantic gravitational force is pulling all the stuff in the star towards the centre. And the greater the pressure inside a star, the hotter it gets. When the temperature gets really high – much hotter than you or I can possibly imagine – the star starts to behave like a sort of slow-acting hydrogen bomb, giving out huge quantities of heat and light, and we see it shining brightly in the night sky. The intense heat tends to make the star swell up like a balloon, but at the same time gravity pulls it back in again. There is a balance between the outward push of the heat and the inward pull of gravity. The star acts as its own thermostat. The hotter it gets, the more it swells; and the bigger it gets, the less concentrated the mass of matter in the centre becomes, so it cools down a bit. This means it starts to shrink again, and that heats it up again, and so on. That sounds as though the star bounces in and out like a heart beating, but it isn’t like that. Instead, it settles into an intermediate size, which keeps the star at just the right temperature to stay that way. I began by saying that the sun is just a star like many others, but actually there are lots of different kinds of stars, and they come in a great range of sizes. Our sun is not very big, as stars go. It is slightly bigger than Proxima Centauri, but much smaller than lots of other stars. What is the largest star we know? That depends on how you measure them. The star that measures the greatest distance across is called VY Canis Majoris. From side to side (diameter), it is 2,000 times the size of the sun. And the sun’s diameter is 100 times that of the Earth. However, VY Canis Majoris is so wispy and light that, despite its huge size, its mass is only about 30 times that of the sun, instead of the billions of times it would be if its material were equally dense. Others, such as the Pistol Star, and more recently discovered stars such as Eta Carinae and R136a1 (not a very catchy name!), are 100 times as massive as the sun, or even more. And the sun is more than 300,000 times the mass of the Earth, which means that the mass of Eta Carinae is 30 million times that of the Earth. If a giant star like R136a1 has planets, they must be very very far away from it, or they would be instantly burned to vapour. Its gravity is so huge (because of its vast mass) that its planets could indeed be a very long way away and still be held in orbit around it. If there is such a planet, and anybody lives on it, R136a1 would probably look about as big to them as our sun looks to us, because although it is much larger, it would also be much further away – just the right distance away, in fact, and just the right apparent size to sustain life, otherwise life wouldn’t be there! ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:6:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"The life story of a star Actually, however, it is unlikely that there are any planets orbiting R136a1, let alone any life on them. The reason is that extremely large stars have a very short life. R136a1 is probably only about a million years old, which is less than a thousandth of the age of the sun so far: not enough time for life to evolve. The sun is a smaller, more ‘mainstream’ star: the kind of star that has a life story lasting billions of years (not just millions), during which it proceeds through a series of drawn-out stages, rather like a child growing up, becoming an adult, passing through middle age, eventually getting old and dying. Mainstream stars mostly consist of hydrogen, the simplest of all the elements. The ‘slow-acting hydrogen bomb’ in the interior of a star converts hydrogen to helium, the second simplest element (something else named after the Greek sun god Helios), releasing a massive amount of energy in the form of heat, light and other kinds of radiation. You remember we said that the size of a star is a balance between the outward push of heat and the inward pull of gravity? Well, this balance stays roughly the same, keeping the star simmering away for several billions of years, until it starts to run out of fuel. What usually happens then is that the star collapses into itself under the unrestrained influence of gravity – at which point all hell breaks loose (if it’s possible to imagine anything more hellish than the interior of a star already is). The life story of a star is too long for astronomers to see more than a tiny snapshot of it. Fortunately, as they scan the skies with their telescopes, astronomers can find a range of stars, each at a different stage of its development: some ‘infant’ stars caught in the act of being formed from clouds of gas and dust, as our sun was four and a half billion years ago; plenty of ‘middle-aged’ stars like our sun; and some old and dying stars, which give a foretaste of what will happen to our sun in another few billion years’ time. Astronomers have built up a rich ‘zoo’ of stars, of all different sizes and stages in their life cycles. Each member of the ‘zoo’ shows what others used to be like, or will be like. An ordinary star like our sun eventually runs out of hydrogen and, as I’ve just described, starts ‘burning’ helium instead (I’ve put that in quotation marks because it isn’t really burning but doing something much hotter). At this stage it is called a ‘red giant’. The sun will become a red giant in about five billion years’ time, which means it is pretty much in the middle of its life cycle at the moment. Long before then, our poor little planet will have become much too hot to live on. In two billion years the sun will be 15 per cent brighter than it is now, which means that the Earth will be like Venus is today. Nobody could live on Venus: the temperature there is over 400 degrees Celsius. But two billion years is a pretty long time, and humans will almost certainly be extinct long before then, so that there will be nobody left to fry. Or maybe our technology will have advanced to the point where we can actually move the Earth out to a more comfortable orbit. Later, when the helium, too, runs out, the sun will mostly disappear in a cloud of dust and debris, leaving a tiny core called a white dwarf, which will cool and fade. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:6:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Supernovas and stardust The story ends differently for stars that are much bigger and hotter than our sun, like the giant stars we were just talking about. These monsters ‘burn’ through their hydrogen much faster, and their ‘hydrogen bomb’ nuclear furnaces go further than just banging hydrogen nuclei together to make helium nuclei. The hotter furnaces of larger stars go on to bang helium nuclei together to make even heavier elements, and so on until they have produced a wide range of heavier atoms. These heavier elements include carbon, oxygen, nitrogen and iron (but so far nothing heavier than iron): elements that are abundant on Earth, and in all of us. After a relatively short time, a very large star like this eventually destroys itself in a gigantic explosion called a supernova, and it is in these explosions that elements heavier than iron are formed. What if Eta Carinae were to explode as a supernova tomorrow? That would be the mother of all explosions. But don’t worry: we wouldn’t know about it for another 8,000 years, which is how long it takes light to travel the vast distance between Eta Carinae and us (and nothing travels faster than light). What, then, if Eta Carinae exploded 8,000 years ago? Well, in that case the light and other radiation from the explosion really could reach us any day now. The moment we see it, we’ll know that Eta Carinae blew up 8,000 years ago. Only about 20 supernovas have been seen in recorded history. The great German scientist Johannes Kepler saw one on 9 October 1604: the debris has expanded since he first saw it. The explosion itself actually occurred some 20,000 years earlier, roughly the time the Neanderthal people went extinct. Supernovas, unlike ordinary stars, can create elements even heavier than iron: lead, for example, and uranium. The titanic explosion of a supernova scatters all the elements that the star, and then the supernova, have made, including the elements necessary for life, far and wide through space. Eventually the clouds of dust, rich in heavy elements, will start the cycle again, condensing to make new stars and planets. That is where the matter in our planet came from, and that is why our planet contains the elements that are needed to make us, the carbon, nitrogen, oxygen and so on: they come from the dust that remained after a long-gone supernova lit up the cosmos. That is the origin of the poetic phrase ‘We are stardust’. It is literally true. Without occasional (but very rare) supernova explosions, the elements necessary for life would not exist. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:6:4","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Going round and around It is a fact we cannot ignore that the Earth and all the sun’s other planets orbit their star in the same ‘plane’. What does that mean? Theoretically, you might think that the orbit of one planet could be tilted at any angle to any other. But that is not the way things are. It is as though there is an invisible flat disc in the sky, with the sun at the centre, and all the planets moving on that disc, just at different distances from the centre. What’s more, the planets all go round the sun in the same direction. Why? It is probably because of how they began. Let’s take the direction of spin first. The whole solar system, which means the sun and the planets, began as a slowly spinning cloud of gas and dust, probably the leftovers of a supernova explosion. Like almost every other free-floating object in the universe, the cloud was spinning on its own axis. And yes, you’ve guessed it: the direction of its spin was the same as the direction of the planets now orbiting the sun. Now, why are all the planets ‘on the level’ on that flat ‘disc’? For complicated gravitational reasons that I won’t go into, but which scientists understand well, a big spinning cloud of gas and dust out in space tends to form itself into a revolving disc, with a massive lump in the middle. And that is what seems to have happened with our solar system. Dust and gas and small chunks of matter don’t stay as gas and dust. Gravitational attraction pulls them towards their neighbours, in the way I described earlier in this chapter. They join forces with those neighbours and form larger lumps of matter. The larger a lump, the greater its gravitational pulling power. So, what happened in our spinning disc was that the larger lumps became even larger, as they sucked in their smaller neighbours. By far the largest lump became the sun in the centre. Other lumps, large enough to attract smaller lumps to them and far enough from the sun not to be sucked into it, became the planets. Reading from nearest the sun outwards, we now call them Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune. Old lists would put Pluto after Neptune, but nowadays it is regarded as too small to count as a planet. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:6:5","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Asteroids and shooting stars Under different circumstances another planet could have formed too, between the orbits of Mars and Jupiter. But the small bits that could otherwise have joined together to make this extra planet were prevented from doing so, probably by the brooding gravitational presence of Jupiter, and they have remained as an orbiting ring of debris called the asteroid belt. These asteroids swarm in a ring between the orbits of Mars and Jupiter, which is where the extra planet would have been if they had managed to get together. The famous rings around the planet Saturn are there for a similar reason. They could have condensed together to make another moon (Saturn already has 62 moons, so this would have been the 63rd), but they actually stayed separate as a ring of rocks and dust. In the asteroid belt – the sun’s equivalent of Saturn’s rings – some of the bits of debris are large enough to be called planetesimals (sort of ‘not quite planets’). The largest of them, called Ceres, is nearly 1,000 kilometres across, large enough to be roughly spherical like a planet, but most of them are just misshapen rocks and bits of dust. They collide with each other from time to time, like billiard balls, and sometimes one of them gets kicked out of the asteroid belt and may even come close to another planet such as Earth. We see them, quite commonly, burning in the upper atmosphere as ‘shooting stars’ or ‘meteors’. Less commonly, a meteor may be large enough to survive the ordeal of passing through the atmosphere and actually make a crash landing. On 9 October 1992, a meteor broke up in the atmosphere and a fragment about the size of a large brick hit a car in Peekskill, New York State. A much larger meteor, the size of a house, exploded above Siberia on 30 June 1908, setting fire to large areas of forest. Scientists now have evidence that an even larger meteor hit Yucatán, in what is now Central America, 65 million years ago, causing a global disaster, which is probably what killed off the dinosaurs. It has been calculated that the energy released by this catastrophic collision was hundreds of times greater than would be released if all the nuclear weapons in the world were simultaneously exploded in Yucatán. There would have been shattering earthquakes, epic tsunamis and worldwide forest fires, and a dense cloud of dust and smoke would have darkened the Earth’s surface for years. This would have starved the plants, which need sunlight, and starved the animals, which need plants. The wonder is not that the dinosaurs died but that our mammal ancestors survived. Perhaps a tiny population survived by hibernating underground. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:6:6","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Light of our lives I want to end this chapter by talking about the importance of the sun for life. We don’t know whether there is life elsewhere in the universe (I’ll discuss that question in a later chapter), but we do know that, if there is life out there, it is almost certainly near a star. We can also say that, if it is anything like our kind of life, at least, it will probably be on a planet about the same apparent distance from its star as we are from our sun. By ‘apparent distance’ I mean distance as perceived by the life form itself. The absolute distance could be very much greater, as we saw in the example of the super-giant star R136a1. But if the apparent distance were the same, their sun would look about the same size to them as ours does to us, which would mean that the amount of heat and light received from it would be about the same. Why does life have to be close to a star? Because all life needs energy, and the obvious source of energy is starlight. On Earth, plants gather sunlight and make its energy available to all other living creatures. Plants could be said to feed off sunlight. They need other things too, such as carbon dioxide from the air, and water and minerals from the ground. But they get their energy from sunlight, and they use it to make sugars, which are a kind of fuel that drives everything else that they need to do. You can’t make sugar without energy. And once you have sugar, you can then ‘burn’ it to get the energy back out again – though you never get all of the energy back; there is always some lost in the process. And when we say ‘burn’, that doesn’t mean it goes up in smoke. Literally burning it is only one way to release the energy in a fuel. There are more controlled ways to let the energy trickle out, slowly and usefully. You can think of a green leaf as a low, spread-out factory whose entire flat roof is one great solar panel, trapping sunlight and using it to drive the wheels of the assembly lines under the roof. That is why leaves are thin and flat – to give them a large surface area for sunlight to fall on. The end products of the factory are sugars of various kinds. These are then piped through the veins in the leaf to the rest of the plant, where they are used to make other things, like starch, which is a more convenient way to store energy than sugar. Eventually, the energy is released from the starch or sugar to make all the other parts of the plant. When plants are eaten by herbivores (which means just that: ‘plant-eaters’), such as antelopes or rabbits, the energy is passed to the herbivores – and again, some of it is lost in the process. The herbivores use it to build up their bodies and fuel their muscles as they go about their business. Their business includes, of course, grazing or browsing on lots more plants. The energy that powers the muscles of the herbivores as they walk and munch and fight and mate comes ultimately from the sun, via plants. Then other animals – meat-eaters or ‘carnivores’ – come along and eat the herbivores. The energy is passed on yet again (and yet again some of it is lost in the transition), and it powers the muscles of the carnivores as they go about their business. In this case, their business includes hunting down yet more herbivores to eat, as well as all the other things they do, like mating and fighting and climbing trees and, in the case of mammals, making milk for their babies. Still, it is the sun that ultimately provides the energy, even though by now that energy has reached them by a very indirect route. And at every stage of that indirect route, a good fraction of the energy is lost – lost as heat, which contributes to the useless task of heating up the rest of the universe. Other animals, parasites, feed on the living bodies of both herbivores and carnivores. Once again, the energy that powers the parasites comes ultimately from the sun, and once again not all of it is used because some of it is wasted as heat. Finally, when anything","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:6:7","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"7 WHAT IS A RAINBOW? THE EPIC OF Gilgamesh is one of the oldest stories ever written. Older than the legends of the Greeks or the Jews, it is the ancient heroic myth of the Sumerian civilization, which flourished in Mesopotamia (now Iraq) between 5,000 and 6,000 years ago. Gilgamesh was the great hero king of Sumerian myth – a bit like King Arthur in British legends, in that nobody knows whether he actually existed, but lots of stories were told about him. Like the Greek hero Odysseus (Ulysses) and the Arabian hero Sinbad the Sailor, Gilgamesh went on epic travels, and he met many strange things and people on his journeys. One of them was an old man (a very, very old man, centuries old) called Utnapashtim, who told Gilgamesh a strange story about himself. Well, it seemed strange to Gilgamesh, but it may not seem so strange to you because you have probably heard a similar story . . . about another old man with a different name. Utnapashtim told Gilgamesh of an occasion, many centuries earlier, when the gods were angry with humankind because we made so much noise they couldn’t sleep. The chief god, Enlil, suggested that they should send a great flood to destroy everybody, so the gods could get a good night’s rest. But the water god, Ea, decided to warn Utnapashtim. Ea told Utnapashtim to tear down his house and build a boat. It would have to be a very big boat, because Utnapashtim was to take into it ‘the seed of all living creatures’. Utnapashtim built the boat just in time, before it rained for six days and six nights without stopping. The flood that followed drowned everybody and everything that was not safely inside the boat. On the seventh day the wind dropped and the waters grew calm and flat. Utnapashtim opened a hatch in the tightly sealed boat and released a dove. The dove flew away, looking for land, but failed to find any and returned. Then Utnapashtim released a swallow, but the same thing happened. Finally Utnapashtim released a raven. The raven didn’t come back, which suggested to Utnapashtim that there was dry land somewhere and the raven had found it. Eventually the boat came to rest on a mountaintop poking out of the water. Another god, Ishtar, created the first rainbow, as a token of the gods’ promise to send no more terrible floods. So that is how the rainbow came into being, according to the ancient legend of the Sumerians. Well, I said the story would be familiar. All children reared in Christian, Jewish or Islamic countries will immediately recognize that it is the same as the more recent story of Noah’s Ark, with one or two minor differences. The name of the boat-builder changes from Utnapashtim to Noah. The many gods of the older legend turn into the one god of the Jewish story. The ‘seed of all living creatures’ comes to be spelled out as ‘every living thing of all flesh, two of every sort’ – or, as the song has it, ‘the animals went in two by two’ – and the Epic of Gilgamesh surely meant something similar. In fact, it is obvious that the Jewish story of Noah is nothing more than a retelling of the older legend of Utnapashtim. It was a folk tale that got passed around, and it travelled down the centuries. We often find that seemingly ancient legends have come from even older legends, usually with some names or other details changed. And this one, in both versions, ends with the rainbow. In both the Epic of Gilgamesh and the Book of Genesis, the rainbow is an important part of the myth. Genesis specifies that it was actually God’s bow, which he put up in the sky as a token of his promise to Noah and his descendants. There is one more difference between the Noah story and the earlier Sumerian tale of Utnapashtim. In the Noah version, the reason for God’s discontent with humans was that we were all incurably wicked. In the Sumerian story, humanity’s crime was, you might think, less serious. We simply made so much noise the gods couldn’t get to sleep! I think it’s funny. And the theme of noisy humans keeping","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:7:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"The real magic of the rainbow When I was about ten, I was taken to London to see a children’s play called Where the Rainbow Ends. You almost certainly won’t have seen it because it is too unfashionably patriotic for modern theatres to perform. It is all about how exceptionally special it is to be English, and at the climax of the adventure the children are rescued by St George, the patron saint of England (not Britain, for Scotland, Wales and Ireland have their own patron saints). But what I most vividly remember is not St George but the rainbow itself. The children actually went to the place where the rainbow planted its foot, and we saw them walking about in the middle of the rainbow where it hit the ground. It was cleverly staged, with coloured spotlights beaming down through swirling mist, and the children stumbled about in a spellbound daze. I think it was at about this moment that the shining-armoured, silver-helmeted St George appeared, and we children gasped at the scene as the children on the stage shouted: ‘St George! St George! St George!’ But it was the rainbow itself that seized my imagination. Never mind St George: how wonderful it must be to stand right in the foot of a giant rainbow! You can see where the author of the play got the idea. A rainbow really does look like a proper object, hanging out there, perhaps a few miles away. It seems to have its left foot planted, say, in a wheat field and its right foot (if you are lucky enough to see a complete rainbow) on a hilltop. You feel you ought to be able to go straight to it and stand right where the rainbow steps on the ground, like the children in the play. All the myths I have described to you have the same idea. The rainbow is seen as a definite thing, in a definite place, a definite distance away. Well, you’ll probably have worked out that it isn’t really like that! First, if you try to approach the rainbow, no matter how fast you run, you’ll never get there: the rainbow will run away from you until it fades away altogether. You can’t catch it. But it isn’t really running away because it isn’t really in a particular place at all, ever. It’s an illusion – but a fascinating illusion, and understanding it leads on to all sorts of interesting things, some of which we’ll come to in the next chapter. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:7:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"What light is made of First, we need to understand about something called the spectrum. It was discovered in the time of King Charles II – that’s about 350 years ago – by Isaac Newton, who may well have been the greatest scientist ever (he discovered lots of other things besides the spectrum, as we saw in the chapter on night and day). Newton discovered that white light is really a mixture of all the different colours. To a scientist, that’s what white means. How did Newton find this out? He set up an experiment. First he blacked out his room so that no light could get in, and then he opened a narrow chink in the curtain, so that a pencil-thin beam of white sunlight came in. He then let the beam of light pass through a prism, which is a sort of triangular chunk of glass. What a prism does is splay the narrow white beam out; but the splayed-out beam that emerges from the prism is no longer white. It is multicoloured like a rainbow, and Newton gave a name to the rainbow he made: the spectrum. Here’s how it works. When a beam of light travels through air and hits glass, it gets bent. The bending is called refraction. Refraction doesn’t have to be caused by glass: water does the trick too, and that will be important when we come back to the rainbow. It is refraction that makes an oar look bent when you stick it in the river. But now here’s the point. The angle at which light bends is slightly different depending on what colour the light is. Red light bends at a shallower angle than blue light. So, if white light really is a mixture of coloured lights, as Newton guessed, what’s going to happen when you bend white light through a prism? The blue light is going to bend further than the red light, so they will be separated from each other when they emerge from the other side of the prism. And the yellow and green lights will come out in between. The result is Newton’s spectrum: all the colours of the rainbow, arranged in the correct rainbow order – red, orange, yellow, green, blue, violet. Newton wasn’t the first person to make a rainbow with a prism. Other people had already got the same result. But many of them thought the prism somehow ‘coloured’ the white light, like adding a dye. Newton’s idea was quite different. He thought that white light was a mixture of all the colours, and the prism was just separating them from each other. He was right, and he proved it with a pair of neat experiments. First, he took his prism, as before, and stuck a narrow slit in the way of the coloured beams coming out of it, so that only one of them, say the red beam, passed through the slit. Then he put another prism in the path of this narrow beam of red light. The second prism bent the light, as usual. But what came out of it was only red light. No extra colours were added, as they would have been if what prisms did was add colour like a dye. The result Newton got was exactly what he expected, supporting his theory that white light is a mixture of light of all colours. The second experiment was more ingenious still, using three prisms. It was called Newton’s Experimentum Crucis, which is Latin for ‘critical experiment’ – or, as we might say, ‘experiment that really clinches the argument’. White light passed through a slit in Newton’s curtain and through the first prism, which spread it out into all the colours of the rainbow. The spread-out rainbow colours then passed through a lens, which brought them all together before they passed through the second of Newton’s prisms. This second prism had the effect of merging the rainbow colours back into white light again. That already neatly proved Newton’s point. But just to make quite sure, he then passed the beam of white light through a third prism, which splayed the colours out into a rainbow again! As neat a demonstration as you could wish for, proving that white light is indeed a mixture of all the colours. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:7:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"How raindrops make rainbows Prisms are all very well, but when you see a rainbow in the sky, there isn’t a great big prism hanging up there. No, but there are millions of raindrops. So, does each raindrop act as a tiny prism? It is a bit like that, but not quite. If you want to see a rainbow you have to have the sun behind you when you look at a rainstorm. Each raindrop is more like a little ball than a prism, and light behaves differently when it hits a ball from how it behaves when it hits a prism. The difference is that the far side of a raindrop acts as a tiny mirror. And that is why you need the sun behind you if you want to see a rainbow. The light from the sun turns a somersault inside every raindrop and is reflected backwards and downwards, where it hits your eyes. Here’s how it works. You are standing with the sun behind and above you, looking at a distant shower of rain. The sunlight hits a single raindrop (of course it hits lots of other raindrops too, but wait, we’re coming to that). Let’s call our one particular raindrop A. The beam of white light hits A on its upper near surface, where it is bent, just as it was on the near surface of Newton’s prism. And of course the red light bends less than the blue, so the spectrum is already sorting itself out. Now all the coloured beams travel through the raindrop until they hit the far side. Instead of passing through into the air, they are reflected back towards the near side of the raindrop, this time the lower part of the near side. And as they pass through the near side of the raindrop, they are again bent. Again the red light bends less than the blue. So, as the sunbeam leaves the raindrop, it has been splayed out into a proper little spectrum. The separated coloured beams, having doubled back around the inside of the raindrop, are now hurtling back in the general direction of where you are standing. If your eye happens to be in the path of one of those beams, say the green one, you’ll see pure green light. Somebody shorter than you might see the red beam coming from A. And somebody taller than you might see the blue beam from A. Nobody sees the full spectrum from any one raindrop. Each of you sees only one pure colour. Yet all of you say you see a rainbow, with all the colours. How come? Well, so far, we have only been talking about one raindrop, called A. There are millions of other raindrops, and they are all behaving in the same kind of way. While you are looking at A’s red beam, there is another raindrop called B, which is lower than A. You don’t see B’s red beam because it hits you in the stomach. But B’s blue beam is in exactly the right place to hit you in the eye. And there are other raindrops lower than A but higher than B, whose red and blue beams miss your eye but whose yellow or green beams hit your eye. So lots of raindrops together add up to a complete spectrum, in a line, up and down. But a line up and down is not a rainbow. Where does the rest of the rainbow come from? Don’t forget that there are other raindrops, stretching from one side of the rain shower to the other and at all heights. And of course they fill in the rest of the rainbow for you. Every rainbow you see, by the way, is trying to be a complete circle, with your eye at the centre of it – like the complete circular rainbow you sometimes see when you water the garden with a hose and the sun shines through the spray. The only reason we don’t usually see the whole circle is that the ground gets in the way. So that’s why you see a rainbow at any one split second. But in the next split second, all the raindrops have fallen to a lower position. A has now fallen to where B was, so you now see A’s blue beam instead of its green one. And you can’t see any of B’s beams (although the dog at your feet can). And a new raindrop (C, whose beams you couldn’t see at all before) has now fallen into the place where A was, and you now see its red beam. That’s why a rainbow seems to stay still, although the r","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:7:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"On the right wavelength? Let’s now look at what the spectrum – the ordered range of colours from red through orange, yellow, green and blue to violet – really is. What is it about red light that makes it bend at a shallower angle than blue light? Light can be thought of as vibrations: waves. Just as sound is vibrations in the air, light consists of what are called electromagnetic vibrations. I won’t try to explain what electromagnetic vibrations are because it takes too long (and I’m not sure that I entirely understand it myself). The point here is that although light is very different from sound, we can talk about high-frequency (short-wavelength) and low-frequency (long-wavelength) vibrations in light, just as we can for sound. High-pitched sound – treble or soprano – means high-frequency, or short-wavelength, vibrations. Low-frequency, or long-wavelength, sounds are deep, bass sounds. The equivalent for light is that red (long wavelength) is the bass, yellow the baritone, green the tenor, blue the alto and violet (short wavelength) the treble. There are sounds that are too high-pitched for us to hear. They are called ultrasound; bats can hear them and use the echoes for finding their way around. There are also sounds that are too low for us to hear. They are called infrasound; elephants, whales and some other animals use these deep rumbles for keeping in touch with each other. The deepest bass notes on a big cathedral organ are almost too low to hear: you seem to ‘feel’ them fluttering your whole body. The range of sounds that we humans can hear is a band of frequencies in the middle, between ultrasound, which is too high for us (but not bats) to hear, and infrasound, which is too low for us (but not elephants) to hear. And the same is true of light. The colour equivalent of ultrasound bat squeaks is ultraviolet, which means ‘beyond violet’. Although we can’t see ultraviolet light, insects can. There are some flowers that have stripes or other patterns for luring insects in to pollinate them, patterns that can only be seen in the ultraviolet range of wavelengths. Insect eyes can see them, but we need instruments to ‘translate’ the patterns into the visible part of the spectrum. For example, the evening primrose flower looks yellow to us, with no pattern, no stripes. But if you photograph it in ultraviolet light you suddenly see a starburst of stripes. The spectrum goes into higher and higher frequencies, far beyond ultraviolet, far beyond what even insects can see. X-rays could be thought of as ‘light’ of even higher ‘pitch’ than ultraviolet. And gamma rays are even higher still. At the other end of the spectrum, insects can’t see red, but we can. Beyond red is ‘infrared’, which we can’t see, although we can feel it as heat (and some snakes are especially sensitive to it, using it to detect their prey). A bee might call red ‘infra-orange’. Deeper ‘bass notes’ than infrared are microwaves, which you use to cook things. And even deeper bass (longer wavelength) are radio waves. What is a bit surprising is that the light we humans can actually see – the spectrum or ‘rainbow’ of visible colours between the slightly ‘higher-pitched’ violet and the slightly ‘lower-pitched’ red – is a very tiny band in the middle of a huge spectrum ranging from gamma rays at the high-pitched end to radio waves at the low-pitched end. Almost the whole of the spectrum is invisible to our eyes. The sun and the stars are pumping out electromagnetic rays at a full range of frequencies or ‘pitches’, all the way from radio waves at the ‘bass’ end to gamma rays at the ‘treble’ end. Although we can’t see outside the tiny band of visible light, from red to violet, we have instruments that can detect these invisible rays. Scientists called radio astronomers take ‘photographs’ of stars using radio waves rather than light waves or X-rays. The instrument they use is called a radio telescope. Other scientists take photographs of the sky at the other end of the sp","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:7:4","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"8 WHEN AND HOW DID EVERYTHING BEGIN? LET’S START with an African myth from a Bantu tribe, the Boshongo of the Congo. In the beginning there was no land, just watery darkness, and also – importantly – the god Bumba. Bumba got a stomach-ache and vomited up the sun. Light from the sun dispelled the darkness, and heat from the sun dried up some of the water, leaving land. Bumba’s stomach-ache still hadn’t gone, though, so he then sicked up the moon, the stars, animals and people. Many Chinese origin myths involve a character called Pan Gu, sometimes depicted as a giant hairy man with a dog’s head. Here’s one of the Pan Gu myths. In the beginning there was no clear distinction between Heaven and Earth: it was all one gooey mess surrounding a big black egg. Curled up inside the egg was Pan Gu. Pan Gu slept inside the egg for 18,000 years. When he finally awoke he wanted to escape, so he picked up his axe and hewed his way out. Some of the contents of the egg were heavy and sank to become the Earth. Some of them were light and floated up to become the sky. The Earth and the sky then swelled at a rate of (the equivalent of) 3 metres a day for another 18,000 years. Some versions of the story have Pan Gu pushing the sky and the Earth apart, after which he was so exhausted that he died. Various bits of him then became the universe that we know. His breath became the wind, his voice became thunder; his two eyes became the moon and the sun, his muscles farmland and his veins roads. His sweat became rain, and his hairs became stars. Humans are descended from the fleas and lice that once lived on his body. By the way, the story of Pan Gu pushing the sky and the Earth apart is rather like the (probably unrelated) Greek myth of Atlas, who also held up the sky (although, weirdly, pictures and statues usually show him carrying the whole Earth on his shoulders). Now here is one of many origin myths from India. Before the beginning of time there was a great dark ocean of nothingness, with a giant snake coiled up on the surface. Sleeping in the coils of the snake was Lord Vishnu. Eventually Lord Vishnu was awakened by a deep humming sound from the bottom of the ocean of nothingness, and a lotus plant grew out of his navel. In the middle of the lotus flower sat Brahma, Vishnu’s servant. Vishnu commanded Brahma to create the world. So Brahma did just that. No problem! And all living creatures too, while he was about it. Easy! What I find a little disappointing about all these origin myths is that they begin by assuming the existence of some kind of living creature before the universe itself came into being – Bumba or Brahma or Pan Gu, or Unkulukulu (the Zulu creator) or Abassie (Nigeria) or ‘Old Man in the Sky’ (Salish, a tribe of native Americans from Canada). Wouldn’t you think that a universe of some kind would have to come first, to provide a place for the creative spirit to go to work? None of the myths gives any explanation for how the creator of the universe himself (and it usually is a he) came into existence. So they don’t get us very far. Let’s turn instead to what we know of the true story of how the universe began. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:8:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"How did everything begin, really? Do you remember from Chapter 1 that scientists work by setting up ‘models’ of how the real world might be? They then test each model by using it to make predictions of things that we ought to see – or measurements that we ought to be able to make – if the model were correct. In the middle of the twentieth century there were two competing models of how the universe came into being, called the ‘steady state’ model and the ‘big bang’ model. The steady state model was very elegant, but eventually turned out to be wrong – that is, predictions based on it were shown to be false. According to the steady state model, there never was a beginning: the universe had always existed in pretty much its present form. The big bang model, on the other hand, suggested that the universe began at a definite moment in time, in a strange kind of explosion. The predictions made on the basis of the big bang model keep turning out to be right, and so it has now been generally accepted by most scientists. According to the modern version of the big bang model, the entire observable universe exploded into existence between 13 and 14 billion years ago. Why do we say ‘observable’? The ‘observable universe’ means everything for which we have any evidence at all. It is possible that there are other universes that are inaccessible to all our senses and instruments. Some scientists speculate, perhaps fancifully, that there may be a ‘multiverse’: a bubbling ‘foam’ of universes, of which our universe is only one ‘bubble’. Or it may be that the observable universe – the universe in which we live, and the only universe for which we have direct evidence – is the only universe there is. Either way, in this chapter we are limiting ourselves to the observable universe. The observable universe seems to have begun in the big bang, and this remarkable event happened just under 14 billion years ago. Some scientists will tell you that time itself began in the big bang, and we should no more ask what happened before the big bang than we should ask what is north of the North Pole. You don’t understand that? Nor do I. But I do understand, sort of, the evidence that the big bang happened, and when. That is what this chapter is about. First, I need to explain what a galaxy is. We’ve already seen, in our analogy with footballs in Chapter 6 , that the stars are spaced out at incredibly huge distances from one another compared with the planets orbiting our sun. But, vastly spaced out as they are, the stars are still actually clustered together into groups; and these groups are called galaxies. A galaxy is seen through astronomers’ powerful telescopes as a swirling pattern that is actually made up of billions of stars, and also clouds of dust and gas. Our sun is just one of the stars that make up the particular galaxy called the Milky Way. It is called that because on dark nights we get an end-on view of part of it. We see it as a mysterious streak or path of milky white across the sky, which you might mistake for a long, wispy cloud until you realize what it really is – and when you do, the thought should strike you dumb with awe. Since we are in the Milky Way galaxy, we can never see it in its full glory. The universe – our observable universe – is a very big place. The next important point is this. It is possible to measure how far away from us each galaxy is. How? How, for that matter, do we know how far away anything in the universe is? For nearby stars the best method uses something called ‘parallax’. Hold your finger up in front of your face and look at it with your left eye closed. Now open your left eye and close your right. Keep switching eyes, and you’ll notice that the apparent position of your finger hops from side to side. That is because of the difference between the viewpoints of your two eyes. Move your finger nearer, and the hops will become greater. Move your finger further away and the hops become smaller. All you need to know is","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:8:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Rainbows and red shift OK, so now we know what a galaxy is, and how to find out its distance from us. For the next step in the argument, we need to make use of the light spectrum, which we met in Chapter 7 on the rainbow. I was once asked to contribute a chapter to a book in which scientists were invited to nominate the most important invention ever. It was fun, but I had left it rather late before joining the party and all the obvious inventions had already been taken: the wheel, the printing press, the telephone, the computer and so on. So I chose an instrument that I was pretty sure nobody else would choose, and is certainly very important even though not many people have ever used one (and I must confess that I’ve never used one myself). I chose the spectroscope. A spectroscope is a rainbow machine. If it is attached to a telescope, it takes the light from one particular star or galaxy and spreads it out as a spectrum, just as Newton did with his prism. But it is more sophisticated than Newton’s prism, because it allows you to make exact measurements along the spread-out spectrum of starlight. Measurements of what? What is there to measure in a rainbow? Well, this is where it starts to get really interesting. The light from different stars produces ‘rainbows’ that are different in very particular ways, and this can tell us a lot about the stars. Does this mean that starlight has a whole variety of strange new colours, colours that we never see on Earth? No, definitely not. You have already seen, on Earth, all the colours that your eyes are capable of seeing. Do you find that disappointing? I did, when I first understood it. When I was a child, I used to love Hugh Lofting’s Doctor Dolittle books. In one of the books the doctor flies to the moon, and is enchanted to behold a completely new range of colours, never before seen by human eyes. I loved this thought. For me it stood for the exciting idea that our own familiar Earth may not be typical of everything in the universe. Unfortunately, though the idea is worthwhile, the story was not true – could not be true. That follows from Newton’s discovery that the colours we see are all contained in white light and are all revealed when white light is spread out by a prism. There are no colours outside the range we are used to. Artists may come up with any number of different tints and shades, but all these are combinations of those basic component colours of white light. The colours we see inside our heads are really just labels made up by the brain to identify light of different wavelengths. We’ve already encountered the complete range of wavelengths here on Earth. Neither the moon nor the stars have any surprises to offer in the colour department. Alas. So what did I mean when I said that different stars produce different rainbows, with differences we can measure using a spectroscope? Well, it turns out that when starlight is splayed out by a spectroscope, strange patterns of thin black lines appear in very particular places along the spectrum. Or sometimes the lines are not black but coloured, and the background is black. The pattern of lines looks like a barcode, the sort of barcode you see on things you buy in shops to identify them at the cash till. Different stars have the same rainbow but different patterns of lines across it – and this pattern really is a kind of barcode, because it tells us a lot about the star and what it is made of. It isn’t only starlight that shows the barcode lines. Lights on Earth do too, so we’ve been able to investigate, in the laboratory, what makes them. And what makes the barcodes, it turns out, is different elements. Sodium, for example, has prominent lines in the yellow part of the spectrum. Sodium light (produced by an electric arc in sodium vapour) glows yellow. The reason for this is understood by physical scientists, but not by me because I’m a biological scientist who doesn’t understand quantum theory. When I went to school in the city","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:8:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Backwards to the big bang What does the red shift mean? Fortunately, scientists understand it well. It is an example of what is called a ‘Doppler shift’. Doppler shifts can happen wherever we have waves – and light, as we saw in the previous chapter, consists of waves. It’s often called the ‘Doppler effect’ and it is more familiar to us from sound waves. When you are standing at a roadside watching the cars whizz by at high speed, the sound of every car’s engine seems to drop in pitch as it passes you. You know the car’s engine note really stays the same, so why does the pitch seem to drop? The answer is the Doppler shift, and the explanation for it is as follows. Sound travels through the air as waves of changing air pressure. When you listen to the note of a car engine – or let’s say a trumpet, because it is more pleasant than an engine – sound waves travel through the air in all directions from the source of the sound. Your ear happens to lie in one of those directions, it picks up the changes in air pressure produced by the trumpet, and your brain hears them as sound. Don’t imagine molecules of air flowing from the trumpet all the way to your ear. It isn’t like that at all: that would be a wind, and winds travel in one direction only, whereas sound waves travel outwards in all directions, like the waves on the surface of a pond when you drop a pebble in. The easiest kind of wave to understand is the so-called Mexican Wave, in which people in a large sports stadium stand up and then sit down again in order, each person doing so immediately after the person on one side of them (say their left side). A wave of standing and then sitting moves swiftly around the stadium. Nobody actually moves from their place, yet the wave travels. Indeed, the wave travels far faster than anybody could run. What travels in the pond is a wave of changing height in the surface of the water. The thing that makes it a wave is that the water molecules themselves are not rushing outwards from the pebble. The water molecules are just going up and down, like the people in the stadium. Nothing really travels outwards from the pebble. It only looks like that because the high points and low points of the water move outwards. Sound waves are a bit different. What travels in the case of sound is a wave of changing air pressure. The air molecules move a little bit, to and fro, away from the trumpet, or whatever is the source of the sound, and back again. As they do so, they knock against neighbouring air molecules and set them moving backwards and forwards too. Those in turn knock against their neighbours and the result is that a wave of molecule-knocking – which amounts to a wave of changing pressure – travels outwards from the trumpet in all directions. And it is the wave that travels from the trumpet to your ear, not the air molecules themselves. The wave travels at a fixed speed, regardless of whether the source of the sound is a trumpet or a speaking voice or a car: about 768 miles per hour in air (four times faster under water, and even faster in some solids). If you play a higher note on your trumpet, the speed at which the waves travel remains the same, but the distance between the wave crests (the wavelength) becomes shorter. Play a low note, and the wave crests space out more but the wave still travels at the same speed. So high notes have a shorter wavelength than low ones. That is what sound waves are. Now for the Doppler shift. Imagine that a trumpeter standing on a snow-covered hillside plays a long, sustained note. You get on a toboggan and speed past the trumpeter (I chose a toboggan rather than a car because it is quiet, so you can hear the trumpet). What will you hear? The successive wave crests leave the trumpet at a definite distance from each other, defined by the note the trumpeter chose to play. But when you are whizzing towards the trumpeter, your ear will gobble up the successive wave crests at a higher rate than if you were standing","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:8:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"9 ARE WE ALONE? SO FAR AS I know there are few, if any, ancient myths about alien life elsewhere in the universe, perhaps because the very idea of there being a universe vastly bigger than our own world hasn’t been around all that long. It took until the 1500s for scientists to see clearly that the Earth orbits the sun, and that there are other planets that do so too. But the distance and number of the stars, let alone other galaxies, were unknown and undreamed of until relatively modern times. And it isn’t that long since people first realized that the direction we call straight up in one part of the world (for example Borneo) would be straight down in another part of the world (in this case Brazil). Before then, people thought that ‘up’ was the same direction everywhere, towards the place where the gods lived, ‘above’ the sky. There have long been numerous legends and beliefs about strange alien creatures near at hand: demons, spirits, djinns, ghosts . . . the list goes on. But in this chapter when I ask ‘Are we alone?’ I am going to mean ‘Are there alien life forms on other worlds elsewhere in the universe?’ As I said, myths about aliens in this sense are rare among primitive tribes. They are all too common, however, among modern city dwellers. These modern myths are interesting because, unlike ancient myths, we can actually watch as they start. We see myths being dreamed up before our very eyes. So the myths in this chapter will be modern. In California in March 1997 a religious cult called Heaven’s Gate came to a sad end when all 39 of its members took poison. They killed themselves because they believed that a UFO from outer space would take their souls to another world. At the time a bright comet called Hale–Bopp was prominent in the sky and the cult believed – because their spiritual leader told them so – that an alien spacecraft was accompanying the comet on its journey. They bought a telescope to observe it, but then sent it back to the shop because it ‘didn’t work’. How did they know it didn’t work? Because they couldn’t see the spacecraft through it! Did the cult leader, a man called Marshall Applewhite, believe the nonsense he taught his followers? Probably he did, because he was one of those who took the poison, so it looks as though he was sincere! Many cult leaders are in the business only so they can take possession of their female followers, but Marshall Applewhite was one of several cult members who had earlier had themselves castrated, so perhaps sex was not uppermost in his mind. One thing most such people seem to have in common is a love of science fiction. The members of the Heaven’s Gate cult were obsessed with Star Trek. Of course, there is no shortage of science fiction stories about aliens from other planets, but most of us know that’s just what they are: fiction, imagined, invented stories, not accounts of things that actually happened. But there are quite a lot of people who firmly, sincerely and unshakeably believe that they have personally been captured (‘abducted’) by aliens from outer space. So eager are they to believe this that they will do so on the flimsiest of ‘evidence’. One man, for instance, believed he had been abducted, for no better reason than that he often got nosebleeds. His theory was that the aliens had put a radio transmitter in his nose to spy on him. He also thought he might be part alien himself, on the grounds that his colouring was a little darker than his parents’. A surprisingly large number of Americans, many of them otherwise normal, sincerely believe that they personally have been taken aboard flying saucers and been the victims of horrific experiments conducted by little grey men with large heads and huge, wraparound eyes. There is a whole mythology of ‘alien abductions’, which is as rich, as colourful and as detailed as the mythology of ancient Greece and the gods of Mount Olympus. But these alien abduction myths are recent, and you can actually go and talk to peopl","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:9:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Is there really life on other planets? Nobody knows. If you forced me to give an opinion one way or the other, I’d say yes, and probably on millions of planets. But who cares about an opinion? There is no direct evidence. One of the great virtues of science is that scientists know when they don’t know the answer to something. They cheerfully admit that they don’t know. Cheerfully, because not knowing the answer is an exciting challenge to try to find it. One day we may have definite evidence of life on other planets, and then we’ll know for sure. For now, the best a scientist can do is write down the kind of information that might reduce the uncertainty, might take us from guesswork to an estimate of likelihood. And that, in itself, is an interesting and challenging thing to do. The first thing we might ask is how many planets there are. Until quite recently, it was possible to believe that the ones orbiting our sun were the only ones, because planets could not be detected by even the largest telescopes. Nowadays we have good evidence that lots of stars have planets, and new ‘extra-solar’ planets are discovered almost every day. An extra-solar planet is a planet orbiting a star other than the sun (sol is the Latin for sun and extra is the Latin for outside). You might think that the obvious way to detect a planet is to see it through a telescope. Unfortunately, planets are too dim to be seen at any great distance – they don’t glow in their own right but only reflect their star’s light – so we can’t see them directly. We have to rely on indirect methods, and the best method again makes use of the spectroscope, the instrument we met in Chapter 8. Here’s how. When a heavenly body orbits another one of approximately equal size, they orbit each other, because they exert approximately equal gravitational force on each other. Several of the bright stars that we see when we look up are actually two stars – so-called binaries – in orbit around each other like the two ends of a dumbbell connected by an invisible rod. When one body is much smaller than the other, as is the case with a planet and its star, the smaller one whizzes around the larger one, while the larger one makes only little token movements in response to the gravitational pull of the smaller. We say that Earth orbits the sun, but actually the sun also makes tiny movements in response to the gravity of Earth. And a planet as large as Jupiter can have an appreciable effect on the position of its star. These token movements of a star are too small to count as ‘going round’ the planet, but they are large enough to be detected by our instruments, even though we can’t see the planet at all. How we detect these movements is interesting in its own right. Any star is too far away for us to be able to see it actually moving, even with a powerful telescope. But, strangely, although we can’t see a star move, we can measure the speed with which it does so. That sounds odd, but this is where the spectroscope comes in. Remember the Doppler shift from Chapter 8? When the star’s movement happens to be away from us, the light from it will be red-shifted. When the star’s movement is towards us its light will be blue-shifted. So, if a star has an orbiting planet, the spectroscope will show us a rhythmically pulsating red-blue-red-blue shift pattern, and the timing of these regular shifts will tell us the length of the planet’s year. Of course it’s complicated when there’s more than one planet. But astronomers are good at mathematics and they can cope with that complication. At the time of writing (May 2012) 701 planets have been detected by this means, orbiting 559 stars. There will surely be more by the time you read this. There are other methods of detecting planets. For example, when a planet passes across the face of its star, a small portion of the face of the star is obscured or eclipsed – like when we see the moon eclipsing the sun, except that the moon looks much bigger because it is","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:9:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Looking for Goldilocks Life as we know it depends on water. Once again, we should beware of fixing our attention on life as we know it, but for the moment exobiologists (scientists searching for extraterrestrial life) regard water as essential – so much so that a good part of their effort is given over to searching the heavens for signs of it. Water is a lot easier to detect than life itself. If we find water it certainly doesn’t mean there has to be life, but it is a step in the right direction. For life as we know it to exist, at least some of the water has to be in liquid form. Ice won’t do, nor will steam. Close inspection of Mars shows evidence of liquid water, in the past if not today. And several other planets have at least some water, even if it is not in liquid form. Europa, one of the moons of Jupiter, is covered with ice, and it has been plausibly suggested that under the ice is a sea of liquid water. People once thought Mars was the best candidate for extraterrestrial life within the solar system, and a famous astronomer called Percival Lowell even drew what he claimed were canals criss-crossing its surface. Spacecraft have now taken detailed photographs of Mars, and have even landed on its surface, and the canals have turned out to be figments of Lowell’s imagination. Nowadays Europa has taken the place of Mars as the prime site of speculation about extraterrestrial life in our own solar system, but most scientists think we have to look further afield. Evidence suggests that water is not particularly rare on extra-solar planets. What about temperature? How finely tuned does the temperature of a planet have to be, if it is to support life? Scientists talk of a so-called ‘Goldilocks Zone’: ‘just right’ (like baby bear’s porridge) between two wrong extremes of too hot (like father bear’s porridge) and too cold (like mother bear’s porridge). The orbit of Earth is ‘just right’ for life: not too close to the sun, where water would boil, and not too far from the sun, where all the water would freeze solid and there wouldn’t be enough sunlight to feed the plants. Although there are billions and billions of planets out there, we cannot expect more than a minority of them to be just right, where temperature and distance from their star are concerned. Recently (May 2011) a ‘Goldilocks planet’ was discovered orbiting a star called Gliese 581, which is about 20 light years away from us (not very far as stars go, but still a vast distance by human standards). The star is a ‘red dwarf’, much smaller than the sun, and its Goldilocks zone is correspondingly closer in. It has (at least) six planets, named Gliese 581e, b, c, g, d and f. Several of them are small, rocky planets like Earth, and one of them, Gliese 581d, is thought to be in the Goldilocks zone for liquid water. It is not known whether Gliese 581d actually has water, but if so it is likely to be liquid rather than ice or vapour. Nobody is suggesting that Gliese 581d actually does have life, but the fact that it has been discovered so soon after we started looking makes one think there are probably lots of Goldilocks planets out there. What about the size of a planet? Is there a Goldilocks size – not too big and not too small, but just right? The size of a planet – more strictly its mass – has a big impact upon life because of gravity. A planet with the same diameter as Earth, but mostly made of solid gold, would have a mass more than three times as great. The gravitational pull of the planet would be over three times as strong as we are used to on Earth. Everything would weigh more than three times as much, and that includes any living bodies on the planet. Putting one foot in front of the other would be a great labour. An animal the size of a mouse would need to have thick bones to support its body, and it would lumber about like a miniature rhinoceros, while an animal the size of a rhinoceros might suffocate under its own weight. Just as gold is heavier than the iron,","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:9:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Here’s looking at you If there are living creatures on other planets, what might they look like? There’s a widespread feeling that it’s a bit lazy for science fiction authors to make them look like humans, with just a few things changed – bigger heads or extra eyes, or maybe wings. Even when they are not humanoid, most fictional aliens are pretty clearly just modified versions of familiar creatures, such as spiders, octopuses or mushrooms. But perhaps it is not just lazy, not just a lack of imagination. Perhaps there really is good reason to suppose that aliens, if there are any (and I think there probably are), might not look too unfamiliar to us. Fictional aliens are proverbially described as bug-eyed monsters, so I’ll take eyes as my example. I could have taken legs or wings or ears (or even wondered why animals don’t have wheels!). But I’ll stick to eyes and try to show that it isn’t really lazy to think that aliens, if there are any, might very well have eyes. Eyes are pretty good things to have, and that is going to be true on most planets. Light travels, for practical purposes, in straight lines. Wherever light is available, such as in the vicinity of a star, it is technically easy to use light rays to find your way around, to navigate, to locate objects. Any planet that has life is pretty much bound to be in the vicinity of a star, because a star is the obvious source of the energy that all life needs. So the chances are good that light will be available wherever life is present; and where light is present it is very likely that eyes will evolve because they are so useful. It is no surprise that eyes have evolved on our planet dozens of times independently. There are only so many ways to make an eye, and I think every one of them has evolved somewhere in our animal kingdom. There’s the camera eye, which, like the camera itself, is a darkened chamber with a small hole at the front letting in light, through a lens, which focuses an upside-down image on a screen – the ‘retina’ – at the back. Even a lens is not essential. A simple hole will do the job if it is small enough, but that means that very little light gets through, so the image is very dim – unless the planet happens to get a lot more light from its star than we get from the sun. This is of course possible, in which case the aliens could indeed have pinhole eyes. Human eyes have a lens, to increase the amount of light that is focused on the retina. The retina at the back is carpeted with cells that are sensitive to light and tell the brain about it via nerves. All vertebrates have this kind of eye, and the camera eye has been independently evolved by lots of other kinds of animals, including octopuses. And invented by human designers too, of course. Jumping spiders have a weird kind of scanning eye. It is sort of like a camera eye except that the retina, instead of being a broad carpet of light-sensitive cells, is a narrow strip. The strip retina is attached to muscles which move it about so that it ‘scans’ the scene in front of the spider. Interestingly, that is a bit like what a television camera does too, since it has only a single channel to send a whole image along. It scans across and down in lines, but does it so fast that the picture we receive looks like a single image. Jumping spider eyes don’t scan so fast, and they tend to concentrate on ‘interesting’ parts of the scene such as flies, but the principle is the same. Then there’s the compound eye, which is found in insects, shrimps and various other animal groups. A compound eye consists of hundreds of tubes, radiating out from the centre of a hemisphere, each tube looking in a slightly different direction. Each tube is capped by a little lens, so you could think of it as a miniature eye. But the lens doesn’t form a usable image: it just concentrates the light in the tube. Since each tube accepts light from a different direction, the brain can combine the information from them all to reconstruct an imag","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:9:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"10 WHAT IS AN EARTHQUAKE? IMAGINE THAT YOU are sitting quietly in your room, perhaps reading a book or watching television or playing a computer game. Suddenly there is a terrifying rumbling sound, and the whole room starts to shake. The light swings wildly from the ceiling, ornaments clatter off the shelves, furniture is hurled across the floor, you are tipped out of your chair. After two minutes or so everything settles down again and there is a blessed silence, broken only by the crying of a frightened child and the barking of a dog. You pick yourself up and think how lucky you are that the whole house didn’t collapse. In a very severe earthquake, it might well have done. While I was beginning to write this book, the Caribbean island of Haiti was hit by a devastating earthquake and the capital city, Port au Prince, was largely destroyed. Two hundred and thirty thousand people are believed to have been killed, and many others, including poor orphaned children, long wandered the streets, homeless, or living in temporary camps. Later, as I was revising the book, another earthquake, even stronger, occurred under the sea off the north-eastern coast of Japan. It caused a gigantic wave – a ‘tsunami’ – that wrought unimaginable destruction when it swept ashore, carrying whole towns with it, killing thousands of people and leaving millions homeless, and setting off dangerous explosions in a nuclear power plant already damaged by the earthquake. Earthquakes, and the tsunamis they cause, are common in Japan (the very word ‘tsunami’ was originally Japanese), but the country had experienced nothing like this in living memory. The prime minister described it as the country’s worst experience since the Second World War, when atomic bombs destroyed the Japanese cities of Hiroshima and Nagasaki. Indeed, earthquakes are common all the way around the rim of the Pacific Ocean – the New Zealand city of Christchurch suffered severe damage and loss of life in a quake just one month before that which struck Japan. This so-called ‘ring of fire’ includes much of California and the western United States, where there was a famous earthquake in the city of San Francisco in 1906. The larger city of Los Angeles is also vulnerable, lying as it does on the notorious San Andreas Fault. In an earthquake, the whole landscape behaves like a sort of liquid. It looks like the sea, with waves passing through it. Solid, dry land, with waves sweeping through it as they do on the sea! That’s an earthquake. If you are down on the ground, you don’t see the waves because you’re too close to them, and too small compared with them. You just feel the ground moving and shaking beneath your feet. In a moment I’m going to explain what an earthquake really is, and what a ‘fault line’ is – like the San Andreas Fault, and similar ones in other parts of the world. But first, let’s look at some myths. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:10:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Earthquake myths We’ll begin with a pair of myths that may have grown up around particular earthquakes, earthquakes that actually happened at certain moments in history. A Jewish legend tells how two cities, Sodom and Gomorrah, were destroyed by the Hebrew god because the people who lived there were so wicked. The only good person in either city was a man called Lot. The god sent two angels to warn Lot to get out of Sodom while he still could. Lot and his family headed for the hills, just before the god started to rain fire and brimstone down on Sodom. They had been given strict orders not to look back, but unfortunately Lot’s wife disobeyed the god. She turned around and took a peek. So the god promptly turned her into a pillar of salt – which, some people say, you can see to this day. Some archaeologists claim to have found evidence that a large earthquake shattered the region where Sodom and Gomorrah are believed to have stood about 4,000 years ago. If this is true, the legend of their destruction might belong in our list of earthquake myths. Another biblical myth which might have started with a particular earthquake is the story of how Jericho was brought down. Jericho, which lies a little north of the Dead Sea in Israel, is one of the oldest cities in the world. It has suffered from earthquakes right up to recent times: in 1927 it was close to the centre of a severe one which shook the whole region and killed hundreds of people in Jerusalem, some 25 kilometres (15 miles) away. The old Hebrew story tells of a legendary hero called Joshua, who wanted to conquer the people who lived in Jericho thousands of years ago. Jericho had thick city walls, and the people locked themselves inside so they couldn’t be attacked. Joshua’s men couldn’t break through the walls, so he ordered his priests to blow rams’ horns and all the people to shout at the tops of their voices. The noise was so great that the walls shook and fell down flat. Joshua’s soldiers then rushed in and slaughtered everybody in the city, including the women and children, and even all the cows, sheep and donkeys. They also burned everything – except the silver and the gold, which they gave to their god, as he instructed them to do. The way the myth is told, this was a good thing: the god of Joshua’s people wanted it to happen so that his people could take over all the land that had previously belonged to the people of Jericho. Since Jericho is such an earthquake-prone place, people nowadays have suggested that the legend of Joshua and Jericho may have begun with an ancient earthquake, which shook the city so violently that the walls fell down. You can easily imagine how a distant folk memory of a disastrous earthquake could be exaggerated and distorted as it was passed by word of mouth down through generations of people who couldn’t read or write, until eventually it grew into the legend of the great tribal hero Joshua, and all that noisy shouting and horn-blowing. The two myths just described may have begun with particular earthquakes in history. There are also lots of other myths, from all around the world, that have come into being as people have tried to understand what earthquakes are in general. Since Japan experiences so many earthquakes, it’s not surprising that Japan has some pretty colourful earthquake myths. According to one of these, the land floated on the back of a gigantic catfish called Namazu. Whenever Namazu flipped his tail, the Earth would shake. Many thousands of miles south, the Maoris of New Zealand, who arrived by canoe and settled there a few centuries before European sailors arrived, believed that Mother Earth was pregnant with her child, the god Ru. Whenever baby Ru kicked or stretched inside his mother’s womb, there was an earthquake. Back in the north, some Siberian tribes believed that the Earth sat on a sledge, pulled by dogs and driven by a god called Tull. The poor dogs had fleas, and when they scratched there was an earthquake. In one","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:10:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"What earthquakes really are First, we need to hear the remarkable story of plate tectonics. Everybody knows what a map of the world looks like. We know the shape of Africa and the shape of South America, and we know that the wide Atlantic Ocean separates them. We can all recognize Australia, and we know that New Zealand lies to the south-east of Australia. We know that Italy looks like a boot, about to kick the ‘football’ of Sicily, and some people think New Guinea looks like a bird. We can easily recognize the outline of Europe, even though the borders within it change all the time. Empires come and go; the frontiers between countries are shifted again and again through history. But the outlines of the continents themselves stay fixed. Don’t they? Well, no, they don’t, and that is the big point. They move, although admittedly very slowly, and so do the positions of the mountain ranges: the Alps, the Himalayas, the Andes, the Rockies. To be sure, these great geographical features are fixed on the timescale of human history. But the Earth itself – if it could think – would think that no time at all. Written history goes back only about 5,000 years. Go back a million years (that’s 200 times as far back as written history stretches) and the continents all have pretty much the same shapes they do today, as far as our eyes would notice. But go back 100 million years and what do we see? The South Atlantic Ocean was a narrow channel by comparison with today, and it looks as though you could almost have swum from Africa to South America. Northern Europe was nearly touching Greenland, which was nearly touching Canada. And India was not part of Asia at all, but right down by Madagascar, and tilted on its side. Africa lurched over the same way, too, compared with the more upright stance we see today. Come to think of it, did you ever notice, when looking at a modern map, that the eastern side of South America looks suspiciously like the western side of Africa, as though they ‘wanted’ to fit together, like pieces in a jigsaw puzzle? It turns out that, if we go back a bit further in time (well, about 50 million years further back, but even that is just ‘a bit’ on the vast, slow geological timescale), we find that they actually did fit together. One hundred and fifty million years ago, Africa and South America were completely joined up, not just to each other but to Madagascar, India and Antarctica too – and to Australia and New Zealand, round the other side of Antarctica. They were all one big land mass called Gondwana, which later split up into pieces, creating one daughter continent after another. It sounds like a pretty tall story, doesn’t it? I mean, it sounds pretty ridiculous that anything as massive as a continent could move thousands of miles – but we now know that it happened, and what is more, we understand how. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:10:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"How the Earth moves We also know that the continents don’t only move away from each other. Sometimes they bump into each other, and when that happens huge mountain ranges get pushed up towards the sky. That’s how the Himalayas were formed: when India collided with Asia. Actually, it isn’t quite true that India collided with Asia. As we shall see soon, what collided with Asia was a much bigger thing, called a ‘plate’, much of it under water, with India sitting on top of it. All continents sit on these ‘plates’. We’ll come to them soon, but first let’s think a bit more about these ‘collisions’, and about the continents moving apart. When you hear a word like ‘collided’ you might think of a sudden crash, as when a truck collides with a car. That isn’t the way it was – and is. The movement of the continents happens agonizingly slowly. Somebody once said it happens about as fast as fingernails grow. If you sit and stare at your fingernails, you don’t see them growing. But if you wait a few weeks, you can see that they have grown, and you need to cut them. In the same way, you can’t see South America in the act of moving away from Africa. But if you wait 50 million years, you notice that the two continents have moved a long way apart. ‘The speed with which fingernails grow’ is the average speed at which the continents move. But fingernails grow at a pretty constant speed, whereas the continents move in jerks: there’s a jerk, then a pause of a hundred years or so while the pressure to move again builds up, then another jerk, and so on. Perhaps now you are beginning to guess what earthquakes really are? That’s right: an earthquake is what we feel when one of those jerks happens. I’m telling you this as a known fact, but how do we know it? And when did we first discover it? That’s a fascinating story, which I now need to tell. Various people in the past have noticed the jigsawy kind of fit between South America and Africa, but they didn’t know what to make of it. About 100 years ago, a German scientist called Alfred Wegener made a bold suggestion. It was so bold that most people thought he was a bit mad. Wegener suggested that the continents drifted about like gigantic ships. Africa and South America and the other great southern land masses had, in Wegener’s view, once been joined together. Then they tore apart from each other and cruised off through the sea in their separate directions. That was what Wegener thought, and people laughed at him for it. But it now turns out that he was right – well, almost right, and certainly much more right than the people who laughed at him. The modern theory of plate tectonics, which is supported by a huge amount of evidence, isn’t quite the same as Wegener’s idea. Wegener was definitely right that Africa and South America, India, Madagascar, Antarctica and Australia had once all been joined up and later split apart. But the way it happened, according to the theory of plate tectonics, is a bit different from the way Wegener saw it. He thought of the continents as ploughing through the sea, floating, not on water but on the soft, molten or semi-molten layers of the Earth’s crust. The modern theory of plate tectonics sees the whole crust of the Earth, including the bottom of the sea, as a complete set of interlocking plates. (That’s ‘plates’ as in ‘armour plates’, not the kind of plates you eat off.) So it isn’t just the continents that move: it’s the plates that they sit on, and there is no bit of the Earth’s surface that isn’t part of a plate. Most of the area of most of the plates lies under the sea. The land masses we know as the continents are the high ground of the plates, sticking up above the water. Africa is just the top of the much larger African plate, which stretches halfway across the South Atlantic. South America is the top of the South American plate, which stretches across the other half of the South Atlantic. Other plates are the Indian and Australian plates; the Eurasian plate, which ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:10:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Sea-floor spreading Yes. The answer lies in something called ‘sea-floor spreading’. You know those moving walkways that you see at large airports to help people with luggage cover the long distances between, say, the entrance to the terminal and the departure lounge? Instead of having to walk all the way, they step on a moving belt and are carried along to some point where they have to start walking again. The moving walkway at an airport is only just wide enough for two people to stand side by side. But now imagine a moving walkway that is thousands of miles wide, stretching most of the way from the Arctic to the Antarctic. And imagine that, instead of moving at walking pace, it moves at the speed with which fingernails grow. Yes, you’ve guessed it. South America, and the whole South American plate, is being carried away from Africa and the African plate, on something like a moving walkway that lies deep under the sea bed and stretches from the far north to the far south of the Atlantic Ocean, moving very slowly. What about Africa? Why isn’t the African plate moving in the same direction, and why doesn’t it keep up with the South American plate? The answer is that Africa is on a different moving walkway, one that is travelling in the opposite direction. The African moving walkway goes from west to east, while the South American moving walkway goes from east to west. So what is going on in the middle? Next time you are at a big airport, stop just before you step on the moving walkway and watch it. It wells up out of a slit in the floor, and moves away from you. It is a belt, going round and round, travelling forwards above the floor and coming back towards you under the floor. Now imagine another belt, welling out of the same slit but going in exactly the opposite direction. If you put one foot on one belt and the other foot on the other belt you’d be forced to do the splits. The equivalent of the slit in the floor at the bottom of the Atlantic Ocean runs all along the deep sea floor from the far south to the far north. It is called the mid-Atlantic ridge. The two ‘belts’ well up through the mid-Atlantic ridge and head off in opposite directions, one carrying South America steadily westwards, the other carrying Africa away to the east. And, like the belts at the airport, the great belts that move the tectonic plates roll around and come back deep within the Earth. Next time you are at an airport, get on the moving walkway and let it carry you, while you imagine you are Africa (or South America if you prefer). When you get to the other end of the walkway and step off, watch the belt dive underground, ready to make its way back to where you’ve just come from. The moving belts at an airport are driven by electric motors. What drives the moving belts that carry the great plates of the Earth with their cargo of continents? Deep beneath the Earth’s surface there are what are called convection currents. What’s a convection current? Maybe you have an electric convector heater in your house. Here’s how it works to heat a room. It heats air. Hot air rises because it is less dense than cold air (that’s how hot-air balloons work). The hot air rises until it hits the ceiling, where it can’t rise any more and is forced sideways by the fresh hot air pushing up from beneath. As it travels sideways, the air cools down, whereupon it sinks. When it hits the floor, it again moves sideways, creeping along the floor until it gets caught up in the heater and rises again. That explanation is a bit too simple, but the basic idea is all that matters here: under ideal conditions a convector heater can get the air moving round and round – circulating. This kind of circulation is called a ‘convection current’. The same thing happens in water. In fact, it can happen in any liquid or any gas. But how can there be convection currents under the Earth’s surface? It isn’t liquid down there, is it? Well, yes, it is – sort of. Not liquid like water, but sort of h","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:10:4","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Driven by heat The plates are made of hard rock, and, as we’ve seen, most of them is under the sea. Each plate is several miles thick. This thick layer of armour plating is called the lithosphere, which literally means ‘sphere of rock’. Under the sphere of rock is an even thicker layer, if you can believe it, which isn’t actually called the sphere of treacle but probably should be (it’s actually the upper mantle). The hard rocky plates of the sphere of rock could be said to ‘float’ on the sphere of treacle. Deep heat beneath and within the sphere of treacle causes agonizingly slow, grinding convection currents in the treacle, and it is those convection currents that carry the great rocky plates floating above. Convection currents follow pretty complicated paths. Just think about all the different ocean currents, and even the winds, which are sort of high-speed convection currents. So it’s no wonder that the various plates on the Earth’s surface are carried in all sorts of directions, rather than round and round as if they were all on a simple merry-go-round. No wonder the plates bump into each other or tear rendingly away from each other, dive one under the other or grate sideways against each other. And no wonder we feel these titanic forces – grinding, wrenching, roaring, scraping forces – as earthquakes. Terrible as earthquakes can be, the wonder is that they aren’t even more terrible. Sometimes a moving plate slides underneath a neighbouring plate. This is called ‘subduction’. Part of the African plate, for example, is being subducted under the Eurasian plate. This is one reason why there are earthquakes in Italy, and it is one reason why Mount Vesuvius erupted in ancient Roman times and destroyed the towns of Pompeii and Herculaneum (because volcanoes tend to sprout along the edges of the plates). The Himalayan mountains, including Mount Everest, were forced up to their great height as the Indian plate was steadily subducted under the Eurasian plate. We began with the San Andreas Fault, so let’s end there. The San Andreas Fault is a long, rather straight ‘slippage’ line between the Pacific plate and the North American plate. Both plates are moving north-west, but the Pacific plate is moving faster. The city of Los Angeles lies on the Pacific plate, not the North American plate, and is steadily creeping up on San Francisco, most of which is on the North American plate. Earthquakes are constantly to be expected in this whole region, and experts are predicting that there will be a big one within the next ten years or so. Fortunately, California, unlike Haiti, is well equipped to deal with the terrible plight of earthquake victims. One day, parts of Los Angeles might end up in San Francisco. But that is a long way off, and none of us will be around to see it. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:10:5","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"11 WHY DO BAD THINGS HAPPEN? WHY DO BAD things happen? After a dreadful disaster such as an earthquake or a hurricane, you’ll hear people saying things like this: ‘It’s so unfair. What did those poor people ever do to deserve such a fate?’ If a really good person gets a painful disease and dies, while a really bad person remains in the best of health, once again we cry, ‘Unfair!’ Or we say, ‘Where’s the justice in that?’ It is hard to resist this feeling that, somehow, there ought to be a kind of natural justice. Good things should happen to good people. Bad things, if they must happen at all, should only happen to bad people. In Oscar Wilde’s delightful play The Importance of Being Earnest, an elderly governess called Miss Prism explains how, long ago, she wrote a novel. When she is asked whether it ended happily, she replies: ‘The good ended happily, and the bad unhappily. That is what fiction means.’ Real life is different. Bad things do happen, and they happen to good people as well as bad. Why? Why is real life not like Miss Prism’s fiction? Why do bad things happen? Lots of peoples believe that their gods intended to create a perfect world but unfortunately something went wrong – and there are almost as many ideas about what that something was. The Dogon tribe of West Africa believe that at the beginning of the world there was a cosmic egg from which two twins hatched. All would have been well if the twins had hatched at the same time. Unfortunately, one of them hatched too soon, and spoiled the gods’ plan of perfection. That, according to the Dogon, is why bad things happen. There are lots of legends about how death came into the world. All over Africa, different tribes believe that the chameleon was given the news of everlasting life and told to carry it to humans. Unfortunately the chameleon walked so slowly (they do, I know: as a child in Africa I had a pet chameleon called Hookariah) that the news of death, carried by a nippier lizard (or other faster animal in other versions of the legend), arrived first. In one West African legend, the news of life was brought by a slow toad, unfortunately overtaken by a fast dog bringing the news of death. I must say I’m a bit puzzled why the order in which news arrives should matter so much. Bad news is still bad, whenever it arrives. Disease is a special kind of bad thing, and it has spawned plenty of myths of its own. One reason is that for a long time diseases were rather mysterious. Our ancestors faced other dangers – from lions and crocodiles, from enemy tribes, from the threat of starvation – but you could see them coming, and understand them. Smallpox, on the other hand, or the Black Death, or malaria, must have seemed to pounce from nowhere, without warning, and it wasn’t obvious how to guard against these assaults. It was a terrifying mystery. Where did diseases come from? What did we do to deserve this painful death, this agonizing toothache or these hideous spots? No wonder people resorted to superstition when desperately trying to understand disease, and even more desperately trying to protect themselves from it. In many African tribes, until quite recently, anybody who got ill, or had a sick child, would automatically look around for an evil magician or witch to blame. If my child has a high fever, it must be because an enemy paid a witch doctor to cast a spell on her. Or maybe it is because I couldn’t afford to sacrifice a goat when she was born. Or perhaps it is because a green caterpillar walked across the path in front of me and I forgot to spit out the evil spirit. In ancient Greece, sick pilgrims would spend the night in a temple dedicated to Asclepius, the god of healing and medicine. They believed the god would either heal them himself or reveal the cure in a dream. Even today, a surprisingly large number of sick people travel to places like Lourdes, where they plunge into a sacred pool in the hope that the holy water will heal them (actually, one might susp","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:11:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Why do bad things happen *really*? Why does anything happen? That’s a complicated question to answer, but it is a more sensible question than ‘Why do bad things happen?’ This is because there is no reason to single out bad things for special attention unless bad things happen more often than we would expect them to, by chance; or unless we think there should be a kind of natural justice, which would mean that bad things should only happen to bad people. Do bad things happen more often than we ought to expect by chance alone? If so, then we really do have something to explain. You may have heard people refer jokingly to ‘Murphy’s Law’, sometimes called ‘Sod’s Law’. This states: ‘If you drop a piece of toast and marmalade on the floor, it always lands marmalade side down.’ Or, more generally: ‘If a thing can go wrong, it will.’ People often joke about this, but at times you get the feeling they think it is more than a joke. They really do seem to believe the world is out to get them. I do a certain amount of filming for television documentaries, and one of the things that can go wrong in filming ‘on location’ is unwanted noise. When an aircraft drones in the distance, you have to stop filming and wait for it to go, and this can be extremely irritating. Costume dramas of life in earlier centuries are ruined by even a trace of aircraft noise. Film people have a superstition that aircraft deliberately choose moments when silence is most important to fly overhead, and they invoke Sod’s Law. Recently, a film crew I was working with chose a location where we felt sure there should be a minimum of noise, a huge empty meadow near Oxford. We arrived early in the morning to make doubly sure of peace and quiet – only to discover, when we arrived, a lone Scotsman practising the bagpipes (perhaps banished from the house by his wife). ‘Sod’s Law!’ we all proclaimed. The truth, of course, is that there is noise going on most of the time, but we only notice it when it is an irritation, as when it interferes with filming. There is a bias in our likelihood of noticing annoyance, and this makes us think the world is out to annoy us deliberately. In the case of the toast, it wouldn’t be surprising to find that it really does fall marmalade side down more often than not, because tables are not very high, the toast starts marmalade side up and there is usually time for one half-rotation before it hits the ground. But the toast example is just a colourful way to express the gloomy idea that ‘if a thing can go wrong it will.’ Perhaps this would be a better example of Sod’s Law: ‘When you toss a coin, the more strongly you want heads, the more likely it is to come up tails.’ That, at least, is the pessimistic view. There are optimists who think that the more you want heads, the more likely the coin is to come up heads. Perhaps we could call that ‘Pollyanna’s Law’ – the optimistic belief that things usually turn out for the good. Or it could be called ‘Pangloss’s Law’, after a character invented by the great French writer Voltaire. His ‘Dr Pangloss’ thought that ‘All is for the best in this best of all possible worlds.’ When you put it like that, you can quickly see that Sod’s Law and Pollyanna’s Law are both nonsense. Coins, and slices of toast, have no way of knowing the strength of your desires, and no desire of their own to thwart them – or fulfil them. Also, what is a bad thing for one person may be a good thing for another. Rival tennis players may both pray fervently for victory, but one has to lose! There is no special reason to ask, ‘Why do bad things happen?’ Or, for that matter, ‘Why do good things happen?’ The real question underlying both is the more general question: ‘Why does anything happen?’ ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:11:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Luck, chance and cause People sometimes say, ‘Everything happens for a reason.’ In one sense this is true. Everything does happen for a reason – which is to say that events have causes, and the cause always comes before the event. Tsunamis happen because of undersea earthquakes, and earthquakes happen because of shifts in the earth’s tectonic plates, as we saw in Chapter 10. That is the true sense in which ‘everything happens for a reason’, the sense in which ‘reason’ means ‘past cause’. But people sometimes use reason in a very different sense, to mean something like ‘purpose’. They will say something like ‘The tsunami was a punishment for our sins’ or ‘The reason for the tsunami was to destroy the strip clubs and discos and bars and other sinful places.’ It is amazing how often people resort to this kind of nonsense. Maybe it is a hangover from childhood. Child psychologists have shown that very young children, when asked why certain rocks are pointy, reject scientific causes as an explanation and prefer the answer: ‘So that animals can scratch themselves when they get itchy.’ Most children grow out of that kind of explanation for the pointy rocks. But quite a lot of adults seem unable to shake off the same kind of explanation when it comes to major misfortunes like earthquakes, or good fortune such as lucky escapes from earthquakes. What about ‘bad luck’? Is there such a thing as bad luck, or indeed good luck? Are some people luckier than others? People sometimes talk of a ‘run’ of bad luck. Or they will say, ‘So many bad things have happened to me lately, I’m due for a piece of really good luck.’ Or they may say, ‘So-and-so is such an unlucky person, things always seem to turn out badly for her.’ ‘I’m due for a piece of good luck’ is an example of a widespread misunderstanding of the ‘Law of Averages’. In the game of cricket, it often makes a big difference which team bats first. The two captains toss a coin to decide who gets the advantage, and each team’s supporters very much hope their captain will win the toss. Before a recent match between India and Sri Lanka, a Yahoo web page posed the question: ‘Will Dhoni [the Indian captain] be lucky once again with the toss?’ Of the answers they received, the following was chosen as ‘Best Answer’: ‘I firmly believe in the law of averages, so my bet is on Sangakkara [the Sri Lanka captain] being lucky and winning the much hyped toss.’ Can you see what rubbish this is? In a series of previous matches, Dhoni had won the toss every time. Coins are supposed to be unbiased. So the misunderstood ‘Law of Averages’ ought to see to it that Dhoni, having been lucky so far, should now lose the toss, to redress the balance. Another way to put this would be to say that it was now Sangakkara’s turn to win the toss. Or that it would be unfair if Dhoni won the toss yet again. But the reality is that, no matter how many times Dhoni has won the toss before, the chances that he will win it again this time are always 50:50. ‘Turns’ and ‘fairness’ simply don’t come into it. We may care about fairness and unfairness, but coins don’t give a toss! Nor does the universe at large. It is true that if you toss a penny 1,000 times, you’d expect approximately 500 heads and 500 tails. But suppose you’ve tossed the penny 999 times and it’s so far come up heads every time. What would you bet for the last toss? According to the widespread misunderstanding of the ‘Law of Averages’, you should bet on tails, because it is tails’ turn, and it would be so unfair if it came up heads yet again. But I would place my bet on heads, and so would you if you were wise. A sequence of 999 heads in a row suggests that someone has tinkered with the penny, or with the method of tossing it. The misunderstood ‘Law of Averages’ has been the ruin of many gamblers. Admittedly, with hindsight you can say, ‘Sangakkara was very unlucky to lose the toss, because it meant that India batted on a perfect pitch and that helped them to amass a h","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:11:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Pollyanna and paranoia So, we have seen that bad things, like good things, don’t happen any more often than they ought to by chance. The universe has no mind, no feelings and no personality, so it doesn’t do things in order to either hurt or please you. Bad things happen because things happen. Whether they are bad or good from our point of view doesn’t influence how likely it is that they will happen. Some people find it hard to accept this. They’d prefer to think that sinners get their comeuppance, that virtue is rewarded. Unfortunately the universe doesn’t care what people prefer. But now, having said all that, we should pause for thought. Funnily enough, I have to admit that something a bit like Sod’s Law is true. Although it is definitely not true that the weather, or an earthquake, is out to get you (for they don’t care about you, one way or the other), things are a bit different when we turn to evolution. If you are a rabbit, a fox is out to get you. If you are a minnow, a pike is out to get you. I don’t mean the fox or pike thinks about it, although it may. I’d be equally happy to say that a virus is out to get you, and nobody believes viruses think about anything. But evolution by natural selection has seen to it that viruses, and foxes, and pikes, behave in ways that are actively bad for their victims – behave as though they are deliberately out to get them – in ways that you couldn’t say of earthquakes or hurricanes or avalanches. Earthquakes and hurricanes are bad for their victims, but they don’t take active steps to do bad things: they don’t take active steps to do anything, they just happen. Natural selection, the struggle for existence as Darwin called it, means that every living creature has enemies that are working hard for its downfall. And sometimes the tricks that natural enemies use give the appearance of being cleverly planned. Spider webs, for example, are ingenious traps laid for unsuspecting insects. A fearsome little insect called an ant lion digs booby traps for its prey to fall into. The ant lion itself sits under the sand at the bottom of the conical pit that it digs, and seizes any ant that falls into the pit. Nobody is suggesting that the spider or the ant lion is ingenious – that it has thought up its cunning trap. But natural selection makes them evolve brains that behave in ways that look ingenious to our eyes. In the same way, a lion’s body looks ingeniously designed to bring about the doom of antelopes and zebras. And we can imagine that, if you were an antelope, a stalking, chasing, pouncing lion might seem out to get you. It’s easy to see that predators (animals that kill and then eat other animals) are working for the downfall of their prey. But it’s also true that prey are working for the downfall of their predators. They work hard to escape being eaten, and if they all succeeded the predators would starve to death. The same thing holds between parasites and their hosts. It also holds between members of the same species, all of whom are actually or potentially competing with one another. If the living is easy, natural selection will favour the evolution of improvements in enemies, whether predators, prey, parasites, hosts or competitors: improvements that will make life hard again. Earthquakes and tornadoes are unpleasant and might even be called enemies, but they are not ‘out to get you’ in the same ‘Sod’s Law’ kind of way that predators and parasites are. This has consequences for the sort of mental attitude that any wild animal, such as an antelope, might be expected to have. If you are an antelope and you see the long grass rustling, it could be just the wind. That’s nothing to worry about, for the wind is not out to get you: it is completely indifferent to antelopes and their well-being. But that rustle in the long grass could be a stalking leopard, and a leopard is most definitely out to get you: you taste good to a leopard and natural selection favoured ancestral leopards that wer","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:11:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Illness and evolution – work in progress? As I said, predators aren’t the only things that are out to get us. Parasites are a more sneaky threat, but they are just as dangerous. Parasites include tapeworms and flukes, bacteria and viruses, which make a living by feeding off our bodies. Predators such as lions also feed off bodies, but the distinction between a predator and a parasite is usually clear. Parasites feed off still-living victims (though they may eventually kill them) and they are usually smaller than their victims. Predators are either larger than their victims (as a cat is larger than a mouse) or, if smaller (as a lion is smaller than a zebra), not very much smaller. Predators kill their prey outright and then eat them. Parasites eat their victims more slowly, and the victim may stay alive a long time with the parasite gnawing away inside. Parasites often attack in large numbers, as when our body suffers a massive infection with a flu or cold virus. Parasites that are too small to see with the naked eye are often called ‘germs’, but that’s rather an imprecise word. They include viruses, which are very very small indeed; bacteria, which are larger than viruses but still very small (there are viruses that act as parasites on bacteria); and other single-celled organisms like the malarial parasite, which are much larger than bacteria but still too small to be seen without a microscope. Ordinary language has no general name for these larger singled-celled parasites. Some of them can be called ‘protozoa’, but that’s now rather an outdated term. Other important parasites include fungi, for example ringworm and athlete’s foot (big things like mushrooms and toadstools give a false impression of what most fungi are like). Examples of bacterial diseases are tuberculosis, some kinds of pneumonia, whooping cough, cholera, diphtheria, leprosy, scarlet fever, boils and typhus. Viral diseases include measles, chickenpox, mumps, smallpox, herpes, rabies, polio, rubella, various strains of influenza and the cluster of diseases that we call the ‘common cold’. Malaria, amoebic dysentery and sleeping sickness are among those diseases caused by ‘protozoa’. Other important parasites, larger still – large enough to be seen with the naked eye – are the various kinds of worms, including flatworms, roundworms and flukes. When I was a boy living on a farm, I would quite often find a dead animal like a weasel or a mole. I was learning biology at school, and I was interested enough to dissect these little corpses when I found them. The main thing that impressed me was how full of wriggling, live worms they were (roundworms, technically called nematodes). The same was never true of the domesticated rats and rabbits we were given to dissect at school. The body has a very ingenious and usually effective system of natural defence against parasites, called the immune system. The immune system is so complicated that it would take a whole book to explain it. Briefly, when it senses a dangerous parasite the body is mobilized to produce special cells, which are carried by the blood into battle like a kind of army, tailor-made to attack the particular parasites concerned. Usually the immune system wins, and the person recovers. After that, the immune system ‘remembers’ the molecular equipment that it developed for that particular battle, and any subsequent infection by the same kind of parasite is beaten off so quickly that we don’t notice it. That is why, once you have had a disease like measles or mumps or chickenpox, you’re very unlikely to get it again. People used to think it was a good idea if children caught mumps, say, because the immune system’s ‘memory’ would protect them against getting it as an adult – and mumps is even more unpleasant for adults (especially men, because it attacks the testicles) than it is for children. Vaccination is the ingenious technique of doing something similar on purpose. Instead of giving you the disease itself, th","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:11:4","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"12 WHAT IS A MIRACLE? IN THE FIRST chapter of this book I talked about magic, and separated supernatural magic (casting a spell to turn a frog into a prince, or rubbing a lamp to conjure up a genie) from conjuring tricks (illusions, such as silk handkerchiefs turning into rabbits, or women being sawn in half). Nobody nowadays believes in fairy-tale magic. Everybody knows that pumpkins turn into coaches only in Cinderella. And we all know that rabbits come out of apparently empty hats only by trickery. But there are some supernatural tales that are still taken seriously, and the ‘events’ they recount are often called miracles. This chapter is about miracles – stories of supernatural happenings that many people believe, as opposed to fairy-tale spells, which nobody believes, and conjuring tricks, which look like magic but we know are faked. Some of these tales are ghost stories, spooky urban legends or stories of uncanny coincidence – stories like, ‘I dreamed about a celebrity whom I hadn’t thought about for years, and the very next morning I heard that he’d died in the night.’ Many more come from the hundreds of religions around the world, and these in particular are often called miracles. To take just one example, there is a legend that, about 2,000 years ago, a wandering Jewish preacher called Jesus was at a wedding where they ran out of wine. So he called for some water and used miraculous powers to turn it into wine – very good wine, as the story goes on to tell us. People who would laugh at the idea that a pumpkin could turn into a coach, and who know perfectly well that silk handkerchiefs don’t really turn into rabbits, are quite happy to believe that a prophet turned water into wine or, as devotees of another religion would have it, flew to heaven on a winged horse. ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:12:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Rumour, coincidence and snowballing stories Usually when we hear a miracle story it’s not from an eye witness, but from somebody who heard about it from somebody else, who heard it from somebody else, who heard it from somebody else’s wife’s friend’s cousin . . . and any story, passed on by enough people, gets garbled. The original source of the story is often itself a rumour that began so long ago and has become so distorted in the retelling that it is almost impossible to guess what actual event – if any – started it off. After the death of almost any famous person, hero or villain, stories that somebody has seen them alive start rushing around the globe. This was true of Elvis Presley, of Marilyn Monroe, even of Adolf Hitler. It’s hard to know why people enjoy passing on such rumours when they hear them, but the fact is that they do, and that is a big part of the reason why rumours spread. Here’s a recent example of how such a rumour gets started. Soon after Michael Jackson died in 2009, an American television crew was given a guided tour of his famous mansion called Neverland. In one scene of the resulting film, people thought they saw his ghost at the end of a long corridor. The recording is very unconvincing – however, it was enough to start wild rumours flying around. Michael Jackson’s ghost is at large! Copycat sightings soon emerged. For example, there is a photograph that a man took of the polished surface of his car. To you and me, especially when we compare the ‘face’ with the other clouds on either side, what we are looking at is obviously the reflection of a cloud. But to the overheated imagination of the devoted fan it could only be the ghost of Michael Jackson, and the picture on YouTube has received more than 15 million hits! Actually, there’s something interesting going on here, which is worth mentioning. Humans are social animals, the human brain is pre-programmed to see the faces of other humans even where there aren’t any. This is why people so often see faces in the random patterns made by clouds, or on slices of toast, or in damp patches on walls. Spine-tingling ghost stories are fun to tell, especially if they are really scary, and even more so if you claim that they are true. When I was eight, my family lived briefly in a house called Cuckoos, about 400 years old, with wonky black Tudor beams. Not surprisingly, the house had a legend about a long-dead priest hidden in a secret passage. There was a story that you could hear his footsteps on the stairs, but with the twist that you could hear one step too many – spookily explained by the fact that the staircase was said to have had an extra step in the sixteenth century! I remember the pleasure I took in passing the story on to my schoolfriends. It never occurred to me to ask how good the evidence was. It was enough that the house was old, and my friends were impressed. People get a thrill from passing on ghost stories. The same applies to miracle stories. If a rumour of a miracle gets written down in a book, the rumour becomes hard to challenge, especially if the book is ancient. If a rumour is old enough, it starts to be called a ‘tradition’ instead, and then people believe it all the more. This is rather odd, because you might think they would realize that older rumours have had more time to get distorted than younger rumours that are close in time to the alleged events themselves. Elvis Presley and Michael Jackson lived too recently for traditions to have grown up, so not many people believe stories like ‘Elvis seen on Mars’. But maybe in 2,000 years’ time . . . ? What about those strange stories people tell of having a dream about somebody they haven’t seen or thought of for years, then waking up to find a letter from that person waiting on the doormat? Or waking up to hear or read that the person died in the night? You may have had such an experience yourself. How do we explain coincidences like that? Well, the most likely explanation is that they r","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:12:1","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"A good way to think about miracles There was a famous Scottish thinker in the eighteenth century called David Hume who made a clever point about miracles. He began by defining a miracle as a ‘transgression’ (or breaking) of a law of nature. Walking on water, or turning water into wine, or stopping or starting a clock by the power of thought alone, or turning a frog into a prince, would be good examples of breaking a law of nature. Miracles like that would be very disturbing indeed to science, for the reasons discussed in the chapter on magic. Disturbing if they ever happened, that is! So how should we respond to stories of miracles? This was the question Hume turned to; and his answer was the clever point I mentioned. If you want to know Hume’s actual words, here they are, but you have to remember that he wrote them more than two centuries ago, and English style has changed since then. No testimony is sufficient to establish a miracle, unless the testimony be of such a kind, that its falsehood would be more miraculous than the fact which it endeavours to establish. Let’s put Hume’s point into other words. If John tells you a miracle story, you should believe it only if it would be even more of a miracle for it to be a lie (or a mistake, or an illusion). For example, you might say, ‘I would trust John with my life, he never tells a lie, it would be a miracle if John ever told a lie.’ That’s all well and good, but Hume would say something like this: ‘However unlikely it might be that John could tell a lie, is it really more unlikely than the miracle that John claims to have seen?’ Suppose John claimed to have watched a cow jump over the moon. No matter how trustworthy and honest John might normally be, the idea of his telling a lie (or having an honest hallucination) would be less of a miracle than a cow literally jumping over the moon. So you should prefer the explanation that John was lying (or mistaken). That was an extreme and imaginary example. Let’s take something that really happened, to see how Hume’s idea might work in practice. In 1917, two young English cousins called Frances Griffiths and Elsie Wright took photographs, which they said were of fairies. To modern eyes, the photographs are obvious fakes, but at the time, when photography was still quite a new thing, even the great writer Sir Arthur Conan Doyle, creator of the famously un-foolable Sherlock Holmes, was taken in by it, and so were quite a lot of other people. Years later, when Frances and Elsie were old women, they came clean and admitted that the ‘fairies’ were nothing more than cardboard cut-outs. But let’s think like Hume, and work out why Conan Doyle and the others should have known better than to fall for the trick. Which of the following two possibilities do you think would be the more miraculous, if it were true? 1 There really were fairies, tiny people with wings, flitting about among the flowers. 2 Elsie and Frances were making it up, and faking the photographs. It’s really no contest, is it? Children play make-believe all the time, and it is so easy to do. Even if it were hard to do; even if you felt that you knew Elsie and Frances very well, and they were always completely truthful girls, who would never dream of playing a trick; even if the girls had been given a truth drug, and had sailed through a lie-detector test with flying colours; even if this all added up to its being a miracle if they told a lie, what would Hume say? He would say that the ‘miracle’ of their lying would still be a smaller miracle than the fairies they claimed to show actually existing. Elsie and Frances didn’t do any serious harm with their prank, and it is even rather funny that they managed to fool the great Conan Doyle. But such tricks by young people are sometimes no laughing matter, to put it mildly. Back in the seventeenth century, in a village in New England called Salem, a group of young girls became hysterically obsessed with ‘witches’, and started imagining, o","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:12:2","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"Today’s miracle, tomorrow’s technology There are things that not even the best scientists of today can explain. But that doesn’t mean we should block off all investigation by resorting to phoney ‘explanations’ invoking magic or the supernatural, which don’t actually explain at all. Just imagine how a medieval man – even the most educated man of his era – would have reacted if he had seen a jet plane, a laptop computer, a mobile telephone or a satnav device. He would probably have called them supernatural, miraculous. But these devices are now commonplace; and we know how they work, for people have built them, following scientific principles. There never was a need to invoke magic or miracles or the supernatural, and we now see that the medieval man would have been wrong to do so. We don’t have to go back as far as medieval times to make the point. A gang of Victorian international criminals equipped with modern mobile phones could have coordinated their activities in ways that would have looked like telepathy to Sherlock Holmes. In Holmes’s world, a suspect in a murder case who could prove that he was in New York the evening after the murder was committed in London would have a perfect alibi, because in the late nineteenth century it was impossible to be in New York and in London on the same day. Anyone who claimed otherwise would seem to be invoking the supernatural. Yet modern jet planes make it easy. The eminent science-fiction writer Arthur C. Clarke summed the point up as Clarke’s Third Law: Any sufficiently advanced technology is indistinguishable from magic. If a time machine were to carry us forward a century or so, we would see wonders that today we might think impossible – miracles. But it doesn’t follow that everything we might think impossible today will happen in the future. Science-fiction writers can easily imagine a time machine – or an anti-gravity machine, or a rocket that can carry us faster than light. But the mere fact that we can imagine them is no reason to suppose that such machines will one day become reality. Some of the things we can imagine today may become real. Most will not. The more you think about it, the more you realize that the very idea of a supernatural miracle is nonsense. If something happens that appears to be inexplicable by science, you can safely conclude one of two things. Either it didn’t really happen (the observer was mistaken, or was lying, or was tricked); or we have exposed a shortcoming in present-day science. If present-day science encounters an observation, or an experimental result, that it cannot explain, then we should not rest until we have improved our science so that it can provide an explanation. If it requires a radically new kind of science, a revolutionary science so strange that old scientists scarcely recognize it as science at all, that’s fine too. It’s happened before. But don’t ever be lazy enough – defeatist enough, cowardly enough – to say ‘It must be supernatural’ or ‘It must be a miracle’. Say instead that it’s a puzzle, it’s strange, it’s a challenge that we should rise to. Whether we rise to the challenge by questioning the truth of the observation, or by expanding our science in new and exciting directions, the proper and brave response to any such challenge is to tackle it head-on. And, until we have found a proper answer to the mystery, it’s perfectly OK simply to say, ‘This is something we don’t yet understand, but we’re working on it.’ Indeed, it is the only honest thing to do. Miracles, magic and myths – they can be fun, and we have had fun with them throughout this book. Everybody likes a good story, and I hope you enjoyed the myths with which I began most of my chapters. But even more I hope that, in every chapter, you enjoyed the science that came after the myths. I hope you agree that the truth has a magic of its own. The truth is more magical – in the best and most exciting sense of the word – than any myth or made-up mystery or miracle. Scie","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:12:3","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"ACKNOWLEDGEMENTS Thanks to: Lalla Ward, Lawrence Krauss, Sally Gaminara, Gillian Somerscales, Philip Lord, Katrina Whone, Hilary Redmon; Ken Zetie, Tom Lowes, Owen Toller, Will Williams and Sam Roberts from St Paul’s School, London; Alain Townsend, Bill Nye, Elisabeth Cornwell, Carolyn Porco, Christopher McKay, Jacqueline Simpson, Rosalind Temple, Andy Thomson, John Brockman, Kate Kettlewell, Mark Pagel, Michael Land, Todd Stiefel, Greg Langer, Robert Jacobs, Michael Yudkin, Oliver Pybus, Rand Russell, Edward Ashcroft, Greg Stikeleather, Paula Kirby, Anni Cole-Hamilton and the staff and pupils of Moray Firth School. © JEREMY SUTTON HIBBERT RICHARD DAWKINS was first catapulted to fame with his iconic book The Selfish Gene, which he followed with a string of bestselling books, including the phenomenal The God Delusion. The Magic of Reality is his first book written for a younger, more general readership and it also became an immediate bestseller in its original, colour-illustrated hardback edition. Dawkins is a fellow of the Royal Society and the Royal Society of Literature, and has won numerous awards. He was a professor at Oxford University until 2008 and he remains a fellow of New College. He has also written and presented several television documentaries, including The Genius of Charles Darwin in 2008 and Faith Schools Menace? in 2010. © CLARE HAYTHORNTHWAITE DAVE McKEAN has illustrated and designed many award-winning books and graphic novels. He has created hundreds of album, comic and book covers, and has designed characters for two of the Harry Potter films. He has also directed two feature films, MirrorMask and Luna. WWW.RICHARDDAWKINS.NET WWW.DAVEMCKEAN.COM AUDIO EDITION ALSO AVAILABLE MEET THE AUTHORS, WATCH VIDEOS AND MORE AT SimonandSchuster.com THE SOURCE FOR READING GROUPS COVER ILLUSTRATION BY DAVE MCKEAN Also by Richard Dawkins The Selfish Gene The Extended Phenotype The Blind Watchmaker River Out of Eden Climbing Mount Improbable Unweaving the Rainbow A Devil’s Chaplain The Ancestor’s Tale The God Delusion The Greatest Show on Earth ","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:13:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["Mythology"],"content":"INDEX ‘accident prone’, 226 Adam (and Eve), 35–7 , 58 , 59 , 220 agriculture, 46 aliens: abduction by, 182–6 fictional, 181 , 193–4 life on other planets, 14–15 , 186–93 myths and legends, 180–1 , 185–6 vision, 194–8 allergies, 233–4 amphibians, 50 ancestors, 38–43 , 46–50 , 52–3 , 70 Andersen, Hans Christian, 19 Anguilla, 66–7 , 68 ant lion, 227–8 apes, 48–9 , 60 , 72 aphelion, 115–16 , 118 , 120 Applewhite, Marshall, 181 Arctic tern, 107 Asclepius, 218 asteroids, 136–7 astrology, 219 Atlas, 163 atomic number, 92 , 171 atoms: compounds, 79–80 crystals, 80–3 , 88 elements, 79 inside the atom, 85–91 knowledge of, 15 , 79 mass, 91–2 , 93 models, 86–8 nucleus, 87–9 , 91–3 radioactive isotopes, 44 splitting, 86 Australian aborigines, 100–2 Aztec religion, 124–6 Babel, Tower of, 56 , 61–2 bacteria, 12–13 , 65 , 96 , 140 , 230–1 , 235 bad things, 216–17 , 220–3 , 226–7 Barotse tribe, 124 bats, 157 , 197 Beagle, HMS, 67 big bang model, 164–5 , 177 birds, 50 , 57 , 107 , 140 , 197 Blackmore, Sue, 185 Bohr, Niels, 87 Boshongo myth, 162 Brahma, 163 breeding: between different species, 42 , 59 , 65 , 68–9 gene pools, 73–5 horses and donkeys, 42 , 59 , 65 interbreeding, 47 , 49 , 71 Mendel’s experiments, 16–17 natural selection, 30–1 selective, 28–9 Brown, Derren, 20 Buckyballs and Buckytubes, 94–5 cancers, 234–5 carbon, 79 , 80–1 , 88 , 92–3 , 94–5 carbon-14, 46 , 93 cards, shuffling and dealing, 25–6 , 251–2 carnivores, 72 , 139–40 , 142 Cassini space probe, 116 chameleon, 217 chance, 23–6 , 220–1 , 223–5 chimpanzees, 18 , 48 , 51 , 52–3 , 72 Chinese myths, 162–3 chlorine ions, 82 chromosomes, 17 , 51 Chumash people, 148–9 Clancy, Susan, 182 Clarke, Arthur C., 256 clocks and watches, 243–4 clouds, 141–2 coaches, 19 , 23–4 , 26 , 31 , 238–9 , 253 coal, 141–2 , 192 Coatlicue, 125–6 coin tossing, 222 , 224–6 coincidence, 238 , 241–3 , 251 colours, 90 , 151–8 , 169–72 , 176 comets, 115–17 , 181 Conan Doyle, Sir Arthur, 246–7 conjurors, 20–1 , 252 continental drift, 208 , 210 continents, 205–9 , 210 , 212 convection currents, 211–12 , 213 Crick, Francis, 17–18 cricket, 224–6 crystals, 80–3 , 84 , 88 , 90 Darwin, Charles: on evolution, 27 , 29–30 Galapagos visit, 67–8 on natural selection, 29–30 , 74 , 227 , 229 tree picture, 60–1 , 64 dates, 45–6 , 93 day–night cycle, 100–2 , 106–7 death, 217 Demeter, 102–3 Democritus, 79 Devil, 220 , 247 dialects, 62 , 64 , 71 diamond, 80–1 , 82 , 88 dinosaurs, 12 , 13 , 14 , 50 , 137 disease, 217–20 , 231–5 distance, measuring, 166–8 diversity, 57–8 DNA, 16–18 , 50–3 , 64–5 , 67 , 70 , 73 Dogon tribe, 217 dogs, 18 , 59 , 156 , 217 , 233 dolphins, 72 , 149 , 197 Doppler, Christian, 175–6 Doppler shift, 173 , 175–6 , 188 dreams, 184 , 241–3 Dreamtime, 100 duck-billed platypus, 49 , 198 dust mites, 96 Earth: axis, 104–5 , 111 , 118–20 centre, 85 , 212 convection currents, 212 , 213 orbit, 103 , 108–9 , 115 , 118–19 , 134 , 166–7 , 191 sea-floor spreading, 210–12 spinning, 103–5 tectonic plates, 209–14 , 223 earthquakes: causes, 208 , 213–14 , 223 diseases, 219 experiences of, 200–1 myths, 202–5 Eden, 36 Egyptian religion, 127 electrons, 87–9 , 91–3 , 171 elements, 78 , 79 , 92–3 , 133–4 , 170–2 ellipses, 113–15 , 117 emotions, 18 energy, 138–43 Eta Carinae, 130 , 133–4 Europa, 190 , 191 evaporation, 141 evolution: auto-immune diseases, 235 Galapagos islands, 67–71 gene pools, 74–5 gradual, 26–7 languages, 57 , 63–5 , 66 , 68 , 71 natural selection, 30–1 , 68 , 70 , 75 , 227–9 pregnancies, 233 selective breeding, 28–9 tree picture, 61 eyes, 194–7 faces, seeing, 240 fairies, photographs of, 245–6 fairy godmother, 23–4 false memory syndrome, 183 , 185 Fatima, miracle of, 247–9 Feynman, Richard, 243 fish, 40–1 , 43 , 48 , 50 , 66 , 198 fossils, 13 , 43–5 , 60 , 93 Franklin, Rosalind, 18 frogs, 23 , 26 , 27–31 , 50 , 52 , 66 fungi, 140 , 230 Galapagos islands, 67–71 galaxies, 13 , 14 , 165–8 , 172–3 , 176–7 gas giants, 190 gases, 79 , 83 , 85 , 89 gene: flow, 64 , 66 , 73 pool, 73–5 genes, 16–17 , 29–30 , 51–3 , 6","date":"2022-09-07","objectID":"/b27_the_magic_of_reality/:14:0","tags":["Mythology and Modern Science"],"title":"The Magic of Reality","uri":"/b27_the_magic_of_reality/"},{"categories":["probability"],"content":"PREFACE Join me in my odyssey through the worlds of science, gambling, and the securities markets. You will see how I overcame risks and reaped rewards in Las Vegas, Wall Street, and life. On the way, you will meet interesting people from blackjack card counters to investment experts, from movie stars to Nobel Prize winners. And you’ll learn about options and other derivatives, hedge funds, and why a simple investment approach beats most investors in the long run, including experts. I began life in the Great Depression of the 1930s. Along with millions of others, my family was struggling to get by from one day to the next. Though we didn’t have helpful connections and I went to public schools, I found a resource that made all the difference: I learned how to think. Some people think in words, some use numbers, and still others work with visual images. I do all of these, but I also think using models. A model is a simplified version of reality, like a street map that shows you how to travel from one part of a city to another or the vision of a gas as a swarm of tiny elastic balls ceaselessly bouncing against one another. I learned that simple devices such as gears, levers, and pulleys follow basic rules. You could discover the rules by experimenting and, if you got them right, could then use the rules to predict what would happen in new situations. Most amazing to me was the magic of a crystal set—an early primitive radio made with wire, a mineral crystal, and headphones. Suddenly, I heard voices coming from hundreds or thousands of miles away, carried through the air by some mysterious process. The notion that things I couldn’t even see followed rules I could discover just by thinking—and that I could use what I discovered to change the world—inspired me from an early age. Because of circumstances, I was largely self-taught and that led me to think differently. First, rather than subscribing to widely accepted views—such as you can’t beat the casinos—I checked for myself. Second, since I tested theories by inventing new experiments, I formed the habit of taking the result of pure thought—such as a formula for valuing warrants—and using it profitably. Third, when I set a worthwhile goal for myself, I made a realistic plan and persisted until I succeeded. Fourth, I strove to be consistently rational, not just in a specialized area of science, but in dealing with all aspects of the world. I also learned the value of withholding judgement until I could make a decision based on evidence. I hope my story will show you a unique perspective and that A Man for All Markets will help you think differently about gambling, investments, risk, money management, wealth-building, and life. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:1:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"FOREWORD Ed Thorp’s memoir reads like a thriller—mixing wearable computers that would have made James Bond proud, shady characters, great scientists, and poisoning attempts (in addition to the sabotage of Ed’s car so he would have an “accident” in the desert). The book reveals a thorough, rigorous, methodical person in search of life, knowledge, financial security, and, not least of all, fun. Thorp is also known to be a generous man, intellectually speaking, eager to share his discoveries with random strangers (in print but also in person)—something you hope to find in scientists but usually don’t. Yet he is humble—he might qualify as the only humble trader on planet Earth—so, unless the reader can reinterpret what’s between the lines, he or she won’t notice that Thorp’s contributions are vastly more momentous than he reveals. Why? Because of their simplicity. Their sheer simplicity. For it is the straightforward character of his contributions and insights that made them both invisible in academia and useful for practitioners. My purpose here is not to explain or summarize the book; Thorp—not surprisingly—writes in a direct, clear, and engaging way. I am here, as a trader and a practitioner of mathematical finance, to show its importance and put it in context for my community of real-world scientist-traders and risk-takers in general. That context is as follows. Ed Thorp is the first modern mathematician who successfully used quantitative methods for risk taking—and most certainly the first mathematician who met financial success doing it. Since then there has been a cohort of such “quants”, such as the whiz kids in applied mathematics at SUNY Stony Brook—but Thorp is their dean. His main and most colorful predecessor, Girolamo (sometimes Geronimo) Cardano, a sixteenth-century polymath and mathematician who—sort of—wrote the first version of Beat the Dealer, was a compulsive gambler. To put it mildly, he was unsuccessful at it—not least because addicts are bad risk-takers; to be convinced, just take a look at the magnificence of Monte Carlo, Las Vegas, and Biarritz, places financed by their compulsion. Cardano’s book Liber de ludo aleae (“Book on Games of Chance”) was instrumental in the later development of probability, but, unlike Thorp’s book, was less of an inspiration for gamblers and more for mathematicians. Another mathematician, a French Protestant refugee in London, Abraham de Moivre, a frequenter of gambling joints and the author of The doctrine of chances: or, a method for calculating the probabilities of events in play (1718) could hardly make both ends meet. One can easily count another half a dozen mathematician-gamblers, including greats like Fermat and Huygens—who were either indifferent to the bottom line or not particularly good at mastering it. Before Ed Thorp, mathematicians of gambling had their love of chance largely unrequited. Thorp’s method is as follows: He cuts to the chase in identifying a clear edge (that is something that in the long run puts the odds in his favor). The edge has to be obvious and uncomplicated. For instance, calculating the momentum of a roulette wheel, which he did with the first wearable computer (and with no less a coconspirator than the great Claude Shannon, father of information theory), he estimated a typical edge of roughly 40 percent per bet. But that part is easy, very easy. It is capturing the edge, converting it into dollars in the bank, restaurant meals, interesting cruises, and Christmas gifts to friends and family—that’s the hard part. It is the dosage of your betting—not too little, not too much—that matters in the end. For that, Ed did great work on his own, before the theoretical refinement that came from a third member of the Information Trio: John Kelly, originator of the famous Kelly Criterion, a formula for placing bets that we discuss today because Ed Thorp made it operational. A bit more about simplicity before we discuss dosing. For an academic judged by hi","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:2:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 1: LOVING TO LEARN My first memory is of standing with my parents on an outdoor landing at the top of some worn and dirty wooden steps. It was a gloomy Chicago day in December 1934, when I was two years and four months old. Even wearing my only set of winter pants and a jacket with a hood, it was cold. Black and leafless, the trees stood out above the snow-covered ground. From inside the house a woman was telling my parents, “No, we don’t rent to people with children.” Their faces fell and they turned away. Had I done something wrong? Why was I a problem? This image from the depths of the Great Depression has stayed with me always. I next recall being taken at age two and a half to our beloved family physician, Dr. Dailey. My alarmed parents explained that I had yet to speak a single word. What was wrong? The doctor smiled and asked me to point to the ball on his desk. I did so, and he asked me to pick up his pencil. After I had done this and a few more tasks he said, “Don’t worry, he’ll talk when he’s ready.” We left, my parents relieved and a little mystified. After this, the campaign to get me to talk intensified. About the time of my third birthday, my mother and two of her friends, Charlotte and Estelle, took me along with them to Chicago’s then famous Montgomery Ward department store. As we sat on a bench near an elevator, two women and a man got off. Charlotte, keen to tempt me into speech, asked, “Where are the people going?” I said clearly and distinctly, “The man is going to buy something and the two women are going to the bathroom to do pee-pee.” Charlotte and Estelle both blushed deeply at the mention of pee-pee. Far too young to have learned conventional embarrassment, I noticed this but didn’t understand why they reacted that way. I also was puzzled by the sensation I had caused with my sudden change from silence to talkativeness. From then on I spoke largely in complete sentences, delighting my parents and their friends, who now plied me with questions and often received surprising answers. My father set out to see what I could learn. Born in Iowa in 1898, my father, Oakley Glenn Thorp, was the second of three children, with his brother two years older and sister two years younger. When he was six his family broke up. His father took him and his brother to settle in the state of Washington. His mother and sister remained in Iowa. In 1915 my grandfather died from the flu, three years before the Great Flu Pandemic of 1918–19, which killed between twenty and forty million people worldwide. The two boys lived with an uncle until 1917. Then my father, at age eighteen, went to France to join World War I as part of the great American Expeditionary Force. He fought with the infantry in the trenches, rose from private to sergeant, and was awarded the Bronze Star, the Silver Star, and two Purple Hearts for heroism in places like Château-Thierry, Belleau Wood, and the Battles of the Marne. As a very small boy I remember sitting in his lap on a humid afternoon examining the shrapnel scars on his chest and the minor mutilation of some of his fingers. Following his discharge from the army after the war, my father enrolled at Oklahoma A\u0026M. He completed a year and a half before he had to leave for lack of funds, but his hunger and respect for education endured and he instilled them in me, along with his unspoken hope that I would achieve more. Sensing this and hoping it would bring us closer, I welcomed his efforts to teach me. As soon as I began to talk, he introduced me to numbers. I found it easy to count first to a hundred, then to a thousand. Next I learned how to increase any number by adding one to get the next number, which meant I could count forever if I only knew the names of the numbers. I soon learned how to count to a million. Adults seemed to think this was a very big number so I sat down to do it one morning. I knew I could eventually get there but I had no idea how long it was going to take. To get star","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:3:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 2: SCIENCE IS MY PLAYGROUND In the 1940s, graduates of Narbonne High School were not expected to go on to college. The course requirements reflected this. In the seventh and eighth grades, though hungry for academic learning, I was required to take practical subjects, including wood shop, metal shop, drafting, typing, print shop, and electric shop. I wanted to pursue my interest in radio and electronics, which was sparked a couple of years earlier when I got one of the first simple radios, a crystal set. Made with a rectifier of galena, which was a shiny black crystal, a wire called a cat’s whisker for touching it in the right spot, and a coil of wire, it had earphones, an antenna wire, and a variable capacitor for tuning in different stations. Then like magic: Through my earphones came voices from the air! The mechanical world of wheels, pulleys, pendulums, and gears was ordinary. I could see, touch, and watch it in action. But this new world was one of invisible waves that traveled through space. You had to figure out through experiments that it was actually there and then use logic to grasp how it worked. It was no surprise that the required course that caught my interest was electric shop, where we each had to build a small operational electric motor. The teacher, Mr. Carver, was a universally liked, plump, avuncular man whom the other teachers called Bunny. I suspect that Jack Chasson had a word with him, for somehow he learned of my interest in electronics and told me about the world of amateur radio. At that time there already was a web of do-it-yourselfers who built or bought their own radio transmitters and receivers and talked night and day by voice or by Morse code all over the globe. It was in effect the first Internet. With less electricity than it takes to power a lightbulb, I could talk to people around the world. I asked Mr. Carver how I could be part of it. He told me that all I needed to do was pass what was then a rather difficult examination. In those days the exam began with a series of written questions on radio theory. Next was a test on Morse code. That hurdle, since relaxed, was a major obstacle for most, and Mr. Carver warned me about the long, tedious hours of practice needed for proficiency. We had to copy code as well as send it with a telegrapher’s key at an error-free rate of thirteen words per minute. A word meant any five characters, so this was sixty-five characters a minute, or a little faster than one per second. I thought about it, then went out and bought a used “tape machine” for what was then the enormous sum of $15, almost three weeks’ income from delivering newspapers. The machine looked like a stubby black shoe box. The lid unclipped to reveal two spindles. It came with a collection of reels of pale-yellow paper tapes. These tapes had short holes for the “dots” and long holes for the “dashes.” You could look at them and read off the code for letters, and thus “read” the tape. The machine wound the tape from one spindle to the other, like the old reel-to-reel high-fidelity music tape players and the later cassette-tape machines. For power, you simply wound up the machine with a crank. It was simple, low-tech, and effective. When a hole moved past a spring contact, the circuit closed for the length of time its journey took. Long holes gave dashes, and short holes gave dots. The box was hooked to a simple device, an “audio oscillator,” that emitted a fixed tone such as the piano middle C. As the tape ran, the contact in the box switched the oscillator alternately on and off, sending dots and dashes. The great thing about the machine as a teaching aid was that its speed was adjustable, from the slow rate of one word per minute up to fast rates like twenty-five words per minute. My plan was to understand every tape at a slow rate, then speed the tapes up slightly and master them again. To motivate our class and give us a benchmark, Mr. Carver showed us a chart of the rate of progre","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:4:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 3: PHYSICS AND MATHEMATICS In August 1949, as I was turning seventeen, I went off to the Berkeley campus of the University of California. Now divorced, my mother sold our house, moved away, and installed my twelve-year-old brother in military school. It would be several years before I saw much of either parent again, an echo of my father’s experience, as he was without parents and on his own from age sixteen. He had joined the army, and I went off to university. Like him, I was on my own from then on. I found room and board a few blocks south of the campus. Shortly before leaving for college, I learned that my mother had cashed the war bonds I had paid for with my paper route and spent the money. Her unexpected betrayal was an emotional blow that estranged us for years, and whether I could support myself at the university was now in doubt. I survived with scholarships, part-time jobs, and $40 a month for my first year from my father. I got by on less than $100 a month, including everything: books, tuition, food, shelter, and clothes. On Sundays, when my boardinghouse did not provide meals, I visited church open houses, where I consumed large quantities of free hot chocolate and doughnuts. The campus was overflowing with returning veterans on the GI Bill. Basic science courses such as physics and chemistry were taught in classrooms of several hundred, but the best professors were lecturing and quality was high. In chemistry, my major, I was one of fifteen hundred students. We were split into four lecture sections of almost four hundred each. The course was taught by a famous professor, and we were using his book. As he was then preparing a revision, he offered 10 cents per misprint to the first student to report it. I set to work and soon brought him a list of ten errors to see if he would pay. He gave me my dollar. Encouraged, I came back with a list of seventy-five more mistakes. That netted me $7.50 but he wasn’t happy. When I returned a few days later with several hundred he explained that they needed to be errors, not mere misprints. Despite my objections, he disqualified nearly all of them. This unilateral retroactive change in the deal, which I would later encounter often on Wall Street, done by someone for their benefit just because they could get away with it, violated my sense of fair play. I quit reporting additional corrections. As the semester wound to a close, I had missed only a single point out of the hundreds given out for the written exams and the lab work, ranking me number one. After my unfortunate experience with the chemistry exam in high school, this was vindication. Part of our grade came when we were asked each week to chemically analyze a sample that was not known to us. After hearing that some students might sabotage others by secretly changing these unknowns, I made a practice of holding back part of mine so that, if this were done to me, I could prove that I had correctly analyzed whatever I had. On the very last sample given us to evaluate that semester, I was told I got it wrong. I knew better, and to prove it I asked that the part I had saved be tested. The decision on my appeal was left to the teaching assistant for my lab sections, who refused to act. The points I lost caused me to end the term in fourth place rather than first. Outraged, I did not enroll in chemistry the second semester and changed my major to physics. Thus I missed organic chemistry, the study of carbon compounds, and the basis for all living things. It is fundamental for biology. This rash decision, which led me to change my school and my major subject, would change my whole path in life. In hindsight, it turned out for the best, as my interests and my future were in physics and mathematics. Decades later, when I wanted to know some organic chemistry to explore ideas for extending healthy human longevity, I learned it as I needed to. Although it wasn’t as good a school then in math and physics as Berkeley, I transfer","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:5:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 4: LAS VEGAS Vivian and I decided to spend some of our Christmas holidays in Las Vegas because Las Vegas, to attract gamblers, had turned itself into a bargain vacation spot. As a twenty-six-year-old with a PhD in mathematics, I was earning too little at UCLA to treat money casually. I also believed then, as I do now after more than fifty years as a money manager, that the surest way to get rich is to play only those gambling games or make those investments where I have an edge. Since I knew of no one who had ever found a way to beat the casinos, gambling in Vegas wasn’t on my list of priorities. Seeing Las Vegas back in 1958, I did not imagine today’s glittering strip of giant high-rise hotels, jammed together helter-skelter, fronted at all hours by multi-lane, gridlocked traffic. Legendary casinos such as the Sands, Flamingo, Dunes, Riviera, and Tropicana are gone, the mob and cash-skimming operations replaced by multibillion-dollar public companies. Back then the long, straight, uncrowded highway had a dozen or so one-story hotel-casino complexes scattered on either side with hundreds of yards of sand and tumbleweeds separating them. Just before we left for our vacation, my colleague Professor Robert Sorgenfrey told me about a new strategy for playing blackjack that claimed to give players the smallest house edge of any casino game. Next best was baccarat, with a house advantage as low as 1.06 percent, and then craps, with some bets costing just 1.41 percent. The new figure of 0.62 percent for the house edge in blackjack was so close to even that I planned to risk a few dollars for fun. Devised by four mathematicians during their time in the army, the strategy covered several hundred possible decisions a player might face. I condensed its main features onto a card that fit into the palm of my hand. My only experience in a casino had been my earlier adventure putting a few coins in a slot machine. After settling into our hotel room, we headed for the casino. Weaving past drinkers, smokers, and slot machines, I found two rows of blackjack tables, separated by an aisle or “pit” complete with reserves of chips, extra cards, and cocktail waitresses who offered alcoholic nirvana to the marks, or suckers, all of whom the pit boss monitored closely. It was early afternoon and the few open tables were busy. Managing to get a seat, I plunked down my entire stake—a stack of ten silver dollars—on the green felt table behind my “betting spot.” I didn’t expect to win, since the odds were slightly against me, but as I expected to build a device to successfully predict roulette and had never gambled before, it was time to get casino experience. I knew virtually nothing about casinos, their history, or how they operated. I was like a person who had glanced at recipes but never been in a kitchen. The game I was about to play, blackjack, or twenty-one, was basically the same as the Spanish game of twenty-one, referred to as early as 1601 in a story by Cervantes. During the mid-eighteenth century, as part of the European gambling craze of that time, the French called it vingt-et-un. Later, when the game was introduced in the twentieth century to US gaming establishments, bonuses were sometimes offered for special combinations of cards, among them a ten-to-one payoff if a player’s first two cards were the Ace of Spades plus one of the two black Jacks, a “blackjack.” Though the bonus was soon rescinded, this name for the game stuck, and any two-card total of 21—that is, any Ace plus any Ten-value card—is now called blackjack as well. The action starts when the players place their bets on “spots” in front of them, after which the dealer gives two cards to everyone and also two cards to himself. The first of the dealer’s cards is dealt faceup for everyone to see, and the second is placed facedown under it. Then, starting with the player on his left, the dealer asks each participant in turn how he wants to play his hand. The goal of both pl","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:6:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 5: CONQUERING BLACKJACK It wasn’t the money that drew me to blackjack. Though we could certainly use extra dollars, Vivian and I expected to lead the usual low-budget academic life. What intrigued me was the possibility that merely by sitting in a room and thinking, I could figure out how to win. I was also curious to explore the world of gambling, about which I knew nothing. Back from Las Vegas, I headed for the section in the UCLA library where the mathematical and statistical research articles were kept. Grabbing from its shelf the volume containing the article with the strategy I had played in the casino, I stood and began to read. As a mathematician, I had heard that winning systems were supposed to be impossible; I didn’t know why. I did know that the theory of probability had begun more than four hundred years earlier with a book on games of chance. Attempts to find winning systems over the following centuries stimulated the development of the theory, eventually leading to proofs that winning systems for casino gambling games were, under most circumstances, impossible. Now I benefited from my habit of checking it out for myself. As my eyes gobbled up equations, suddenly I saw both why I could beat the game and how to prove it. I started with the fact that the strategy I had used in the casino assumed that every card had the same chance of being dealt as any other during play. This cut the casino’s edge to just 0.62 percent, the best odds of any game being offered. But I realized that the odds as the game progressed actually depended on which cards were still left in the deck and that the edge would shift as play continued, sometimes favoring the casino and sometimes the player. The player who kept track could vary his bets accordingly. With the help of a mental picture based on ideas from an advanced mathematics course, I believed the player edge must often be substantial. Moreover, and this also was new, I saw how the player could condense and use this information in actual play at the table. I decided to begin by finding the best strategy to use when I knew which cards had already been played. Then I could bet more when the odds were in my favor and bet less otherwise. The casino would win more of the small bets, but I would win a majority of the big wagers. And if I bet enough where I had an advantage, I should eventually get ahead and stay ahead. I left the UCLA library and went home to figure out the next steps. Almost at once, I wrote to Roger Baldwin, one of the four authors of the blackjack article, asking for details about the calculations, telling him I wished to extend the analysis of the game. He generously sent me the actual computations a few weeks later, consisting of two large boxes of lab manuals filled with thousands of pages of calculations done by the authors on desk calculators while they served in the army. During the spring of 1959, wedged in between my teaching duties and research in the UCLA Mathematics Department, I mastered every detail, my excitement mounting as I strove to speed up the enormous number of calculations that lay between me and a winning system. The Baldwin strategy was the best way to play the game when nothing was known about which cards had already been played. Their analysis was for a single deck because that was the only version played in Nevada at the time. The Baldwin group also showed that the advice of the reigning gambling experts was poor, unnecessarily giving the casinos an extra 2 percent advantage. Any strategy table for blackjack must tell the player how to act for each case that can arise from the ten possible values of the dealer’s upcard versus each of the fifty-five different pairs of cards that can be dealt to the player. To find the best way for the player to manage his cards in each of these 550 different situations, you need to calculate all the possible ways subsequent cards can be dealt and the payoffs that result. There may be thousands, even mi","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:7:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 6: THE DAY OF THE LAMB I flew into Washington, DC, with its leaden wintry skies and the first snow flurries of what would become a gigantic storm. The city was still packed with people after the recent inauguration of the new president of the United States, John F. Kennedy. The meeting of the American Mathematical Society was held in the old Willard Hotel. Instead of the anticipated scholarly audience of forty or fifty, I found an animated standing-room-only crowd of hundreds. Scattered among the mathematicians were others who were sporting sunglasses, gaudy oversized pinkie rings and cigars, as well as reporters with cameras and notepads. In the manner of mathematics meetings, I had prepared a matter-of-fact talk, which I began with an explanation of how to win by counting 5s. I went on to mention that counting Tens was much better, and then hinted at the cornucopia of additional counting systems revealed by my methodology. My terse technical presentation didn’t deter my audience. I finished and placed a woefully inadequate fifty copies of my speech on the table in front of me. The group surged toward them like carnivores competing for fresh meat. Responding to requests, the officials in charge of the meeting arranged a press conference for me following my talk, after which I was televised by a major network and interviewed on several radio programs. The scientists and technical types generally understood and believed the winning strategy I described, but the casinos and some of the press did not. In a cynical editorial, The Washington Post said there was a mathematician in town claiming to have a winning gambling system, which reminded them of this ad: Send $1 for a surefire weed killer. Back comes a note saying “Grab by the roots and pull like hell.” One casino spokesman mockingly claimed they sent cabs to the airport to meet players with systems. (I’ve waited over fifty years now for those cabs.) Another told of how I had sent a detailed questionnaire asking about the precise blackjack rules used in his gaming house. He stated that I was so ignorant, I didn’t even know the rules of the game. When starting my calculations a couple of years earlier, I had, in fact, sent just such an inquiry to twenty-six Nevada casinos. My object was to learn how the rules varied from one establishment to another, in particular to see if some places had rules even more favorable than usual. Thirteen of the twenty-six casinos were kind enough to reply to an ignorant academic. A young reporter for the Post named Tom Wolfe followed up after my talk with an interview. The Post ran his story, “You Can So Beat the Gambling House at Blackjack, Math Expert Insists.” He was curious rather than skeptical, sympathetic but probing. Wolfe later became one of America’s most famous authors. By this time, Washington’s airports were buried in two feet of snow, so I boarded a train for Boston. During the long ride back I wondered how my research into the mathematical theory of a game might change my life. In the abstract, life is a mixture of chance and choice. Chance can be thought of as the cards you are dealt in life. Choice is how you play them. I chose to investigate blackjack. As a result, chance offered me a new set of unexpected opportunities. Ever since my first meeting with Claude Shannon in September, we had been working on the roulette project approximately twenty hours a week. Meanwhile, I was teaching courses, doing research in pure mathematics, attending department functions, writing up my blackjack research, and adjusting to being a new father. Following a roulette work session at the Shannons’, Claude asked me at dinner if I thought anything would ever top this in my life. My thoughts then were much like I expected his to have been: that acknowledgment, applause, and honor are welcome and add zest to life but they are not ends to be pursued. I felt then, as I do now, that what matters is what you do and how you do it, the quality of t","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:8:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 7: CARD COUNTING FOR EVERYONE Back at the Massachusetts Institute of Technology, I attracted attention in the cafeteria when, once a week, I cashed yet another $100 bill from my casino winnings. With the way our currency has depreciated since 1961, the impact then was almost the same as if I were to pay today with $1,000 bills. Meanwhile, my two-year appointment at MIT would end June 30, just three months away. The department chairman, W. T. “Ted” Martin, encouraged me to stay a third year at MIT and told me how highly I was regarded by institute professor Shannon. There was a chance this could lead to a permanent position either then or at a later date. Whether to try for this was a difficult decision. MIT had become one of the world’s great mathematics centers, following its transformation by projects for the government during World War II from a technical school to a scientific powerhouse. Simply walking down the hall, I would chat with people like the prodigy Professor Norbert Wiener (cybernetics) and the future Abel Prize winner Isadore Singer. The C. L. E. Moore Instructorship program, of which I was part, had brought in new PhDs like John Nash, who later won the Nobel for economics, and future Fields Medal winner Paul Cohen. Though there’s no Nobel Prize for mathematics, the Fields and the Abel prizes have that status. Cohen had left a few days before I arrived; his name was just being scraped off his door. I finally decided not to stay on. From a career standpoint, I thought I had the talent to keep up with the big boys but I felt I needed more mathematical background. I also hadn’t collaborated on research with a senior faculty mentor or other colleagues in my area of specialty, and such work with others often is key to advancing in an academic department. Instead, I had spent much of my time working on blackjack and on building a computer with Professor Shannon to predict roulette. However, my work with Shannon wasn’t part of any academic field. It wasn’t mathematics per se and had no constituency and no name. It couldn’t help my academic career. Ironically, thirty years later MIT had become a world leader in the development of what would be called wearable computers, and the time line placed on the Internet by its Media Lab credited Shannon and me with building the first one. New Mexico State University was bidding for bright young faculty members and subsidizing an incoming supply of good graduate students. They had just received a $5 million post-Sputnik Centers of Excellence Grant from the National Science Foundation, an amount equivalent to more than $40 million today, with a mandate to build a PhD program over the next four years. They proposed to jump my pay, from the $6,600 that both MIT and the University of Washington offered, to $9,000 a year and promote me to associate professor with tenure. I would also have a six-hour-a-week teaching load consisting of my choice of graduate courses. It provided the opportunity I wanted to expand my mathematical background, learning through teaching, doing my own research, directing doctoral theses, and collaborating with my students. The position in New Mexico seemed to me to be the best next career step, even though my colleagues regarded it as an ill-advised gamble at what had been a mathematical backwater. Most important, a move to New Mexico would take Vivian and baby Raun to a much better climate and closer to our families. As I was making this decision I agreed to write a book about blackjack. This came about after I mentioned my successful casino test to a few of my friends. The MIT grapevine did the rest. Yale Altman, representing the academic publisher Blaisdell (then a subsidiary of Random House), invited me to propose a book. I gave him the ten chapter headings from an outline I was already sketching, and he accepted it enthusiastically. My working title was Fortune’s Formula: A Winning Strategy for Blackjack. Then Random House took the project away f","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:9:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 8: PLAYERS VERSUS CASINOS After my book appeared, legions of blackjack players hit the tables in Nevada. Anyone could bring the book’s palm-sized strategy cards and find a game with rules good enough to let them play on level terms with the casino, even without counting cards. Then there were the card counters and would-be card counters. Many were good and some would go on to make their living from blackjack, but for the majority the effort and persistence required to practice card counting, the restraint and discipline needed, to say nothing of the temperament, were obstacles to success. Still, the fact that blackjack could be beaten led to an upsurge in play. As a result, during the next few decades blackjack displaced craps as the dominant table game. However, the casinos were in a bind: Should they let the minority of players who were counters beat them in return for the vastly increased revenues from the great majority of players who couldn’t or wouldn’t count, or should they try to choke off the card counters with countermeasures, even if this would slow the boom in blackjack play? When the casinos first tried a rules change and lost more in revenues than they gained in benefits, they went back to the old rules. Next, they brought in dealing boxes known as shoes, which allowed the use of four, six, or even eight decks. This was supposed to make card counting more difficult. But for those who used the High–Low System, it wasn’t much harder. That was because the correct play of the hand was pretty much the same for various numbers of decks and because the High–Low System already adjusted for the number of unused cards, whether the game was played with one or several decks. The good players, who were getting better with practice, continued to win. The most widely used photo gallery of undesirables was developed for the casinos by Griffin Investigations, Inc., a private detective agency founded in 1967 by Beverly and Robert Griffin. The usual collection of criminals, player cheats, and public nuisances was rapidly expanded by the addition of ever more card counters. They were barred on sight, and their descriptions were shared among the casinos. However, dealers and pit bosses often couldn’t figure out who was counting and who wasn’t. Non-counters who inadvertently aroused suspicion were, to their bafflement, forbidden to play. Players were cheated and beaten in back rooms. Eventually the Griffin agency was successfully sued by two top card counters, one of whom was James Grosjean, a member of the Blackjack Hall of Fame, and the firm filed for bankruptcy in 2005. Card counters formed informal networks and developed new and improved techniques. Beat the Dealer had introduced the idea of a team. Suppose several players, say five, each with a $10,000 bankroll, are playing separately, winning at an average rate of 1 percent or $100 an hour. Then the five players together will gain an average of $500 an hour. If instead they pool their money into one $50,000 bank, when one of them plays he can bet five times as much as he could safely risk on his own $10,000. Consequently he expects to win five times as much, namely, 1 percent of $50,000 or $500 per hour, rather than $100. But it gets better. The four other players can all be playing, too, typically at different tables or casinos, acting as though they each have a $50,000 bankroll, so the group makes $2,500 per hour when all are playing whereas, playing without pooling their bankrolls, they would make collectively only $500 an hour. The next step was obvious. Entrepreneurs went into the blackjack business by recruiting and training players, providing a bankroll, and sharing the profits between the players and the financier. Notable teams include Tommy Hyland’s and the now famous MIT group, chronicled in the book Bringing Down the House, which inspired the 2008 movie 21. Al Francesco pioneered the creation of blackjack teams, and the idea was well publicized by one of his ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:10:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 9: A COMPUTER THAT PREDICTS ROULETTE The modern form of roulette seems to have first appeared in Paris in 1796. It became the favored high-stakes game of the rich and the royal, enshrined in Monte Carlo in the nineteenth century, and celebrated in story and song. With its high stakes, splendid settings, and runs of extreme luck, which were sometimes good but more often bad, it was a target for those with systems attempting to overcome the casino’s advantage. These systems were too complex for gamblers to analyze precisely, but they had plausible features that inspired hope. A favorite was the Labouchère, or cancellation, system. This was used in roulette for bets that paid even money, where you win or lose an amount equal to your bet. Among the even-money bets in roulette are wagers on red or on black, each of which has eighteen chances in thirty-eight of winning. To start the Labouchère, write down a string of numbers, such as 3, 5, and 7. The total of these, 15, is what you try to win. Your first bet is the total of the first and last numbers in the string, 3 + 7, or 10. If you win, cross off the first and last numbers, leaving only 5. Your next bet is 5, and if you win you have reached your goal. If you lose, add 10 to the string so it becomes, 3, 5, 7, 10 and then bet 3 + 10 or 13. In any case, each time you lose you add one number to the string, and each time you win you cross off two numbers. Therefore, you need to win only a little over a third of the time to reach your goal. What can go wrong? Gamblers, trying systems like the Labouchère, were baffled when they never seemed to prevail. However, using the mathematical theory of probability, it was proven that if all roulette numbers were equally likely to come up, and they appeared in random order, it was impossible for any betting system to succeed. Despite this, hope flared briefly at the end of the nineteenth century when the great statistician Karl Pearson (1857–1936) discovered that the roulette numbers being reported daily in a French newspaper showed exploitable patterns. The mystery was resolved when it was discovered that rather than spend hours watching the wheels, the people recording the numbers simply made them up at the end of each day. The statistical patterns Pearson detected simply reflected the failure of the reporters to invent perfectly random numbers. If betting systems don’t work, what about defective wheels where, in the long run, some numbers will come up more than others? In 1947, two graduate students at the University of Chicago, Albert Hibbs (1924–2003) and Roy Walford (1924–2004), found a roulette wheel in Reno that seemed to favor the number 9. They increased an initial stake of $200 to $12,000. The next year they found a wheel at the Palace Club in Las Vegas on which they made $30,000. They took a year off and sailed the Caribbean, then went on to distinguished careers in science. Among many accomplishments, Hibbs became director of space science for Caltech’s Jet Propulsion Laboratory, and Walford became a UCLA medical researcher who showed that caloric restriction in mice could more than double their maximum life span. Hibbs later wrote, “I wanted to conquer space, and my roommate, Roy Walford, decided that he would conquer death.” Feynman must have known about biased wheels when he told me there was no way to beat the game, because Hibbs got his PhD in physics under Feynman at Caltech the previous year. In any case, biased wheels at big casinos were likely a thing of the past, as gambling houses took better care of their equipment. So this was the setting when Claude Shannon and I, in September 1960, set to work to build a computer to beat roulette. So far as we knew, everyone else thought physical prediction was impossible. As it was the last year of my two-year appointment at MIT, we had to complete the task in nine months. We spent twenty hours a week at the Shannons’ three-story wooden house. Dating from 1858, it was sited o","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:11:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 10: AN EDGE AT OTHER GAMBLING GAMES In September 1961, a month after our test of the roulette computer in Las Vegas, Vivian, Raun, and I moved to Las Cruces, New Mexico, where I began my duties as a professor in the Mathematics Department at New Mexico State University. Then a town of thirty-seven thousand people, located in the high desert about four thousand feet above sea level, Las Cruces was established near a principal source of the state’s water, the Rio Grande. Towns were widely spaced in the desert expanse, and the nearest population center was El Paso, Texas, about forty-five miles to the south. After the University of New Mexico in Albuquerque, some two hundred miles to the north, NMSU was the next most important campus in the state university system. When I arrived it was being transformed from an agricultural college to a university. Just to the east of the campus was “A” mountain, a large hill with an enormous white A for “Aggies.” Some claimed that when the football team learned the first letter of the alphabet, it would be changed to B. Our four years in New Mexico were memorable. Our younger daughter, Karen, was born there and our son, Jeff, was born in nearby El Paso. About twenty miles away was White Sands Proving Ground and National Monument, where we found some relief from summer heat, as the sun’s rays were efficiently reflected away by the white gypsum “sand.” I followed up on my childhood interest in astronomy, enjoying New Mexico’s dark skies through a small telescope. The astronomical highlight was a private lunch with Las Cruces resident and fellow NMSU professor Clyde Tombaugh (1906–97), who became world famous in 1930 when, at the Lowell Observatory in Flagstaff, Arizona, he discovered the planet Pluto (recently demoted to a “dwarf planet”). My student William E. “Bill” Walden, who worked at Los Alamos, arranged for me to spend an afternoon there with Stanislaw Ulam (1909–84), one of the twentieth century’s greatest mathematicians. Ulam, part of the Manhattan Project that developed the atomic bomb, later supplied crucial ideas for the hydrogen bomb—the Ulam-Teller concept for thermonuclear weapons. While teaching graduate courses and doing mathematical research at NMSU, I wondered whether what I had learned so far would enable me to beat other gambling games. One of the casino games I noticed on my Nevada blackjack trips was baccarat, which James Bond plays both in Ian Fleming’s book Casino Royale and in the dramatic beginning of the original version of the movie of the same name. Long played in Europe for high and sometimes unlimited stakes, this Continental favorite had been introduced in a slightly modified form by a few Las Vegas casinos. With similarities to blackjack, baccarat was a natural target for my methods. Fortunately, Bill Walden, a computer scientist with an interest in applying mathematics, was happy to be recruited. We began our analysis of baccarat in 1962, with the goal of finding to what extent we might be successful using my card counting methods. Nevada-style baccarat was dealt from eight decks, totaling 416 cards. These have the same values as in blackjack, except only the last digit counts. Thus Aces are 1, 2s through 9s are their numerical value, and 10s, Jacks, Queens, and Kings count as 0, not 10. The game begins after the cards are shuffled and a blank “cut card” is inserted into the pack of cards faceup near the end. The 416 cards are then put into a wooden dealing box known as a shoe. The first card is exposed, its value noted, and this number of cards is discarded, or “burned.” If the exposed card is a 10 or a face card, then ten cards are burned. A casino table had twelve seats, occupied by an assortment of customers and shills (house employees who bet money and may pretend to be players in order to attract customers). There are two main bets on the layout: Banker and Player. After the players place their wagers, the croupier deals two cards each facedown to ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:12:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 11: WALL STREET: THE GREATEST CASINO ON EARTH Gambling is investing simplified. The striking similarities between the two suggested to me that, just as some gambling games could be beaten, it might also be possible to do better than the market averages. Both can be analyzed using mathematics, statistics, and computers. Each requires money management, choosing the proper balance between risk and return. Betting too much, even though each individual bet is in your favor, can be ruinous. When the Nobel Prize winners running the giant hedge fund Long-Term Capital Management made this mistake, its collapse in 1998 almost destabilized the US financial system. On the other hand, playing safe and betting too little means you leave money on the table. The psychological makeup to succeed at investing also has similarities to that for gambling. Great investors are often good at both. Relishing the intellectual challenge and the fun of exploring the markets, I spent the summer of 1964 educating myself about them. I haunted the big Martindale’s bookstore then in Beverly Hills. I read stock market classics like Graham and Dodd’s Security Analysis, Edwards and Magee’s work on technical analysis, and scores of other books and periodicals ranging from fundamental to technical, theoretical to practical, and simple to abstruse. Much of what I read was dross but, like a baleen whale filtering the tiny nutritious krill from huge volumes of seawater, I came away with a foundation of knowledge. Once again, just as with casino games, I was surprised and encouraged by how little was known by so many. And just as in blackjack, my first investment was a loss that contributed to my education. A couple of years earlier, when I knew nothing at all about investing, I heard about a company whose stock was allegedly selling at a bargain price. It was Electric Autolite, and among their products were automobile batteries for Ford Motor Company. The story on the business page of my newspaper said we could expect a great future: technological innovations, big new contracts, and a jump in sales. (The same forecasts for battery makers were being made forty years later.) As I finally had some capital from playing blackjack and from book sales, I decided to let it grow through investing while I focused on family and my academic career. I bought one hundred shares at $40 and watched the stock decline over the next two years to $20 a share, losing half of my $4,000 investment. I had no idea when to sell. I decided to hang on until the stock returned to my original purchase price, so as not to take a loss. This is exactly what gamblers do when they are losing and insist on playing until they get even. It took four years, but I finally got out with my original $4,000. Fifty years later, legions of tech stock investors shared my experience, waiting fifteen years to get even after buying near the top on March 10, 2000. Years later, discussing my Electric Autolite purchase with Vivian as we drove home from lunch, I asked, “What were my mistakes?” She almost read my mind as she said, “First, you bought something you didn’t really understand, so it was no better or worse than throwing a dart into the stock market list. Had you bought a low-load mutual fund [no-load funds weren’t available yet] you would have had the same expected gain but less expected risk.” I thought the story about Electric Autolite meant it was a superior investment. That thinking was wrong. As I would learn, most stock-picking stories, advice, and recommendations are completely worthless. Then Vivian remarked on my second mistake in thinking, my plan for getting out, which was to wait until I was even again. What I had done was focus on a price that was of unique historical significance to me, only me, namely, my purchase price. Behavioral finance theorists, who have in recent decades begun to analyze the psychological errors in thinking that persistently bedevil most investors, call this anchori","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:13:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 12: BRIDGE WITH BUFFETT As my reputation as an investor quietly spread around UC, Irvine, friends and members of the university community asked me to manage money for them. Using the warrant hedging techniques in Beat the Market, I took on several accounts with a minimum investment of $25,000. Among my new clients was Ralph Waldo Gerard, dean of the graduate school at UCI, and his wife, Frosty, so-called because of her crown of white hair. Ralph, a distinguished medical researcher and biologist, was a member of the select National Academy of Sciences. Courtly, curious, and widely informed, he enjoyed discussing big ideas with me, as he had with one of his relatives, the great stock market theorist and philosopher Benjamin Graham. Graham and Dodd’s Security Analysis, first published in 1934, was the landmark book for the fundamental analysis of common stocks, revised and updated several times. Through Graham, Gerard had met Warren Buffett and was an early investor in one of his investment vehicles, Buffett Partnership, Ltd. Warren, who would become Graham’s greatest student and arguably the most successful investor of all time, started his first investment partnership, Buffett Associates, Ltd., in 1956 at the age of twenty-five with $100,100. He told me with a laugh that the $100 was his contribution. After starting ten more partnerships he merged them all into Buffett Partnership, Ltd., early in 1962. During the twelve years from 1956 to 1968, these funds Buffett managed compounded at a rate of 29.5 percent, before he took his fee of one-fourth of the gain in excess of 6 percent. He had no down years, whereas large company stocks and small company stocks each fell in four of those years. After Buffett’s fee, Gerard’s investment was growing at 24 percent a year, surpassing the typical stock market investor’s experience, as measured by small-company stocks, which compounded at 19 percent per year, and large-company stocks, which returned 10 percent. Before taxes, $1 for Buffett’s limited partners grew to $16.29. Each of Warren’s own dollars, growing without the deduction of his fees, became $28.80. So why were the Gerards interested in moving their money from the thirty-eight-year-old Buffett, who had been investing since he was a child and with whom they were netting 24 percent a year, to the thirty-six-year-old Thorp, who had been investing for only a few years and from whom they could expect, on the basis of past performance, to net just 20 percent a year? It was because, after the upward spike in stock prices in 1967, when holders of large-company stocks gained 38 percent on average over the two-year period and small-company stocks were up a manic 150 percent, Warren Buffett said it was too tough to find undervalued companies. Over the next couple of years he would be liquidating his partnership. His investors could cash out or, along with Warren himself, take some or all of their equity as shares in two companies owned by the partnership, one of which was a troubled little textile company called Berkshire Hathaway. Buffett himself now owned $25 million of the $100 million partnership, as a result of his management fees and their growth through reinvestment in the partnership. The Gerards chose to take their distribution entirely in cash and were looking for a new home for it. Ralph liked the analytic approach in Beat the Market and my other writings, and he wanted not only to check me out himself but, as I realized later, to get a reading from the great investor with whom he had done so well. Thus it happened that in the summer of 1968 the Gerards invited Vivian and me to their home for dinner with Susie and Warren Buffett. From their home in the Harbor View Hills section of Newport Beach, the Gerards enjoyed looking at Newport Harbor, the Pacific Ocean, and the spectacular evanescent sunsets behind Catalina Island to the west. After we sat down for dinner, Ralph’s wife, Frosty, asked each person at the table to int","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:14:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 13: GOING INTO PARTNERSHIP Princeton Newport Partners (PNP) was a revolutionary idea when we set it up in 1969. We specialized in the hedging of convertible securities—warrants, options, convertible bonds and preferreds, and other types of derivative securities as they were introduced into the markets. Hedging risk was not new but we took it to an extreme never before tried. To begin with, we designed each of our hedges, which combined the stock and convertible securities of a single company, to minimize the risk of loss whether the stock fell or rose. We invented hedging techniques to further protect our portfolio against changes in interest rates, changes in the level of the overall market, and the catastrophic losses that can occasionally occur from enormous unexpected changes in prices and volatility. We managed this with mathematical formulas, economic models, and computers. This nearly total reliance on quantitative methods was unique, making us the earliest of a new breed of investors who would later be called quants, and who would radically transform Wall Street. I could see from the beginning how our wealth could grow. But when I told friends and colleagues what I was up to, Vivian was almost the only one who got it, despite what I had already done in gambling. Although she wasn’t a scientist or mathematician, she shared two qualities with the best of them: She asked the right questions, and she grasped the essentials. She had spent hours helping me film spinning roulette balls so I could make a machine to predict which number would come up, just as she had dealt thousands of blackjack hands so I could practice counting cards. And she helped me edit my books about gambling and the stock market and negotiate the contracts. My initial plan for Princeton Newport Partners, which for the first five years we called Convertible Hedge Associates, was to find pairs of closely related securities that were priced inconsistently with respect to each other, and use them to construct investments that reduced risk. To form these hedges, we simultaneously bought the relatively underpriced security while offsetting the risk from adverse changes in its price by selling short the comparatively overpriced security. Since the prices of these two securities tended to move in tandem, I expected the combination to reduce risk while capturing extra returns. I identified these situations using the mathematical methods I had worked out for judging the proper price of a warrant, option, or convertible bond versus the common stock of the same company. Betting on a hedge I had researched was like betting on a blackjack hand where I had the advantage. As in blackjack, I could estimate my expected return, estimate my risk, and choose how much of my bankroll to bet. Instead of a $10,000 bankroll I now had $1.4 million, and instead of a $500 maximum bet, the Wall Street casino had no limit. We started betting $50,000 to $100,000 per hedge. To search for opportunities, early every afternoon after the market closed in New York, UC, Irvine students whom I hired went to the offices of two brokerage firms with which I traded. They collected the closing prices for hundreds of warrants, convertible bonds, convertible preferreds, and their associated common stocks. A preferred stock typically pays a regular dividend, whereas a common stock may or may not pay a dividend and, if it does, will generally vary over time. A preferred stock’s dividend is paid first—in preference—before any payments due to the common stock. In the typical case, where the dividend amount is fixed, the preferred is like a bond but more risky because the dividend payments and the claim on assets upon liquidation are only paid after the corresponding bond payments. A so-called convertible preferred is one that can be exchanged for a specified number of shares of the common. So a convertible preferred is like a convertible bond but less secure, as it is paid only if there is enoug","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:15:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 14: FRONT-RUNNING THE QUANTITATIVE REVOLUTION When Black and Scholes published their formula, the same one I was already using, I knew that to maintain PNP’s trading edge I would have to develop my tools for valuing warrants, options, convertible bonds, and other derivative securities rapidly enough to stay ahead of future marching legions of PhDs hungry for academic advancement through publication. Though I had to keep important results secret for the benefit of our investors, I could publicize lesser ideas that I thought would soon be found by others. Before the work of Black and Scholes, I had moved beyond their basic formula, having generalized it to include cases where short-sale proceeds were withheld by the broker (to his benefit, since he got the use of the money) until the short sale was closed. Once they published, I presented these at a meeting of the International Statistical Institute in Vienna, where I was speaking. I also had extended the model to include dividend-paying stocks, since I was trading call options and warrants on many such stocks. Then the CBOE announced it would start trading put options sometime in the following year, 1974. These options, like the call options we were already trading, were called American options, as distinguished from European options. European options can be exercised only during a short settlement period just prior to expiration, whereas American options can be exercised anytime during their life. If the underlying stock pays no dividends, the Black-Scholes formula, which is for the European call option, turns out to coincide with the formula for the American call option, which is the type that trades on the CBOE. A formula for the European put option can be obtained using the formula for the European call option. But the math for American put options differs from that for European put options, and—even now—no general formula has ever been found. I realized that I could use a computer and my undisclosed “integral method” for valuing options to get numerical results to any desired degree of accuracy for this as-yet-unsolved “American put problem.” In a productive hour in the fall of 1973 I outlined the solution, from which my staff programmed a computer to produce precise calculated values. My integral method also had another advantage over the Black-Scholes approach. Whereas the latter was based on one specific model for stock prices, one with limited accuracy, my technique could value options for a wide range of assumed distributions of stock prices. In May 1974 I had dinner with Fischer Black in Chicago, where he had invited me to give a talk at the semiannual CRSP (Center for Research in Security Prices) meeting at the University of Chicago. Then in his thirties, Fischer was trim and tall, with combed-back black hair and “serious” glasses. Focusing intently on whatever finance topic was being discussed, he spoke articulately, logically, and concisely. His notes, compact and ultra-legible, reflected this. He would go on to become one of the most innovative and influential figures in academic and applied finance. Since a computational method for pricing American puts had been easy for me, I brought it to show Fischer and to learn from him how others had solved it. I laid the answer on the table between us but before I could speak Fischer began telling me about his approach to the problem and the difficulties that had so far stopped him. Earlier, I had explored his approach and believed it would work but, as my integral method was so easy, I used it. If Fischer Black didn’t know the answer, no one else did. Owing it to my partners to preserve our competitive advantage, I unobtrusively returned my work to my briefcase. Two other computational methods for finding American put prices were eventually published in scholarly journals in 1977. As with my method for valuing American puts, my associates and I continued to solve problems for valuing so-called derivatives before ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:16:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 15: RISE… On November 1, 1979, ten years after we started Princeton Newport Partners, the annualized return for the S\u0026P 500, including dividends, was 4.6 percent and for small-company stocks 8.5 percent, both with far more volatility than Princeton Newport. We were up 409 percent for the decade, annualizing at 17.7 percent before fees and 14.1 percent after fees. Our initial $1.4 million had grown to $28.6 million. We ended 1979 with a grand dream for the 1980s: to expand our expertise into new investment areas. For me this meant more interesting problems to solve in quantitative finance. For the partnership it could lead to an increase in the amount of capital we could invest at high rates of return. I called our first effort the Indicators Project. The object was to study the financial characteristics of companies, or indicators, to see if they could be used to forecast stock returns. The prototype was Value Line, an investment service that launched a program in 1965 using information such as surprise earnings announcements, price-to-earnings ratios, and momentum to rank stocks into groups from I (best) to V (worst). A stock is said to have positive momentum if its price has recently been trending strongly up, and negative momentum if strongly down. The head of our indicators project was Dr. Jerome Baesel, a talented and articulate young economist whom I met when we were both teaching finance in what is now the Paul Merage School of Business at UC, Irvine. Also crucial to this project and virtually all the others, then and later, was Steven Mizusawa. Steve and I met in 1972 when he and another UCI student asked to do a special mathematics summer project under my direction exploring an aspect of card counting in blackjack. They did an excellent job, and so, when in 1973 I needed someone with computer skills, Steve was available. With degrees in both computer science and physics, Steve has been in charge of our computer operations and much of the associated research. He became a general partner of Princeton Newport Partners and an invaluable friend. The project relied on two vast securities databases and the computing power to process them, both of which had recently become available. Daily historical prices of stocks, the dates and amounts of any cash dividends paid, and other data were marketed by CRSP, the University of Chicago’s Center for Research in Security Prices. The Compustat database provided historical balance sheet and income information. Of the scores of indicators we systematically analyzed, several correlated strongly with past performance. Among them were earnings yield (annual earnings divided by price), dividend yield, book value divided by price, momentum, short interest (the number of shares of a company currently sold short), earnings surprise (an earnings announcement that is significantly and unexpectedly different from the analysts’ consensus), purchases and sales by company officers, directors, and large shareholders, and the ratio of total company sales to the market price of the company. We studied each of these separately, then worked out how to combine them. When the historical patterns persisted as prices unfolded into the future, we created a trading system called MIDAS (multiple indicator diversified asset system) and used it to run a separate long/short hedge fund (long the “good” stocks, short the “bad” ones). The power of MIDAS was that it applied to the entire multitrillion-dollar stock market, with the possibility of investing very large sums. Two professors of finance, Bruce Jacobs and Kenneth Levy, had independently been thinking along the same lines as I learned when they presented their work to the UC–Berkeley program in finance in the fall of 1986. Our system operated successfully until we closed it along with Princeton Newport Partners at the end of 1988. Jacobs and Levy went on to manage several billion dollars using this method. By 1985 our offices in Newport Beach, Califor","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:17:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 16: …AND FALL In the middle of the day on Thursday, December 17, 1987, about fifty armed men and women burst out of the third-floor elevators to raid our office in Princeton, New Jersey. They were from the IRS, the FBI, and the postal authorities. Our employees were searched before they were free to leave the building. They were not allowed to return. The invaders impounded several hundred boxes of books and records, including Rolodexes. They dug through contents of wastebaskets and crawled through the ceiling spaces. It went on into the early-morning hours of the next day. This was part of a campaign by Rudolph Giuliani, US Attorney for the Southern District of New York, to prosecute real and alleged Wall Street criminals. As a prosecutor later told a defense attorney, Giuliani’s real objective in attacking individuals in our Princeton office was to get information to further his case against Michael Milken at Drexel Burnham and Robert Freeman at Goldman Sachs. My partner Jay Regan knew them both well and spoke to them often. Freeman had even been a roommate of Regan’s at Dartmouth. Giuliani believed that Regan could help him bring them down. Regan refused to cooperate. The government used evidence from the raid and testimony from a disgruntled former employee to develop their case. Ironically, when this man was being considered for a job as a trader by the Princeton office, they flew him to Newport Beach to get our opinion. We said emphatically that he was not suitable. However, it was our practice for each office to have the last word in the areas of the business for which it was primarily responsible. The Princeton office hired him. The five top people there were indicted and tried on sixty-four charges of stock manipulation, stock parking, tax fraud, mail fraud, and wire fraud. The defendants, in addition to Jay Regan, were our head trader, head convertible trader, the CFO and his assistant, and a Drexel Burnham convertible trader. Neither I nor any of the forty or so other partners and employees in the Newport Beach office had any knowledge of the alleged acts in the Princeton office. We were never implicated in, or charged with, any wrongdoing in this or any other matter. Our two offices, more than two thousand miles apart, had very different activities, functions, and corporate cultures. The key to the government’s case was a few conversations they discovered on three old trading room audiotapes that had been saved years earlier, then misplaced and forgotten. They were originally created because it was the normal business practice in the Princeton office, as it was elsewhere on Wall Street, to temporarily record all telephone conversations in the trading room. A major purpose for this was to quickly resolve disputes with counterparties over trading orders and executions. With our volume of eighteen billion shares a year, mistakes were inevitable. One such trade, part of a gigantic Japanese warrant hedge executed through a firm I’ll call Enco, was based on what they told us about the terms of the warrant. Our traders said Enco repeatedly assured us that the information they gave us was correct. In fact it wasn’t. Our proof was on those tapes. The resulting mistake in the quantities of securities used for our hedge position cost us $2 million. Ordinarily the tapes ran continuously, keeping the last four days of conversations, writing over the oldest as they recorded the newest. But pending a resolution, our traders saved the tape covering the disputed trade. Later, since Enco refused to admit the error was theirs, our traders prepared for arbitration or litigation by initiating and recording two more conversations in which Enco again told our traders the original information they had given us was correct. This meant two more tapes, including eight more days of conversations, were set aside for evidence. We then showed management at Enco how the facts contradicted what their staff had told us, and asked for compen","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:18:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 17: PERIOD OF ADJUSTMENT Joseph Heller and Kurt Vonnegut were at a party given by a billionaire when Vonnegut asked Heller how it felt to know that their host might have made more money in one day than Heller’s Catch-22 since it was written. Heller said he had something the rich man could never have. When a puzzled Vonnegut asked what that could be, Heller answered, “The knowledge that I’ve got enough.” When Princeton Newport Partners closed, Vivian and I had money enough for the rest of our lives. Though the ending of PNP was traumatic for us all, and the future wealth destroyed was in the billions, it freed us to do more of what we enjoyed most: spend time with each other and the family and friends we loved, travel, and pursue our interests. Taking to heart the lyrics of the song “Enjoy Yourself (It’s Later than You Think),” Vivian and I would make the most of the one thing we could never have enough of—time together. Success on Wall Street was getting the most money. Success for us was having the best life. It was by chance during this time that I discovered the greatest of all financial frauds. On the afternoon of Thursday, December 11, 2008, I got the news I had been expecting for more than seventeen years. Calling from New York, my son, Jeff, told me Bernie Madoff confessed to having defrauded investors of $50 billion in the greatest Ponzi scheme in history. “It’s what you predicted in…1991!” he said. On a balmy Monday morning in the spring of ’91, I arrived at the New York office of a well-known international consulting company. The investment committee hired me as an independent adviser to review their hedge fund investments. I spent a few days examining performance histories, business structures, and backgrounds of managers, as well as making onsite visits. One manager was so paranoid when I interviewed him at his office that he wouldn’t tell me what kind of personal computers they used. When I went to the restroom he escorted me for fear that I might acquire some valuable crumb of information en route. I approved the portfolio, with one exception. The story from Bernard Madoff Investment didn’t add up. My client had been getting regular monthly profits ranging from 1 percent to more than 2 percent for two years. Moreover, they knew other Madoff investors who had been winning every month for more than a decade. Madoff claimed to use a split-strike price strategy: He would buy a stock, sell a call option at a higher price, and use the proceeds to pay for a put option at a lower price. I explained that, according to financial theory, the long-run impact on portfolio returns from many properly priced options with zero net proceeds should also be zero. So we expect, over time, that the client’s portfolio return should be roughly the same as the return on equities. The returns Madoff reported were too large to be believed. Moreover, in months when stocks are down, the strategy should produce a loss—but Madoff wasn’t reporting any losses. After checking the client’s account statements I found that losing months for the strategy were magically converted to winners by short sales of S\u0026P Index futures. In the same way, months that should have produced very large wins were “smoothed out.” Suspecting fraud, I asked my client to arrange for me to visit the Madoff operation on the seventeenth floor of the famous Lipstick Building on Third Avenue in Manhattan. Bernie was in Europe that week, and as we now know, likely raising more money. His brother, Peter, head of compliance and of computer operations, said that I would not be allowed through the front door. I asked my client who it was that did the accounting and annual audits for the Madoff fund. I was told it was handled by a one-man shop run by someone who had been a friend and neighbor of Bernie’s since the 1960s. Now on high alert for fraud, I asked when the client received confirmations of trades. The answer was that they came by mail in batches every week or two, w","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:19:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 18: SWINDLES AND HAZARDS When shifting my focus from beating gambling games to analyzing the stock market, I naïvely thought that I was leaving a world where cheating at cards was problematic and entering an arena where regulation and the rule of law gave investors a fair playing field. Instead, I learned that bigger stakes attracted bigger thieves. Bernie Madoff’s Ponzi scheme was only the largest of the many exposed in 2008 and 2009, as a sharply falling market cut the supply of new money flowing into the swindles. These ranged in size from an $8 billion bank scam to swindles involving hundreds of millions of dollars each (including several hedge funds), and multimillion-dollar real estate, mortgage, and annuity scams. Swindles likely follow a simple mathematical law describing how their number increases as their economic size decreases. The rise of the Internet and electronic connectivity created new opportunities for fraud. On Friday, August 25, 2000, my niece Dana, who was getting interested in stocks, called me after the market closed. “Do you know anything about a stock called Emulex?” she asked. “No, why?” “Well, I own some, and shortly after it opened today it crashed from 113 [dollars per share] to 45 and then they suspended trading!” “What’s the news about it?” I asked. “I don’t know.” “Well, my advice is to do nothing. I’d say there’s a good chance it’s another Internet fraud and that the company is just as sound as it was yesterday.” We soon learned what had happened. A twenty-three-year-old college student had sent a report to the electronic news service Internet Wire for which he formerly worked, purporting to be an official news release from Emulex (EMLX). The report claimed that the company’s president was resigning, good positive earnings for the last two years were being corrected to show large losses, and the SEC was to investigate. This fake information spread quickly and the stock was down 56 percent by the time NASDAQ halted trading. The hoaxer had earlier lost $100,000 selling Emulex short and managed to regain this plus a $250,000 profit before he was apprehended the following week. In the process, at the worst point, he had knocked the market capitalization of EMLX from $4.1 billion to $1.6 billion, a loss of $2.5 billion. Though the stock recovered most of its loss later in the day, it still closed down 7.31 at 105.75, a fall of 6.6 percent or $270 million in market value. The damage was much greater for those who sold during the drop. Eleven days after the hoax was exposed, and after the hoaxer was caught, the stock closed at 100.13, down 11.4 percent, never having fully recovered in the interim. According to the theory of efficient markets (the EMH), the market sets prices so that they accurately reflect all available information. How does the collapse of 60 percent in fifteen minutes in response to false information represent the rational incorporation of information into the price? I also ask believers in the EMH to explain why the stock failed to recover in the eleven days after the hoax was exposed. The news for EMLX was good. So…? Supporters of the efficient market view have slowly accepted minor deviations from the theory. They might acknowledge the market’s response to the EMLX hoax as one of these; but as the press pointed out, the Internet is rife with such attempts, notably in chat rooms, and EMLX was just one in a series of large-scale spectacular attempts to fool the public in order to gain a profit. Shortly afterward, on September 21, 2000, a front-page headline in The New York Times read “SEC Says Teenager Had After-School Hobby: Online Stock Fraud.” The fifteen-year-old New Jersey high school student collected $273,000 in eleven trades. He would first buy a block of stock in a thinly traded company, then flood Internet chat rooms with messages that, say, a $2 stock would be trading at $20 “very soon.” The text here was about as valuable as the message in a fortune cookie. Dr.","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:20:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 19: BUYING LOW, SELLING HIGH It’s the spring of 2000 and another warm sunny day in Newport Beach. From my home six hundred feet high on the hill I can see thirty miles across the Pacific Ocean to Wrigley’s twenty-six-mile-long Catalina Island, stretched across the horizon like a huge ship. To the left, sixty miles away, the top of the equally large San Clemente Island is just visible above the horizon. The ocean starts two and a half miles from where I sit, separated by a ribbon of white surf from wide sandy beaches. An early trickle of boats streams into the sea from Newport Harbor, one of the world’s largest small-boat moorings, with more than eight thousand sail and power vessels, and some of the most expensive luxury homes in the world. Whenever I leave on vacation I wonder if I have made a mistake. As I finish breakfast the sun is rising above the hills behind me, illuminating the financial towers to the west in the enormous business and retail complex of Fashion Island. By the time the skyscrapers are in full sunlight I drive three miles to my office in one of them. The statistical arbitrage operation that Steve and I restarted in 1992 has been running successfully now for eight years. Our computers traded more than a million shares in the first hour and we are ahead $400,000. Currently managing $340 million, we have purchased $540 million worth of stocks long and sold an equal amount short. Our computer simulations and experience show that this portfolio is close to market-neutral, which means that the fluctuations in the value of the portfolio have little correlation to the overall average price changes in the market. Our level of market neutrality, measured by what financial theorists call beta, has averaged 0.06. When beta is zero for a portfolio, its price movements have no correlation with those of the market, and it is called market-neutral. Portfolios with positive beta tend to move up and down with the market, more so for larger beta. The beta of the market itself is chosen to be 1.0. Negative beta portfolios tend to fluctuate oppositely to the market. Our risk-adjusted excess return, the amount by which our annualized return has exceeded that from investments of comparable risk and called alpha by finance theorists, has averaged about 20 percent per year. This means that our past annual rate of return (before fees) of 26 percent can be thought of as the sum of three parts: 5 percent from Treasury bills with no risk, about 1 percent due to our slight correlation to the market, plus the remaining 20 percent, the amount by which our return exceeds investments with comparable risk. Using our model, our computers calculate daily a “fair” price for each of about one thousand of the largest, most heavily traded companies on the New York and American stock exchanges. Market professionals describe stocks with large trading volume as “liquid”; they have the advantage of being easier to trade without moving the price up or down as much in the process. The latest prices from the exchanges flow into our computers and are compared at once with the current fair value according to our model. When the actual price differs enough from the fair price, we buy the underpriced and short the overpriced. To control risk, we limit the dollar value we hold in the stock of any one company. Our caution and our risk-control measures seem to work. Our daily, weekly, and monthly results are “positively skewed,” meaning that we have substantially more large winning days, weeks, and months than losing ones, and the gainers tend to be bigger than the losers. Scanning the computer screen, I see the day’s interesting positions, including the biggest gainers and the biggest losers. I can see quickly if any winners or losers seem unusually large. Everything looks normal. I walk down the hall to Steve Mizusawa’s office, where he is watching his Bloomberg terminal, checking for news that might have a big impact on one of the stocks we trade. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:21:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 20: BACKING THE TRUCK UP TO THE BANKS One day in 1990 my entrepreneurial son, Jeff, called to advise me to open passbook savings accounts at mutual savings and loan associations. Why would I want to tie up money at 5 percent when I was earning 20 percent? Jeff answered, “How would you like a little piece of a few billion dollars of value no one owns?” I said, “Keep talking.” And he explained how it worked. There were at that time a couple of thousand mutual savings and loan associations around the country. They began as associations formed by depositors who pooled their money, allowing the members to borrow as needed, meanwhile paying interest on their loans to those who had money in the pool. The depositors owned the association “mutually,” which meant that the business value that built up during operation was also “owned” by the depositors. As time passed, depositors came and went, but upon leaving they left their share of the business behind. No mechanism existed for extracting this value. The giant slow-motion collapse of the US savings and loan industry, starting in the late 1970s and continuing through the 1980s, created a need for capital to buoy weakened institutions, capital to exploit the new opportunities to fill the void left by failed institutions, and capital to compete with the new larger consolidated savings and loans that were appearing. The mutuals could raise capital only by attracting more depositors, a slow and uncertain process, but their rivals, the “stock” savings and loans, were corporations owned by shareholders. They could get more capital from the marketplace, as they needed it, by selling shares. Facing such competition, some of the more entrepreneurial managers of mutuals decided to “convert” to stock companies, and this began the process of extracting billions of dollars that no one could previously claim. Here’s how it is done. Imagine a hypothetical mutual savings and loan, which we’ll call Magic Wand S\u0026L, or MW, with $10 million in liquidation or book value, and net income of $1 million per year. If MW were a stock bank with one million shares outstanding, each share would have a book value of $10 and earn $1 per share, which is 10 percent of book value. Suppose that if there were such a thing as MW stock, it would, as is typical, trade at one times book value, or $10 per share. Management decides to “convert” MW to a stock savings and loan and issue for the first time one million shares of stock at $10 per share, for proceeds of $10 million. After this initial public offering, or IPO, MW has $10 million in new cash plus the $10 million in equity previously owned by the depositors, for a new total of $20 million in equity. Each share now has a book value of $10 cash plus $10 in contributed equity, for a total of $20. What will the new shares sell for in the marketplace? The contributed equity ought to be worth $10 based on the current market price of comparable stock S\u0026Ls and the $10 in cash ought to be worth another $10, so once the public understands this, we expect the new stock to trade at about $20. Buy a $20 stock for $10. Who loses? No one, but those depositors who do not purchase enough stock on the offering to capture their share of the pre-IPO equity they “owned” give up some of the gains to the others, who then are able to get more. Fortunately, the IPOs are generally structured so depositors have priority over other classes in applying for stock. Usually only one class has still higher priority. Who? You guessed it! The insiders: officers, directors, and employee stock option and benefit plans. This allows the insiders to capture some of the depositors’ value, which provides a powerful motivation for management to convert. Suppose we had the foresight to become a depositor in Magic Wand S\u0026L before the deadline for eligibility to participate in the IPO. Sometime after the eligibility deadline, the bank announces its intention to convert, chooses an investment banker to manag","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:22:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 21: ONE LAST PUFF After twelve successful years, with the stock market vastly overpriced in his opinion, Warren Buffett outlined the windup of Buffett Partnership, Ltd., in October 1969. Partners would get a payout consisting of at least 56 percent cash, possibly some scraps of stock in assorted companies, and an estimated 30 to 35 percent, for those who chose not to have them sold for cash, in two companies—Diversified Retailing and a New England textile company called Berkshire Hathaway. He added the discouraging, “For the first time in my investment lifetime, I now believe there is little choice for the average investor between professionally managed money in stocks and passive investment in bonds.” As I reread Buffett’s letter today I see no clue now, nor did I then, that Berkshire Hathaway would become the successor to Warren’s partnership. Ralph Gerard, a longtime investor with Buffett and the man who introduced us, never did, either. Of the $100 million distributed to partners, about $25 million was Buffett’s. He eventually ended up with nearly half of Berkshire’s stock. Berkshire was what Buffett and his mentor Benjamin Graham called a cigar butt—you can pick it up cheaply and get one last puff. As Forbes said in 1990, in its characteristic shorthand, “[Buffett] bought Berkshire Hathaway textile mills 1965 ($12/share), dissolved partnership 1969 after thirty-fold growth, decided to use Berkshire Hathaway as prime investment vehicle. Textile business floundered (ceased operations in 1985) but investment business boomed.” Focused on Princeton Newport, I lost track of Warren after 1969. Then in 1983, I heard about the remarkable growth of a company called Berkshire Hathaway. Not knowing it was to become Warren’s investment vehicle, I had stopped paying attention to it back in 1969. The stock price then was $42 a share, if you could find anyone to trade with. It was now publicly trading at over $900. I knew at once what this meant. The “cigar butt” had become a humidor of Havanas. Despite its having increased by a multiple of more than 23 in fourteen years, I made my first purchase at $982.50 a share and continued to accumulate stock. By contrast, in 2004 I was talking to a bank president in San Francisco when he mentioned that his mother had been a limited partner in Buffett Partnership, Ltd., and received some Berkshire stock as part of her distribution when the partnership closed. “That’s wonderful,” I said. “At today’s prices [then $80,000 a share or so] she must be very rich.” “Sadly,” he said, “she sold at $79 for a several hundred percent profit.” If asked for advice, I recommended the stock to family, friends, and associates with the understanding that it was a long-term holding with a possibly volatile future. I didn’t suggest it to those who couldn’t understand the reasoning behind the purchase and who would be scared by a big drop in price. The response was sometimes frustrating. In 1985 our divorced house cleaner, Carolyn, got $6,000 as a settlement from an automobile accident. She wanted to invest it to send her children, aged five and six, through college. Week after week she pleaded with me to advise her, but as she knew absolutely nothing about securities or investing, I declined. Urged on by her fortune-teller, who told her that I would double or even triple her money, she persisted. In a weak moment I relented, provided that if she bought the stock I recommended, she would never sell before checking with me. I arranged low commissions for her, as a favor to me by a broker friend, and she bought two shares of Berkshire Hathaway (BRK) for $2,500 each. She later left housecleaning for office work and we lost touch with her. Meanwhile BRK rose to about $5,000 a share just before the October 1987 crash. I learned later from the broker that Carolyn had sold near the post-crash bottom at $2,600 a share. Sixteen years later, in the first quarter of 2003 at the time her children might have been finishing c","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:23:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 22: HEDGING YOUR BETS Hedging your bets supposedly protects against catastrophic losses. But when the 2008 recession hit, many investors in hedge funds lost heavily. The worldwide collapse in credit and in asset prices was worse than any downturn since the Great Depression. Housing prices tumbled, the S\u0026P 500 fell 57 percent from its October 9, 2007, high, and US private wealth declined from $64 trillion to $51 trillion. Small investors like my niece and my house cleaner, watching the equity index funds in their IRAs plunge, asked me if they should dump their stocks. Many investors had to sell, including the wealthiest university endowment fund in the country, Harvard’s, valued at $36.9 billion in early 2008 but now desperate for cash. Hedge funds, which were supposed to protect investors against such declines, dropped an average 18 percent. Even so, the most highly compensated hedge fund manager, James Simons of Renaissance Technologies, made $2.5 billion. The top twenty-five managers collected $11.6 billion, down from $22.5 billion in 2007. It was now twenty years after the end of Princeton Newport Partners, and hedge funds had proliferated until there were ten thousand worldwide, with total equity estimated at $2 trillion. Their worldwide pool of wealthy investors is a mix of private individuals, trusts, corporations, pension and profit-sharing plans, foundations, and endowments. The 2008 crash dealt a massive blow to the hedge fund industry. Four hundred billion had been swept away. This triggered worldwide requests for withdrawals by investors who were angered by losses that weren’t supposed to happen. They were shocked when many funds refused to return their remaining money. As the economy slowly recovered and the market bounced back to new highs, investors forgot what happened to them in 2008–09. By 2015 hedge fund assets reached a new high of $2.9 trillion. Management fees ranging between 1.5 and 2 percent delivered $50 billion to the operators. Their percentage of the profits added perhaps another $50 billion. This $50 billion in performance fees supposedly represents 20 percent of the profits after all other charges. But investors as a group actually pay a larger percentage. To see why, suppose there are two funds that start the year at $1 billion each. One fund nets $300 million and the other loses $100 million. At 20 percent of profits, the first fund collects a $60 million performance fee and the second collects none. Pooling the results from the two funds, we see that investors pay $60 million on a profit of $200 million, a rate of 30 percent of the combined gains and losses. With Princeton Newport, growth from new capital came slowly and was earned by performance. Over forty years this battle for funding changed dramatically. So-called alternative investments became the hottest new frontier for what to do with your money. Beginning in the late 1990s, you could, in effect, just put up a sign saying HEDGE FUND OPENING HERE, and a line of investors would quickly extend around the block. A modest-sized $100 million hedge fund earning a gross return of 10 percent per year ($10 million) may pay the manager or general partner a management fee of 1 percent of $100 million—$1 million. In addition, the manager gets 20 percent of the remaining $9 million in profit, or another $1.8 million, as a performance fee, for a total of $2.8 million per year. Some of this, perhaps $1 million, pays expenses, leaving a net of $1.8 million a year in pretax income. The investors, or limited partners, get the remaining $7.2 million for a 7.2 percent annual return. The general partners in a similar billion-dollar hedge fund—and there are scores of them—might share ten times as much, or $28 million a year. Even a little $10 million hedge fund would, with proportionate fees, expenses, and returns, provide a single general partner with $280,000 a year. It’s clear from this that you can get very rich running a hedge fund. With these re","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:24:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 23: HOW RICH IS RICH? While chatting long-distance with a financial entrepreneur in London, I asked, “How much wealth would you need today to retire and live comfortably for the rest of your life?” “I know my exact number,” he replied. “It’s $20 million.” I said, “According to my calculations, each year you can withdraw the equivalent of 2 percent of that amount, or $400,000 in today’s dollars, with only a small probability of ever using up your fortune.” In his early forties, married and with three small children, he said this sounded good to him. But everyone has a different number. The famous American fiction writer John D. MacDonald characterized levels of wealth in 1970 in his Travis McGee series. As I remember, his economist co-hero Meyer said $100,000 was “adequate” and $250,000 “comfortable,” whereas $1,000,000 was “substantial.” To have $5,000,000 was “impressive.” Since inflation has cut the real buying power of the dollar, MacDonald’s corresponding year 2015 numbers would be six times as large, with $600,000 as adequate, $1,500,000 as comfortable, $6,000,000 as substantial, and $30,000,000 as impressive. Dinesh D’Souza gave the following classification, which I have adjusted for inflation. Table 3: Classifications of Wealth Income Wealth Superrich $15 million+ | $150 million+ Rich $1.5–15 million | $15 million–150 million Upper-middle $112,000–1.5 million | $750,000–15 million Middle $50,000–112,000 | $82,000–750,000 Lower-middle $22,000–50,000 | $15,000–82,000 Poor $0–22,000 | $0–15,000 US household wealth was estimated at $83 trillion at the end of 2014, mainly stocks, bonds, real estate, and personal property. What if we divided it up so everyone had the same amount? With 320 million people participating, each would have about $270,000. However, such estimates of national wealth and of the distribution of wealth and income are problematic. It depends on what’s counted and how it’s counted, and much data is not available. It’s also true that this increasing national wealth has become more unevenly distributed. The median wealth of a US household fell 36 percent, after inflation, from 2003 to 2013, declining from $88,000 to $56,000. In contrast, a household at the 97.5 percentile was 12 percent better off, with its net worth moving from $1.19 million up to $1.36 million. A million dollars still sounds like real money, even though it doesn’t buy nearly what it once did. In fact, it would take $20 million today to match the buying power of $1 million a century ago. How many in the United States have a net worth of $1 million? No one knows exactly, because comprehensive information on personal wealth is difficult to collect. Much of it is not available, not reported, or deliberately hidden to avoid taxes, theft, or criminal prosecution, or simply for personal privacy. Most of the available information applies to household units, of which there are about 125 million. Some households consist of only one person and most of the rest have a single economically dominant individual, so counting wealthy households probably gives a good estimate for the number of wealthy individuals. The number of households worth at least $1 million was thought to be about ten million in 2015. With so many millionaire households, the goal of becoming one of them looks within reach. To see what might be done, imagine you’re an eighteen-year-old blue-collar worker with no savings and no prospects. What if, somehow, you could save $6 a day and buy shares in the Vanguard S\u0026P 500 Index Fund at the end of each month? If that investment grows in a tax-deferred retirement plan at the long-term average for large stocks of about 10 percent, then after forty-seven years you can retire at age sixty-five with $2.4 million. But where do you find an extra $6 a day? The pack-and-a-half-a-day smoker who kicks his drug habit saves $6 each day. If the construction worker who drinks two $5 six-packs of beer or Coke each day switches to tap water he can s","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:25:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 5: Estimating a Household’s Net Worth This quick assessment gives you an idea of where you stand. Later you will want to make a more accurate balance sheet, which I do about once a year. The difference in balance sheet net worth from one year to the next shows the change in your total wealth after income, expenses, gains and losses. This series of annual balance sheets shows how your wealth is changing over the years. In the asset section, for each item list the amount of cash you feel sure it would sell for in a reasonably short time. That car you bought new a year ago for $45,000 might have a replacement cost of $39,000 now, but you might be able to sell it for only $35,000. Put down $35,000. Recent sales of houses comparable with yours might range from $925,000 to $950,000, but after all sales and closing costs, you might net only $875,000. Put down $875,000. What you owe on the mortgage will be deducted in the liabilities section. Unlike liquid listed securities, the current market prices for property such as cars, houses, art, and jewelry are not continuously displayed, but the analogy to securities prices is useful for understanding the impact of commissions on profits and losses. Just as each security has a current price for which you can buy it, the asking price, and a somewhat higher cost to you after paying commissions, we can imagine an all-inclusive “asking” price that we would have to pay for a piece of property identical to what we now own. Call this the replacement cost. And just as each security has a current price someone will pay for it, called the bid price, and a somewhat lower net proceeds to us after commissions, we also can imagine the highest net proceeds we might receive after selling costs for a piece of property. This is the liquidation value that we are listing in the asset section. This spread between replacement value and liquidation value may be high for real property—often as much as 10 to 20 percent. For instance, I buy a $100,000 painting and pay $7,000 more in sales taxes, for a total of $107,000. The next day I change my mind and sell it for the same price of $100,000, paying $10,000 in commissions, for net proceeds of $90,000. The spread was $90,000 to $107,000, a difference of $17,000 or 17 percent of the “base” price of $100,000. This is what is lost in a round of buying and selling. It’s that way with houses, cars, art, and jewelry. In contrast, the cost to trade listed securities is typically only a small fraction of a percent—which, along with their liquidity, makes them more appealing stores of wealth. Wealth, which I use synonymously with the accountants’ term net worth, shows how rich you are now, whereas income measures how much money your wealth, labor, and ingenuity are currently generating. A major part of aggregate wealth increase, especially at the higher wealth levels, comes from investments such as stocks, bonds, real estate, and collectibles. Wealth, not income, is the measure of how rich someone is now. However, examples like the movie star who leaps to fame and starts making $20 million per year show that income may lead to future large increases in wealth. It’s that increase in net worth from year to year that takes you up the ladder of wealth. To measure your increase in wealth from one year to the next, compare the yearly balance sheets. Divide the difference by the beginning wealth to get your percentage change for the year. This gives you an idea of how fast you are compounding. If you also construct an income statement for the period, the net income after expenses should match your change in net worth. Balance sheets are snapshots that tell you where you are at a particular time. The income statement tells you what happens between two balance sheets. To appreciate the income statement without bothering to look anything up, jot down a list of all the sources that added to, or subtracted from, your wealth during the last twelve months. Do this “rough and dirty”;","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:26:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 24: COMPOUND GROWTH: THE EIGHTH WONDER OF THE WORLD For those who want to climb the ladder of wealth, it is helpful to appreciate the unusual arithmetic by which money grows. Compound interest, described in a phrase of disputed origin, is “the eighth wonder of the world.” Wonder or trick, it has built great fortunes, and you can use it to get richer. In 1944, the fifty-one-year-old IRS estate auditor Anne Scheiber left the organization that rewarded her for twenty-three years of distinguished service by never promoting her. Then she invested her savings of $5,000 in the stock market. Living frugally and studying companies, she continually reinvested her dividends. Her portfolio continued to grow until she died in 1995 at age 101. When her lawyer, Ben Clark, tried to meet with officials of Yeshiva University to tell them about a bequest she had left to the school, they had never heard of Anne Scheiber and wondered how to avoid wasting their time. But when the meeting was finally held, they learned that Ms. Scheiber was leaving them $22 million for the benefit of women students. Were Anne Scheiber’s choices unusually lucky? How would an average investor have done? Taking the period from the start of 1944 until the end of 1997, allowing a couple of years for the settlement of the estate and the delivery of securities to Yeshiva, $5,000 invested in a large stock index grew to a mere $3.76 million; but the same amount invested in small stocks grew, on average, to $12.31 million. Starting with a little more than Anne, investing $8,936 instead of $5,000, the average small stock investor would have achieved her $22 million result. Compound interest, or more accurately compound growth, is the process Anne Scheiber used, accumulating wealth by reinvesting her gains. An easy way to think about compound growth, and also about the ladder of wealth, is in terms of doubling and redoubling. Consider two investors, Sam Scared and Charlie Compounder. Suppose Sam Scared starts with $1; each time it doubles, he puts his $1 profit in a sock instead of reinvesting it. After ten doublings, Sam has a profit in the sock of $1 × 10 plus his original $1 for a total of $11. Charlie also starts with $1 and makes the same investments but lets his profit ride. His $1 becomes $2, $4, $8, et cetera, until after ten doublings he has $1,024. Sam’s wealth grows as $1, $2, $3…$11. This is called simple growth, arithmetic growth, or growth by addition. Charlie’s increases as $1, $2, $4…$1,024. This is known variously as compound, exponential, geometric, or multiplicative growth. Over a sufficiently long time, compound growth at a small rate will vastly exceed any rate of arithmetic growth, no matter how large! For instance, if Sam Scared made 100 percent a year and put it in a sock and Charlie Compounder made only 1 percent a year but reinvested it, Charlie’s wealth would eventually exceed Sam’s by as much as you please. This is true even if Sam started with far more than Charlie, even $1 billion to Charlie’s $1. Realizing this truth, Robert Malthus (1766–1834), believing that population grew geometrically and resources grew arithmetically, forecast increasingly great misery. Politicians, dimly aware of the awesome power of compound growth, have in many jurisdictions passed laws against perpetuities to prevent the enormous concentrations of wealth that might arise from investments compounding without limit. On the other hand, some states and counties welcome perpetual trusts, being more interested in deriving revenue from them now. The population of the world increased from 2.5 billion in 1930 to 7.3 billion in 2015, a growth rate of about 1 percent a year. It’s expected to reach 9.7 billion by 2050. Everyone knows that this can’t keep up; the carrying capacity of the earth—the amount of humanity the earth can support as limited by the available solar energy for food, and by other scarce resources—has been estimated as up to one hundred billion people. But","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:27:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 25: BEAT MOST INVESTORS BY INDEXING The easiest way to outperform most investors and grow your wealth is based on a simple concept that all investors should understand, both as a tool for investing and as an example of logical thinking about the markets. Consider a mutual fund that buys every stock trading on a major US stock exchange, investing in each company a percentage equal to that company’s percentage of the total value of all the US stocks. Thus the fund behaves like the entire market, with the same daily percentage price changes and dividend payouts. This means if the oil giant Exxon has a market value, computed as share price times number of shares outstanding, of $400 billion and the total market value of all stocks is $10 trillion, then the index fund puts 4 percent of its net worth in Exxon, and so on for all the other stocks. A mutual fund like this that replicates the composition and investment results of a specified pool of securities is called an index fund, and investors who buy such funds are known as indexers. Call any investment that mimics the whole market of listed US securities “passive” and notice that since each of these passive investments acts just like the market, so does a pool of all of them. If these passive investors together own, say, 15 percent of every stock, then “everybody else” owns 85 percent and, taken as a group, their investments also are like one giant index fund. But “everybody else” means all the active investors, each of whom has his own recipe for how much to own of each stock and none of whom has indexed. As Nobel Prize winner Bill Sharpe says, it follows from the laws of arithmetic that the combined holdings of all the active investors also replicates the index. Although this idea is well known, and I’m not sure where it first appeared, I first heard it from him and he has given the clearest exposition I’ve seen. I’ll call it Sharpe’s Principle. I met Bill Sharpe in 1968 or 1969, when we were both young professors at UCI. Highly regarded, he had already completed the work for which he was awarded the Nobel Prize in 1990. Unfortunately he was in UCI’s School of Social Sciences and I didn’t really get to know him before he was recruited by Stanford just two years after his arrival. Had he still been at UCI after Princeton Newport Partners was well under way, might we have collaborated? He contributed a key simplification for understanding options, the binary model, and I might have been able to convince him that markets have significant inefficiencies—in other words, opportunities for abnormal risk-adjusted returns. Discussing this in 1975 when I invited him to lecture at UCI, Bill argued that my rewards from PNP didn’t demonstrate market inefficiency, because you could argue that I and my associates were simply getting paid according to our worth. Had we turned our talents to other areas of economic endeavor we could expect the same. Before costs, each passive investor gets the same return as the index. This is also true for the active investors as a group but not for each one individually. Holding a larger percentage than the index in some stocks and less in others, they may do better or worse than the index in various periods. Although the results (before costs) for the entire group of active investors matches the return on the index, their individual returns are statistically distributed around it with most fairly close and some quite different. They have more risk without the expectation of more return. Reducing risk through diversification is one reason to buy an index, but an even more important one is reducing the costs that investors bear. Index funds trade infrequently, with stocks turning over just a few percent a year as the “keepers” of the index occasionally add and remove stocks, or because cash flows into or out of the fund. On the other hand, active investors as a group have been trading more than 100 percent of their portfolio per year. This imposes a su","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:28:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 6: With an Investment Making 8%, Paying Tax Every Year at 35%, at 20%, and Paying 20% at the End Value of Investment Investment Ends at Year Pay 35% Tax Every Year Pay 20% Tax Every Year Pay 20% Tax at End 0 1,000 1,000 1,000 1 1,052 1,064 1,064 10 1,660 1,860 1,927 20 2,756 3,458 3,929 30 4,576 6,431 8,250 If the index beats the pool of active investors by 2 percent each year, does that mean it also beats most equity mutual funds? Widely publicized, year-end annual reports show the S\u0026P Index of 500 stocks beating a majority of mutual funds most years but not always. Why? For one thing, we’re comparing apples and oranges: The S\u0026P 500 Index isn’t the whole market—if our universe is the total stock market index then it’s an active investor, although one with low costs—since it doesn’t include most small companies, so the assets of a mutual fund that are not part of the S\u0026P 500 are not subject to Sharpe’s Principle as applied to that index. The S\u0026P 500 stocks are selected by the Standard \u0026 Poor’s Corporation, with occasional deletions and additions. Although these five hundred large companies account for roughly 75 percent of the market value of all publicly traded stocks, it omits some very large companies, notably, before 2010, Berkshire Hathaway, one of the ten largest US companies by market value. In fact, the compound annual return on small companies for the eighty-two years from 1926 through 2007 was 12.45 percent, compared with 10.36 percent for large companies. Yet the extra boost for mutual funds from having had some of their assets in smaller stocks still hasn’t offset their extra costs. Another aspect of the apples-and-oranges comparison is the impact of cash balances. Since fund investors continually add or withdraw money, funds are partly invested in fluctuating cash balances. When the market rises strongly, the interest on this cash doesn’t keep up and the fund return lags the return on the equity portion of its holdings. Conversely, when the market is down sharply, the losses on the fund’s equity position are reduced to the extent it is in cash and by the interest it gets on that cash. The impact of this cash drag is generally small. Also, non-index mutual funds are only part of the total pool of active investors. Conceivably, their managers could be relatively skilled, in which case the mutual fund group would outperform the rest of the active investors. In this case, though the active investors lagged overall, the mutual fund group might excel compared with the others. However, academic studies of the historical returns of mutual funds show little evidence of such managerial skill on the part of mutual funds. Third, it is not the number of active investors that must lag the index, according to Sharpe’s Principle. Instead, it is the return on the total pool of actively managed assets invested in the index that must underperform. Morningstar, which tracks mutual fund performance, does periodic studies comparing fund performance with indexes. The 2009 results are typical. After adjusting for risk, size, and investment category, only 37 percent beat their benchmark over the previous three years, with similar results for five and ten years. The benefits from indexing are shown in table 7. Here I have used historical returns on large stocks, like those in the S\u0026P 500, with my assumed costs. More details are given in appendix B. After costs and inflation, tax-exempt passive investors gained 6.7 percent annually compared with 4.7 percent for the actives, or one-third more. After taxes it is 2.0 percent for the actives and 4.8 percent for the indexers, more than double. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:29:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 7: Comparison of Passive Versus Active Investing Index Passives Actives Before costs 10.1% 10.1% 10.1% After costs — 9.7% 7.7% After inflation 7.1% 6.7% 4.7% Tax-exempt after inflation — 6.7% 4.7% After taxes — 4.8% 2.0% If you index, select a fund with annual expenses less than 0.2 percent. Reject funds that add management fees, sales loads, or other charges. The one charge you can ignore is a penalty for selling before a short holding period, such as thirty days, which funds introduced to prevent costly large-scale rapid in-and-out trading by certain investors. Each year, typically at the end of October, US equity mutual funds assign the year-to-date taxable gains or losses to their current investors. If you were to make an investment shortly before this in a year when the fund had a lot of gain, you could experience the inequity of paying taxes on an amount far larger than your real economic gain. On the other hand, in a year when the fund allocated large losses, a purchase shortly before the time to receive the losses could let investors reduce their tax bill without having had a corresponding economic loss. Tax-exempt investors such as IRAs, 401(k)s, employee benefit plans, and foundations ought to consider swapping their active investments in equities into a broad no-load index fund, unless they have strong reasons to believe their current investments give them a significant edge. In my experience, superior stock-picking ability is rare, which means almost everyone should make the switch. Taxable investors need to review their holdings on a case-by-case basis. For instance, in 2015, with a cost basis of about $1,000 a share, a market price of $225,000 a share, and a combined federal and state tax rate of, say, 30 percent, I would net about $157,800 per share after a sale of my Berkshire Hathaway Class A stock. An index fund purchased with this smaller amount would have to do about 43 percent better than Berkshire in the future for me to catch up. This seems extremely unlikely. Like me with Berkshire, investors who don’t trade, and use no advisers, will avoid the usual expenses paid by active investors. In fact, their costs may be even less than those of indexers. If such a buy-and-hold investor were, for instance, to choose stocks at random, purchasing an amount of each proportional to its market capitalization, we could show, by reasoning like that used to prove Sharpe’s Principle, that the expected return is the same as for the index from which the stocks were chosen minus the presumably small costs of acquiring the stocks. The main disadvantage to buy-and-hold versus indexing is the added risk. In gambling terms, the return to buy-and-hold is like that from buying the index then adding random gains or losses by repeatedly flipping a coin. However, with a holding of twenty or so stocks spread out over different industries, this extra risk tends to be small. The threat to a buy-and-hold program is the investor himself. Following his stocks and listening to stories and advice about them can lead to trading actively, producing on average the inferior results about which I’ve warned. Buying an index avoids this trap. For another way to look at index investing, suppose the same percentage of each US stock were put into a low-cost index fund and all the rest went into a giant pool actively managed by the world’s best managers. Then a clerk managing the index fund with a computer to do the bookkeeping would beat the team of the best managers on earth, by the amount of their extra commissions and fees. In contests promoted by journalists, random portfolios of stocks selected using chance devices such as darts, dice, or (figurative) chimpanzees hold their own against the experts. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:30:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 26: CAN YOU BEAT THE MARKET? SHOULD YOU TRY? When I first became interested in blackjack, everyone said there was no way to beat it. Winning systems, often involving complicated ways of varying the amounts wagered, were proven mathematically to be impossible for many of the classical gambling games. Besides, if someone could beat the casinos, the rules would be changed to stop them. When I became interested in the stock market, I heard the same claims about investing. Academics had developed a series of arguments known as the efficient market hypothesis (EMH). Using financial market data, they showed that tomorrow’s prices looked like random fluctuations around today’s prices, therefore they were not predictable. Besides, if a price change were predictable, somebody would immediately trade on this until it was no longer so. This notion gave rise to an apocryphal story that all finance students have heard. Eugene Fama, father of EMH, was strolling across the University of Chicago campus with a graduate student. Looking down, the student exclaimed, “Look, there is a $100 bill on the ground.” Without a glance down or a break in stride, Fama replied, “No, there isn’t. If there were, someone would have picked it up already.” The cards dealt at blackjack also seem to appear at random but not if you “track the shuffle,” which is a way to beat the game by watching the order in which the discarded cards are stacked, then mathematically analyzing the particular shuffling technique being used, leading to a partial prediction of the new ordering of the cards for the next deal. The likelihood of any card being dealt next at blackjack also is not random if you count the cards. What appears random for one state of knowledge may not be if we are given more information. Future prices are not predictable and no one can beat the market, but only when market prices “truly” fluctuate randomly. Supporters of the efficient market hypothesis, really a collection of related hypotheses, generally believe that securities markets in advanced developed countries respond quickly and almost completely to new information. True believers originally held that most investors were rational and well informed over the decades. However, they have reluctantly yielded to the overwhelming evidence to the contrary, but they still say the collective impact of investors generally keeps current market prices close to the best possible estimate of the value, averaged over all future scenarios. Since the 1960s, academics in economics and finance have defended the various versions of the efficient market hypothesis as they churned out tens of thousands of articles, thousands of PhDs, and hundreds of books. The classic view of the correct price of a common stock is that it is derived from the value of all the future earnings. These earnings are uncertain and subject to unknowable factors. Could anyone have known beforehand how to allow for the impact of 9/11 on the future earnings, hence on the then current market price, of firms headquartered in the Twin Towers of the World Trade Center? These future payoffs are discounted to a present value reflecting their various probabilities and risks. If the market does a good job of using today’s public information to set current prices, then the only investors who have an edge are those with material private information. The high-profile prosecution of investors in the 1980s for illegal trading on inside information makes the point. The EMH is a theory that can never be logically proved. All you can argue is that it is a good or not-so-good description of reality. However, it can be disproven merely by providing examples where it fails, and the more numerous and substantive the examples, the more poorly it describes reality. So far I’ve shown how markets were beaten in the past with examples from gambling, from the trading and results of Princeton Newport Partners, Ridgeline, and other hedge funds, and from the story of Warre","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:31:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 27: ASSET ALLOCATION AND WEALTH MANAGEMENT Private wealth in the industrially advanced countries is spread among major asset classes such as equities (common stocks), bonds, real estate, collectibles, commodities, and miscellaneous personal property. If investors choose index funds for each asset class in which they wish to invest, their combined portfolio risk and return will depend on how they allocate among asset classes. This also is true for investors who don’t index. Table 8 gives a rough overview of the asset categories. Investment assets held by mutual funds, hedge funds, foundations, and employee benefit funds are not included, since their underlying assets have already been counted. Derivative securities, which include warrants, options, convertible bonds, and many later complex inventions, derive their value—as we have seen—from that of an “underlying” security such as the common stock of a company. Instead of listing them separately, they’re understood to be included as part of their underlying asset class. How are your assets divided among the categories in table 8? The big three for most investors are equities, interest rate securities, and real estate. Each accounts for about a quarter of the total net worth of US households, though the proportions fluctuate, particularly when an asset class experiences a boom or a bust. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:32:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 8: Major Asset Classes and Subdivisions EQUITIES Common Stock Preferred Stock Warrants and Convertibles Private Equity INTEREST RATE SECURITIES Bonds US Government Corporate Municipal Convertibles Cash US Treasury Bills Savings Accounts Certificates of Deposit Mortgage-Backed Securities REAL ESTATE Residential Commercial COMMODITIES Agricultural Industrial Currencies Precious metals COLLECTIBLES (Art, gems, coins, autos, etc.) MISCELLANEOUS (MARKETABLE) PERSONAL PROPERTY Motor vehicles, planes, boats, jewelry, etc. Investors who chase returns, buying asset classes on the way up and selling on the way down, have had poor historical results. The tech bubble that ended in 2000, the inflation in real estate prices that peaked in 2006, and the sharp drop in equity prices in 2008–09 were especially costly for them. On the other hand, the buy-low/sell-high investors, whom you might think of as “contrarian” or “value” investors, have tended to outperform by switching some funds between asset classes. The tables in appendix B show that stocks and commercial real estate have provided the best long-run results for investors. Interest rate investments have been roughly break-even after taxes and inflation, and only modestly positive for nontaxable investors. However, though equities have performed best in the long run, they have had extended periods when they have been in drawdown, meaning that they were below their previous all-time high. Real estate fell sharply in the financial crisis of 2008–09. Assuming that the risks and returns for asset classes in the twenty-first century will be similar to what they were in the twentieth, long-term passive investors are likely to do best in common stocks and income-producing commercial real estate, though the data is sketchier for the latter. Diversifying between the two may reduce risk and increase overall return. Many investors do not want the level of risk involved in common stocks or real estate, where the high overall returns are interrupted by savage reductions in wealth. A retired couple I knew had investments worth $6 million, which they planned to use as their means of support for the rest of their lives. Spending 4 percent of this per year, with the unspent part invested in “something safe that keeps up with inflation,” this couple could enjoy the inflation-adjusted equivalent of about $240,000 pretax per year for the twenty-five remaining years at least one of them might live. They chose to put half in tax-free municipal bonds and the rest in equities. They feared a replay of the Great Depression. I thought this plan suited them. As neither husband nor wife was interested in learning about finance and investing, they should remain passive investors. Even my suggestion, in the early 1990s, to put about $500,000 in Berkshire Hathaway, then trading at $12,000 per share, was too much for them to think about. It would have been worth $9 million in 2016 when the husband, who had outlived his wife, finally passed away. Having half their money in relatively safe and stable municipal bonds would likely preserve enough wealth to ride through adversity. The years passed. Although their market value varied inversely with interest rates, the municipal bonds paid an average of 4 percent or so, tax-free, or about $120,000 annually. Overall, US equity investments increased four or five times on average (before taxes, investment adviser fees, and other costs), and Berkshire Hathaway advanced from $12,000 to almost $150,000, fell to $75,000 during the crisis, then rose above $200,000 per share in 2016. When the crisis of 2008 struck, equities lost half their value before rebounding. As tax receipts shriveled, the massive deficits of the US government were echoed at state and local levels. The safety of municipal bonds no longer seemed so assured. However, although they would have done better in equities, they still had enough money and, feeling safe, didn’t worry as they would have done watching t","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:33:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 28: GIVING BACK In 2003, Vivian and I offered to endow a chair in mathematics at the University of California, Irvine. We were guided by what we had learned from the charitable giving we had done over several decades. One principle was to make the gift transformative, with an impact well beyond what you’d expect from the monetary amount. We also wanted to fund projects that wouldn’t happen without our support. These conditions were met. A new chairman had transformed the Math Department in the 1990s, quelling the strife, marginalizing the bad actors, and bringing in talented new faculty. Though there were endowed chairs on the campus, math had none. By creating one, we could attract a star and raise the department to a still-higher level. We stated our objectives as (1) to support the research of an individual mathematician of exceptional talent; and (2) using an unusual investment and distribution policy, to cause the principal to increase through compound growth so that the chair eventually becomes one of the most richly endowed in the world, thereby attracting extraordinary mathematical talent to UCI. To meet our first objective, the endowment is to be used only to supplement the research activities of the chair holder. These funds are in addition to, not instead of, the standard faculty salary and support from the university. If the university is not willing to hire someone, neither are we. This arrangement is to remain unchanged even if, as we expect, the distribution from the endowment one day grows far beyond the salary paid by the campus to the chair holder. Funds used for general departmental, campus, or university budgets, or for any purpose not directly in support of the research activities of the chair holder, are limited to 5 percent of the annual money drawn. We specified a distribution rate of 2 percent annually, which means 0.1 percent covers administration and 1.9 percent goes to the chair. We knew that limiting the annual draw from the endowment to 2 percent was crucial to our long-term compounding objective. We donated appreciated Class A shares of Berkshire Hathaway, which eliminated a possible long-term capital gain for us if, instead, we kept the shares and someday sold them. Stock is to be sold only as needed to fund the chair. However, just one Berkshire A share would create far more cash (over $200,000 in 2016) than the annual payment from the endowment. Therefore, when money is needed, we asked that an A share first be exchanged for fifteen hundred B shares, the specified conversion ratio. Worth about $140 each in mid-2016, the B shares could then be sold in precise amounts to create funds when required. The point is to keep the endowment fully invested in stock until cash is needed. When we are no longer alive, remaining shares will be exchanged for a broad, no-load, US stock index fund with a very low expense ratio, such as the Vanguard S\u0026P 500 or the Vanguard Total US Stock Index. What kind of growth in the value of the endowment might we expect? Over the last two hundred years, a broad US stock index has grown about 7 percent faster than inflation. No one knows whether the future will be equally good, but even if the increase exceeded inflation by only 5 percent, the net annual growth in purchasing power would be 3 percent. Doubling on average every twenty-four years, after a century the endowment and its annual payouts would have grown over nineteen times in today’s dollars. In two hundred years this rate of growth would increase it to 370 times what is was worth when the chair was funded in 2003. If the United States continues to prosper, if the university continues to exist, and if our investment and distribution policies continue to be implemented, the power of compounding may well lead to an endowment fund for our chair in mathematics that, valued in today’s dollars, exceeds that of the current endowment for any chair that now exists in the world. For those who wonder how likely this ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:34:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 29: FINANCIAL CRISES: LESSONS NOT LEARNED On October 9, 2007, the S\u0026P 500 reached an all-time closing high of 1,565. Led by home prices, which began to fall from their inflated 2006 peak, it drifted downward, then accelerated to a low of 676 on March 9, 2009, a decline of 57 percent. A million dollars’ worth of the index at the high fell to $430,000 at the low. Single-family homes declined 30 percent. One bright spot was bonds. Borrowing declined and interest rates fell, pushing US government and higher-quality corporates up strongly. Despite an offset from this rise in bond prices, the net worth of US households, which peaked in June 2007 at $65.9 trillion, fell to $48.5 trillion during the first quarter of 2009, a loss of 26 percent. It was the worst blow to national wealth since the Great Depression eighty years earlier. The lessons learned then by our grandparents were forgotten after two generations. The stock market collapse that triggered that calamity was the climax to a speculative bubble. As stock prices rose in the 1920s, “investors” (mostly gamblers) came to believe that they would continue ever upward. A leading economist of the day encouragingly declared that stocks had permanently reached a new high plateau. But the key to the disaster that followed was easy money and leverage. Investors could buy stocks on as little as 10 percent margin, meaning that they could put up only 10 percent of the purchase price and borrow the other 90 percent. It sounds eerily familiar because it is. The 2008 collapse in housing prices had the same cause: unlimited unsound loans to create highly leveraged borrowers. Here’s how it worked in the stock market in 1929. If shares trading for $100 each were purchased for $10 down and a $90 loan per share and subsequently went to, say, $110, the happy investor then had $20 per share of equity, equal to $110 minus the $90 he originally borrowed from his broker. He doubled his money on a mere 10 percent rise in the stock. He can now borrow 90 percent against this $10-per-share profit to buy an additional $90 worth of stock, bringing the total value of his stock to twice what he originally bought. If the investor repeats this each time his stock goes up another 10 percent, both his equity and his loan will double again at each step. After five such increases of 10 percent over the previous price, the stock will trade at $161 per share, a 61 percent gain. Meanwhile our pyramiding investor will have doubled his equity five times, to thirty-two times the starting amount. Ten thousand dollars becomes $320,000. After ten steps up of 10 percent, during which the investor’s stake undergoes ten doublings, the stock will be at $259 and from the original purchase of $10,000 worth of stocks using only $1,000, the investor now has $10,240,000 of the same security. His equity is 10 percent of this. He’s a millionaire. Such is the hypnotically enticing power of leverage. But what happens if the stock price then drops 10 percent? Our giddy investor loses his entire equity and his broker issues a margin call: Pay off the loan—which is now more than $9 million—or be sold out. As stock prices rose in 1929, investors leveraged themselves in this way to buy more, driving prices higher. The positive feedback loop led to an average total return on large-company stocks of 193 percent from the end of 1925 to the end of August 1929. A purchase of $100 on no borrowing grew to $293, and our 10 percent down investor who pyramided might have doubled his money more than ten times, gaining more than a thousand times his original investment. However, as prices eased in September and October 1929, the equity of the most highly leveraged investors vanished. When they were unable to meet margin calls, their brokers sold their stock. These sales drove prices down, wiping out investors who hadn’t been quite as leveraged, triggering a new round of margin calls and sales, driving prices down further. As the equity bubble bu","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:35:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Chapter 30: THOUGHTS To end this story of my odyssey through science, mathematics, gambling, hedge funds, finance, and investing, I would like to share some of what I learned along the way. Education has made all the difference for me. Mathematics taught me to reason logically and to understand numbers, tables, charts, and calculations as second nature. Physics, chemistry, astronomy, and biology revealed wonders of the world, and showed me how to build models and theories to describe and to predict. This paid off for me in both gambling and investing. Education builds software for your brain. When you’re born, think of yourself as a computer with a basic operating system and not much else. Learning is like adding programs, big and small, to this computer, from drawing a face to riding a bicycle to reading to mastering calculus. You will use these programs to make your way in the world. Much of what I’ve learned came from schools and teachers. Even more valuable, I learned at an early age to teach myself. This paid off later on because there weren’t any courses in how to beat blackjack, build a computer for roulette, or launch a market-neutral hedge fund. I found that most people don’t understand the probability calculations needed to figure out gambling games or to solve problems in everyday life. We didn’t need that skill to survive as a species in the forests and jungles. When a lion roared, you instinctively climbed the nearest tree and thought later about what to do next. Today we often have the time to think, calculate, and plan ahead, and here’s where math can help us make decisions. For instance, are seatbelts and air bags “worth it”? Suppose we upgrade a hundred million vehicles at a cost of $300 each, a total of $30 billion, and have five thousand fewer traffic deaths per year. If these vehicles with their added safety features are around for ten years, that’s fifty thousand lives saved at a cost of $30 billion, or $600,000 per life. Though many in the auto industry disagreed, we spent the money and saved the lives. What about the pack-a-day smoker? Forty years of this will make his life on average seven years shorter. Each cigarette not only brings death twelve minutes closer, but adds health problems to spoil one’s remaining years. Then there are the costs to the rest of us, namely, higher medical costs in the final years, more sick days during the working years, and secondhand smoke damage. But these are averages. Some smokers do not die of smoking-related diseases, whereas others die at an early age. It’s like gambling at roulette. On average you lose 5 cents when you bet $1. But this is an average. Some gamblers are wiped out quickly and others may hold their own for quite a while. One of the major public policy issues today is the trade-off between the costs and the benefits of certain procedures. Some choices are stark. Is it better to spend $500,000 to save the life of someone with super-drug-resistant tuberculosis or to use the same amount to save fifty lives by delivering fifty thousand doses of flu vaccine at $10 each to schoolchildren? Statistical thinking can help us with choices like these. I believe that simple probability and statistics should be taught in grades kindergarten through twelve and that analyzing games of chance such as coin matching, dice, and roulette is one way we can learn enough to think through such issues. Understanding why casinos usually win might help us avoid gambling and teach us to limit our losses to their entertainment value. Gambling now is largely a socially corrosive tax on ignorance, draining money from those who cannot afford the losses. Most of what I’ve learned from gambling also is true for investing. People mostly don’t understand risk, reward, and uncertainty. Their investment results could be much better if they did. For instance, years ago my homeowners’ association kept their cash reserves in thirty-day US Treasury bills for absolute safety. However, they spent ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:36:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Epilogue Freud said that once we have the basic necessities of food, clothing, shelter, and health, then what we seek is wealth, power, honor, and the love of men and women. For financial titans who aggressively continue to seek tens of millions, hundreds of millions, and sometimes billions, you can ask, “Is the winner really the one who dies with the most toys?” How much is enough? When will you be done? Often the answer is “Never.” To preserve the quality of my life and to spend more of it in the company of people I value and in the exploration of ideas I enjoy, I chose not to follow up on a number of business ventures, although I believed that they were nearly certain to become extremely profitable. Once I worked out the major concepts in a subject and proved them in action, I liked new mental challenges, moving on from gambling games to the investment world, with warrants, options, convertible bonds and other derivatives, then statistical arbitrage. Starting as a university professor, I expected to spend my life teaching, doing research, and talking to smart like-minded people; but from childhood I was intrigued by the power of abstract thinking to understand and direct the natural world. When I later saw how physics could predict roulette outcomes through the fog of chance, and mathematics could tip the odds in blackjack, I was drawn into a lifetime of adventure. It was my good fortune to share most of this journey with a remarkable companion. From childhood, my wife, Vivian, loved books and was a voracious reader. One year we kept a journal about the books we read. In twelve months she cruised through more than 150 books at seven hundred words or more per minute. I know that because one day when we were both reading, I couldn’t believe how fast she was turning the pages, so, without her knowing, I timed her for an hour. She passed on her love of books and her extraordinary facility with the English language to our children and grandchildren. She mastered bridge, studied art and art history, learned to prepare quality healthy meals, completed a master’s degree in library science, inspired her family to focus on personal fitness and health, and supported causes and charities. She was also one of the rare people known as super-recognizers. She could casually recognize people she’d met decades before, even though they had been transformed—in my opinion often beyond recognition—by age, style, carriage, shape, and size. When most of us remember the past, the memories fade over time and the “facts” may shift closer to the heart’s desire. When it had to do with people, Vivian’s memory was both extraordinarily accurate and unchanging over time. After she died from cancer in 2011, we celebrated her life with a memorial service. When I think of our lives together, I remember what her brother said then: “Nobody can take away the dance you have danced.” Life is like reading a novel or running a marathon. It’s not so much about reaching a goal but rather about the journey itself and the experiences along the way. As Benjamin Franklin famously said, “Time is the stuff life is made of,” and how you spend it makes all the difference. Best of all is the time I have spent with the people in my life that I care about—my wife, my family, my friends, and my associates. Whatever you do, enjoy your life and the people who share it with you, and leave something good of yourself for the generations to follow. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:37:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Appendix A ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:38:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"THE IMPACT OF INFLATION ON THE DOLLAR This table indicates how the buying power of a dollar has changed.* To see what my $11,000 win at blackjack in 1961 with Manny Kimmel and Eddie Hand was equal to in 2013, we multiply $11,000 by the 2013 index and divide by the 1961 index: $11,000 × 233.0 ÷ 29.9 = $85,719. To convert dollars in year A to dollars in year B, multiply by the index for B and divide by the index for A. Overall, the index has increased by about 3.6 percent a year, but there are some unusual variations. The index falls (deflation!) after the 1929 crash and stays at a reduced level for the next decade. Then it increases rapidly during World War II and the first postwar years. Although inflation has been moderate in the United States and in most first-world countries most of the time, it is occasionally catastrophic. During the German hyperinflation of 1919–23, the currency declined to one hundred billionth of its starting value (divide by 100,000,000,000). Debtors were freed and lenders were ruined. This level of inflation would reduce the $18 trillion or so US national debt of 2015 to the equivalent of $180. In 2009, the African nation of Zimbabwe experienced a hyperinflation comparable to the German one, with Z-one-trillion bills commonplace. From its peak in 1929, the S\u0026P 500 total return index (dividends reinvested) had, at its low in 1932, fallen by 89 percent. However, these were deflationary times, so the nation had the cold comfort of knowing that after adjusting for inflation, the index had lost only 85 percent. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:39:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 9: Consumer Price Index Year Index Year Index Year Index 1913 9.9 1934 13.4 1955 26.8 1914 10.0 1935 13.7 1956 27.2 1915 10.1 1936 13.9 1957 28.1 1916 10.9 1937 14.4 1958 28.9 1917 12.8 1938 14.1 1959 29.2 1918 15.0 1939 13.9 1960 29.6 1919 17.3 1940 14.0 1961 29.9 1920 20.0 1941 14.7 1962 30.3 1921 17.9 1942 16.3 1963 30.6 1922 16.8 1943 17.3 1964 31.0 1923 17.1 1944 17.6 1965 31.5 1924 17.1 1945 18.0 1966 32.5 1925 17.5 1946 19.5 1967 33.4 1926 17.7 1947 22.3 1968 34.8 1927 17.4 1948 24.0 1969 36.7 1928 17.2 1949 23.8 1970 38.8 1929 17.2 1950 24.1 1971 40.5 1930 16.7 1951 26.0 1972 41.8 1931 15.2 1952 26.6 1973 44.4 1932 13.6 1953 26.8 1974 49.3 1933 12.9 1954 26.9 1975 53.8 1976 56.9 1989 124.0 2002 179.9 1977 60.6 1990 130.7 2003 184.0 1978 65.2 1991 136.2 2004 188.9 1979 72.6 1992 140.3 2005 195.3 1980 82.4 1993 144.5 2006 201.6 1981 90.9 1994 148.2 2007 207.3 1982 96.5 1995 152.4 2008 215.3 1983 99.6 1996 156.9 2009 214.5 1984 103.9 1997 160.5 2010 218.1 1985 107.6 1998 163.0 2011 224.9 1986 109.6 1999 166.6 2012 229.6 1987 113.6 2000 172.2 2013 233.0 1988 118.3 2001 177.1 US Department of Labor Bureau of Labor Statistics Washington, DC 20212 Consumer Price Index All Urban Consumers—(CPI-U) US City Average All Items 1982–84=100 * For an insightful discussion of why the inflation index from the 1970s may be much too low as a result of a series of government revisions in the method of calculation, and the consequences to investors and consumers, see “Fooling with Inflation” by Bill Gross (June 2008) at www.pimco.com . For updated Consumer Price Index numbers and for month-by-month values, go to www.bls.gov/cpi or do the usual Google search. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:40:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Appendix B ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:41:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"HISTORICAL RETURNS ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:42:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 10: Historical Returns on Asset Classes, 1926–2013 Series Compound Annual Return* Average Annual Return** Standard Deviation Real (after inflation) Compound Annual Return* Sharpe Ratio† Large Company Stocks 10.1% 12.1% 20.2% 6.9% 0.43 Small Company Stocks 12.3% 16.9% 32.3% 9.1% 0.41 Long-Term Corporate Bonds 6.0% 6.3% 8.4% 2.9% 0.33 Long-Term Government Bonds 5.5% 5.9% 9.8% 2.4% 0.24 Intermediate-Term Government Bonds 5.3% 5.4% 5.7% 2.3% 0.33 US Treasury Bills 3.5% 3.5% 3.1% 0.5% ——— Inflation 3.0% 3.0% 4.1% ——— ——— * Geometric Mean ** Arithmetic Mean † Arithmetic From: Ibbotson, Stocks, Bonds, Bills and Inflation, Yearbook, Morningstar, 2014. Siegal’s Stocks for the Long Run gives US returns from 1801. Dimson et al. give returns for sixteen countries and an analysis. The return series depends on the time period and on the specific index chosen. I’ve used Ibbotson as my standard because detailed annually updated statistics have been readily available. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:43:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 11: Historical Returns (%) to Investors, 1926–2013 * Geometric Mean From: Ibbotson, Stocks, Bonds, Bills and Inflation, Yearbook, Morningstar, 2014. Siegal’s Stocks for the Long Run gives US returns from 1801. Dimson et al. give returns for sixteen countries and an analysis. The return series depends on the time period and on the specific index chosen. I have again used Ibbotson as my standard. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:44:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 12: Schedule of Assumed Costs Which Reduce Historical Returns (%) Stocks Bonds Bills Passive Active Passive Active Passive Active Management Costs 0.2 1.2 0.2 0.7 0.2 0.7 Trading Costs 0.2 1.2 0.1 0.3 0.1 0.1 Estimated Tax Rate on Remainder 20.0 35.0 35.0 35.0 35.0 35.0 ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:45:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 13: Annual Returns (%), 1972–2013 Compound Annual Return* Average Annual Return** Standard Deviation Equity REITs 11.9 13.5 18.4 Large Company Stocks 10.5 12.1 18.0 Small Company Stocks 13.7 16.1 23.2 Long-Term Corporate Bonds 8.4 8.9 10.3 Long-Term Government Bonds 8.2 8.9 12.4 Intermediate-Term Government Bonds 7.5 7.7 6.6 US Treasury Bills 5.2 5.2 3.4 Inflation 4.2 4.3 3.1 * Geometric Mean ** Arithmetic Mean Comparative historical returns from investing in income-generating real estate are indicated in table 13, which lists total returns from publicly traded Real Estate Investment Trusts for the period 1972–2013. From: Ibbotson, Stocks, Bonds, Bills and Inflation, Yearbook, Morningstar, 2014. Siegal’s Stocks for the Long Run gives US returns from 1801. Dimson et al. give returns for sixteen countries and an analysis. The return series depends on the time period and on the specific index chosen. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:46:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Appendix C ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:47:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"THE RULE OF 72 AND MORE The rule of 72 gives quick approximate answers to compound interest and compound growth problems. The rule tells us how many periods it takes for wealth to double with a specified rate of return, and is exact for a rate of 7.85 percent. For smaller rates, doubling is a little quicker than what the rule calculates; for greater rates, it takes a little longer. The table compares the rule in column 2 with the exact value in column 3. The “exact rule” column shows the number that should replace 72 to calculate each rate of return. For an 8 percent return, the number, rounded to two decimal places, is 72.05, which shows how close the rule of 72 is. Notice that the number in column 4 for the exact rule should equal the column 1 return per period multiplied by the corresponding values in column 3 (actual number of periods to double), but that the column 4 figures don’t quite agree with this. That’s because the numbers in columns 3 and 4 are rounded off from the exact figures, correct to two decimal places. The mental calculator may notice that the exact rule changes by about one-third for each 1 percent change in the return per period; so an easy approximation to the exact rule is 72 + (R−8%)/3. For 1 percent this gives 69.67 compared with the exact 69.66, and for 20 percent we get 76.00 compared with the exact 76.04. The formula fits well for the rest of the table, too. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:48:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table for the Rule of 72 Number of Periods to Double Return Per Period By Rule of 72 Actual Exact Rule 1% 72 69.66 69.66 2% 36 35.00 70.01 3% 24 23.45 70.35 4% 18 17.67 70.69 5% 14.4 14.21 71.03 6% 12 11.90 71.37 7% 10.29 10.24 71.71 8% 9 9.01 72.05 9% 8 8.04 72.39 10% 7.2 7.27 72.73 12% 6 6.12 73.40 15% 4.8 4.96 74.39 20% 3.6 3.80 76.04 24% 3 3.22 77.33 30% 2.4 2.64 79.26 36% 2.0 2.25 81.15 The idea behind the rule works for other wealth multiples. For instance, to get a rule for multiplying by 10, divide all the numbers in the table by 0.30103 (which is log10 2). Thus for 8 percent we get approximately 240, so we have a “rule of 240” for multiples of 10. We conclude that a return of 8 percent multiplies wealth by 10 in about 240 ÷ 8 = 30 years. When Berkshire Hathaway offered to buy Shaw Industries for about $2 billion in cash, one manager mentioned that their earnings were up ten times from sixteen years before. By the rule of 240, we quickly find an approximate growth rate of 240 ÷ 16 = 15%. The actual figure is 15.48 percent. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:49:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Appendix D ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:50:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"PERFORMANCE OF PRINCETON NEWPORT PARTNERS, LP ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:51:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 14: Annual Return in Percent Period Beginning and Ending Princeton Newport Partners, LP (1) Princeton Newport Partners, LP (2) S\u0026P 500 Index (3) 3 Month US T-Bill Total Return 11/01/69—12/31/69 +4.0 +3.2 -4.7 +3.0 01/01/70—12/31/70 +16.3 +13.0 +4.0 +6.2 01/01/71—12/31/71 +33.3 +26.7 +14.3 +4.4 01/01/72—12/31/72 +15.1 +12.1 +19.0 +4.6 01/01/73—12/31/73 +8.1 +6.5 -14.7 +7.5 01/01/74—12/31/74 +11.3 +9.0 -26.5 +7.9 01/01/75—10/31/75* +13.1 +10.5 +34.3 +5.1 11/01/75—10/31/76 +20.2 +16.1 +20.1 +5.2 11/01/76—10/31/77 +18.1 +14.1 -6.2 +5.5 11/01/77—10/31/78 +15.5 +12.4 +6.4 +7.4 11/01/78—10/31/79 +19.1 +15.3 +15.3 +10.9 11/01/79—10/31/80 +26.7 +21.4 +32.1 +12.0 11/01/80—10/31/81 +28.3 +22.6 +0.5 +16.0 11/01/81—10/31/82 +27.3 +21.8 +16.2 +12.1 11/01/82—10/31/83 +13.1 +10.5 +27.9 +9.1 11/01/83—10/31/84 +14.5 +11.6 +6.5 +10.4 11/01/84—10/31/85 +14.3 +11.4 +19.6 +8.0 11/01/85—10/31/86 +29.5 +24.5 +33.1 +6.3 11/01/86—12/31/87** +33.3 +26.7 +5.1 +7.1 01/01/88—12/31/88 +4.0 +3.2 +16.8 +7.4 Total Percentage Increase1 2,734% +1,382% 545% 345% Annual Compound Rate of Return1 19.1% 15.1% 10.2% 8.1% * Fiscal year changed to November 1 start date from January 1 start date. ** Fiscal year changed back to January 1 start date. 1 These figures are for the period from inception through 12/31/88. The period 01/01/89 through 05/15/89 is omitted because: (a) the partnership was liquidating and distributing its capital in a series of payments, (b) it was no longer engaged in its traditional business and the return on capital was complex to calculate, (c) available figures are estimates. The partnership was originally called Convertible Hedge Associates and changed its name to Princeton Newport Partners as of 11/01/75. (1) Before allocation to general partners, including managing general partners (2) Net to limited partners (3) Including dividends ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:52:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 15: Princeton Newport Performance Comparison ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:53:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Appendix E ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:54:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"OUR STATISTICAL ARBITRAGE RESULTS FOR A FORTUNE 100 COMPANY The table XYZ Performance Summary gives basic statistics for just over ten years. These results are without the use of leverage and before fees. The actual returns were better for the investor because gains from using leverage exceeded the fees we charged. The graph XYZ Performance Comparison shows the cumulative wealth relatives for XYZ, the S\u0026P 500, and T-bills + 2 percent. From the end of 1994 until about August 1, 2000, we see one of the great bull markets of all time. The S\u0026P 500 exploded at an average rate of 26 percent per year, multiplying wealth by 3.7 during those 5.6 years. The graph indicates a distinct increase in variability from August 1, 1998, through the middle of February 2002. Some contributors may have been the LTCM disaster, which began in August 1998; the dot-com collapse in March 2000; and the destruction of the Twin Towers of the World Trade Center on September 11, 2001. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:55:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 16: XYZ Statistical Arbitrage Results Start DateEnd DateMonths Traded 08/12/199209/13/2002122 XYZ S\u0026P 500 Annualized Rate of Return 18.2% 7.77% Annualized Standard Deviation (Risk) 6.68% 15.07% Return/Risk 2.73 0.52 One Dollar Becomes 5.48 2.14 ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:56:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"Table 17: XYZ Performance Comparison The author at age five. Lomita News (this newspaper has since folded) The author with lab equipment. This photo was taken in Lomita, a town in Southern California where he lived and attended Narbonne High School. The wearable computer that beat roulette. Completed in June 1961 by Claude Shannon and Edward Thorp and used successfully in Vegas. It is now in the MIT Museum. Random House Hedging warrants, 1966. From Beat the Market. A simple mechanical device for counting cards and calculating the advantage, built in 1964. Don Cravens, Life magazine Counting cards at the Tropicana Hotel, 1964. George Kew, Life magazine Working with my PhD thesis students Dorothy Daybell and David Arterburn, New Mexico State University, 1964. Don Cravens, Life magazine Chairing a session at a mathematics meeting, 1964. A standard European single zero roulette. Our wearable computer is on display in the background. Gambler’s Book Club Press, Las Vegas, Nevada Playing blackjack at Lake Tahoe, Nevada, 1981, with Stanford Wong (left) and Peter Griffin (right). Photo by Alfred Eisenstaedt/The LIFE Picture Collection/Getty Images Claude Shannon. Cover design by Richard Adelson. Cover photograph by Tom Campbell/Alpha The cover of Beat the Dealer. Vivian and Edward Thorp at home, 2004. To Vivian and to our children and their families: Raun, Brian, and Ava; Karen, Rich, Claire, Christopher, and Edward; Jeff, Lisa, Kylie, and Thomas. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:57:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"ACKNOWLEDGMENTS “All writing is rewriting” is a claim I came to appreciate as I worked and reworked the manuscript. I received innumerable helpful comments from those who read some or all of the manuscript at various stages of its evolution. Thank you, Catherine Baldwin, Richard Goul, Judy McCoy, Steve Mizusawa, Ellen Neal, Tom Rollinger, Raymond Sinetar, Jeff Thorp, Karen Thorp, Raun Thorp, Vivian Thorp, and Brian Tichenor. Ellen Neal turned my handwritten squiggles into typescript and cheerfully endured endless revisions. Professional editor and writer Richard Cohen and Random House editor Will Murphy, assisted by Mika Kasuga, gave extensive advice ranging from voice and content to detailed line-edits. David Halpern of The Robbins Agency helped me from start to finish. Some chapters draw upon articles I wrote for the financial magazine Wilmott. By giving me that forum, Paul Wilmott, the founder of the magazine, along with the magazine’s editor, Dan Tudball, have contributed to this book. I checked facts, depending on extensive files of correspondence, news clippings, and financial records. It is likely errors remain and for these I apologize. When I have withheld or changed names it was to preserve privacy or confidences, or to avoid a negative impact on the reputation of an individual or an entity. From childhood to old age, I owe any success I have had to the wonderful people who have been a part of my life: my family, friends, mentors, teachers, and the partners and associates who have worked beside me, especially my late wife, Vivian, whose love and support sustained me for almost sixty years. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:58:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"NOTES ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:59:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 1 a single word Curiously, my son had the same experience. He didn’t say anything until he was about the same age. His sister, about a year and a half older, was his interpreter. They would go around as a little pair and when he indicated with body language and facial expressions what he wanted, she would get it done. complete sentences Henriette Anne Klauser, in Writing on Both Sides of the Brain, Harper, San Francisco, 1997, pp. 36–38, tells a similar story of a first grader who wouldn’t write no matter how she prompted him until, suddenly after seven months, he released a torrent of fluency. Great Flu Pandemic More people died from the Great Flu Pandemic of 1918–19 than from any other plague in history, and more than from World War I itself. the US version In the US and the UK, a million is one followed by six zeros. US usage adds three zeros for each step up, so a billion has nine zeros, a trillion twelve zeros, and so on. Practice in the UK adds six zeros at each stage so a billion has twelve zeros, etc. one standard deviation Standard deviation indicates the size of a typical fluctuation around an average value. to the news See Nassim Taleb’s readable and insightful book Fooled by Randomness. quick mental estimate By the rule of 72, discussed later, a 24 percent annual growth rate doubles money in about 72/24=3 years. After nine years we have three doublings, to two, then four, and finally eight times the starting value. But it actually takes about 3.22 years because the rule of 72 underestimates the doubling time more and more as rates increase beyond 8 percent. of the Alamo The story of this epic battle and the subsequent ordeals of those held captive by the Japanese is told by Eric Morris in Corregidor: The American Alamo of World War II, Stein and Day, New York, 1981, reprinted paperback, Cooper Square Press, New York, 2000. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:60:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 2 surgical facilities The horrors of life in such camps is eloquently rendered in Three Came Home: A Woman’s Ordeal in a Japanese Prison Camp, by Agnes Keith, 1949, paperback 1985, Eland Books, London and Hippocrene Books, New York. was still running Edmund Scientific’s Scientifics 2000 Catalog for Science and Engineering Enthusiasts, page 31. several thousand feet See The Darwin Awards, Evolution in Action, “Lawnchair Larry,” pp. 280–81, by Wendy Northcutt, Plume (Penguin), New York, 2002. of these flares String soaked in potassium nitrate solution and dried. recipe and procedure About fifty years later, while listening to Ken Follett’s novel The Man From St. Petersburg, I noted that the terrorist antihero’s recipe and procedure for making nitroglycerine were consistent with how I made it as a boy in my mother’s refrigerator. depth of 5 feet digitalcollections.lmu.edu/cdm/ref/collection/chgface/id/294 . a man’s hand As in 1 Kings 18:44. dimensional analysis I was familiar with the book Dimensional Analysis by Percy W. Bridgeman, Yale University Press, New Haven, CT, 1922. joined the rest In 2015, my granddaughter Claire Goul was one of three hundred semifinalists in the same contest. It was now the Intel Science Talent Search and had become more competitive, with three top prizes of $150,000 each, compared to one top prize of $10,000 back in 1949. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:61:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 3 standard American wheel European wheels had only one green number and further improvements in the odds, such as the player betting on red or black only losing half his stake if that number came up. systems must fail One of the most well-known examples is the Pythagorean theorem from plane geometry. It says that for a right triangle, the sum of the squares of the sides equals the square of the hypotenuse. For instance, the triangle with sides 3, 4, and 5 is a right triangle and 32+42=52. Also 122+52=132 for another right triangle. There are infinitely many and we could check them one at a time, but never finish. The theorem does it all at once. a rubber ring Feynman put the rubber ring, which was made from the same material as that used on the Challenger, in the ice water and showed that, when it became cold like it did during the launch of the Challenger, it became so brittle it was likely to fail. Feynman tells the story in Classic Feynman, edited by Ralph Leighton, Norton, New York, 2006. wealthy older student T. T. Thornton for every frame You can see the film at www.edwardothorp.com . ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:62:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 4 a new strategy Baldwin, et al (1956). and even awe For the rules of blackjack and my original report of this experience, see Thorp, Beat The Dealer, 1962, rev. ed. 1966. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:63:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 5 mathematics course The course was measure theory, basic to probability and statistics. partially played decks For one fifty-two-card deck we can choose a subset by selecting 0, 1, 2, 3, or 4 Aces in five ways, similarly selecting the number of cards for values Two through Nine in each of five ways, and selecting between 0 and 16 Ten-value cards in seventeen ways for a grand total of 5×5×…5×17−1 (there are nine 5s, one each for Ace, Two,…Nine) or a little over 33 million different total partial decks. (We subtract one to delete the case where zero cards of each value are chosen, leading to a subset with no cards in it.) For the eight-deck game the corresponding figures is 33×…×33×129−1, or about 6 quadrillion (6 followed by 15 zeros) partial decks. to a gigantic For people who like to compute, suppose that each of these strategy tables was written on a separate piece of paper the size of a dollar bill. I estimate the volume of a dollar bill at 1.08 cubic centimeters so the strategy tables would occupy 37 cubic meters or 1300 cubic feet. For eight decks they would fill a space of 6.5 cubic kilometers or 1.6 cubic miles. 0.21 percent The Baldwin group said later that their figure of 0.62 percent for the casino’s edge should have been 0.32 percent. An arithmetic error caused the incorrect figure. the blackjack rules Casino blackjack rules have varied over time and among casinos. The rules I used for the calculations were then typical. (half the deck) The chance that the last twenty-six cards contain all four Aces is about 5.5 percent. type from the deck Later exact calculations give numbers which are a bit more favorable to the player. These results also are affected by the many variations in casino rules. For details see Thorp (1962, 1966), Griffin (1999), Wong (1994). $200 bankroll This book spans more than eighty years, during which the value of money has changed dramatically. For an accurate perspective the reader can convert money to current dollars using Appendix A. my mathematical work The discovery was an example in functional analysis, in which both Taylor and the mathematician were specialists. Shannon at noon Our meeting was on September 29, 1960, and I memorialized the details in a letter I wrote that same evening to a friend, mathematician Berthold Schweizer. to the academy Thorp, Edward O., “A Favorable Strategy for Twenty-One,” Proceedings of the National Academy of Sciences, Vol. 47, No. 1, 1961, pp. 110–12. the program booklet Thorp (1960). Fortune’s Formula also was the title of William Poundstone’s 2005 book covering some of this story about blackjack, roulette, the stock market, and the Kelly Criterion. The Boston Globe “Can Beat Blackjack, Says Prof.,” by Richard H. Stewart, Boston Globe, January 24, 1961, page 1. across the country For example, Columbus Dispatch (1961), Las Vegas Sun (1961), Miami News (1961), New York Herald Tribune (1961), New York World Telegram and Sun (1961), Washington Post and Times Herald (1961). ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:64:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 6 ran his story Thomas Wolfe, Washington Post, January 25, 1961, page A3. offered a strand Paul O’Neil (Life magazine, 1964), in a generally accurate story, erroneously quotes me as saying of the string of pearls, “We had it appraised first thing in the morning. It was worth $16.” Neither statement is correct. This was repeated by Bruck (1994) even though I told her this part of the Life story was wrong. A good misquote is hard to kill. the everyday gambler Schwartz, David G., Roll the Bones, Gotham Books, New York, 2006. was a characteristic Feller (1957), (1968). the investment worlds The greatest bond investor ever, William H. Gross, also learned this lesson in the casinos of Las Vegas. Motivated by Beat the Dealer, he went there in the summer of 1966, turning a $200 bankroll into $10,000. See Bill Gross on Investing, William H. Gross, Wiley, New York, 1997, 1998. He would later use the ideas in co-managing two trillion dollars for PIMCO. with the house I realized later that I would have had an advantage if I had started each deal with big bets, maintaining them if the count was good and causing the dealer to shuffle when it wasn’t. a serious loss Why might a bigger bankroll lead to more risk of losing it all? The reasons are partly technical, partly psychological. First the technical: To exploit the larger bankroll we would be likely to make maximum bets ($500 in those days) even in situations that were just slightly favorable. This would increase the size of the fluctuations in our stake and require a longer playing time than I had available to be fairly sure of coming out ahead. Now the psychological: X and Y didn’t have my level of understanding of, and confidence in, the system. If, with a $100,000 bankroll they would in fact quit when we were down say $60,000, then I would be playing and sizing bets and taking risks as though I had a $100,000 bankroll when, unknown to anyone, I really only had a $60,000 bankroll. This subtle distinction between the ostensible bankroll and the real bankroll has contributed to the downfall of many gamblers and investors, as we shall see. An additional psychological problem which I didn’t expect was Kimmel insisting playing his “pidgeon” version of my system, betting big, losing back much of the fruits of my labor, and then excitedly refusing to stop. afternoon in Boston Ivi, her mother, and two sisters came to the US after World War II as refugees from Estonia. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:65:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 7 great mathematics centers Segel, Joel, Recountings, A.K. Peters, Ltd. Wellesley, MA, 2009, tells the story of mathematics at MIT. to stay on My family’s relationship with MIT continued two generations later. Three of my grandchildren, fraternal triplets, entered together as freshmen. See, e.g., “Triplets Celebrate After ALL are Accepted to Prestigious MIT…,” London Daily Mail, Saturday, July 25, 2015. building the first one The MIT Media Lab timeline erroneously gives the year 1966 for our computer, probably because I first revealed its existence in the 1966 revised edition of Beat the Dealer. However, the correct year is 1961, when we completed and successfully tested it in Las Vegas, as explained in numerous subsequent publications, and verified in correspondence dated August 1961, between Shannon and me, now in the archives of the MIT Museum. The device itself also remains at the museum. few early players Among them were Emmanuel Kimmel (Mr. X of Beat the Dealer), Jesse Marcum (the “little dark haired guy from Southern California” in Beat the Dealer), Russell Gutting (“Junior”), Benjamin F. (“System Smitty”) Smith, and Mr. F. (who I was told was Joe Bernstein, the “silver fox” in columnist Herb Caen’s book Don’t Call It Frisco). Marcum seems to have been the only one who consistently adhered to the method. fairly thorough shuffles Of course, seven is not a magic number. The actual number of shuffles needed varies, depending how close to “random” is specified, the type of shuffle, and how “random” is measured. in private games Danger in the Cards has been out of print for many years. William F. Rickenbacker For more on this trip see the letters from Thorp and Barnhart in Blackjack Forum, Vol. XVII #1, Spring 1997, pp. 102–104, XX #1, Spring 2000, pp. 9–30, and XX #2, Summer 2000, pp. 105–107. working for the board He wrote about our experience in a syndicated column titled “Even ‘Honest’ Vegas House Cheats.” bits of publicity E.g., Time, “Games: ‘Beating the Dealer,’ ” January 25, 1963, p. 70. “It’s Bye! Bye! Blackjack” appeared Scherman, 1964. a nine-page story O’Neil, Paul. “The Professor Who Breaks the Bank,” Life, March 27, 1964, pp. 80–91. a heart attack In April of 1966. thwart these card counters Vic Vickery, “Counting on Blackjack,” Las Vegas Style magazine, May 1993, pp. 61, 67. Players Too Smart Carson City (UPI): New York Journal-American, April 3, 1964. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:66:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 8 High–Low System I called this the Complete Point Count in Beat the Dealer. pooling their bankrolls For the optimal trade-off between risk and return, the bet size in a given situation is proportional to bankroll. To the extent the players on a team are unsure about the current size of the group’s bankroll, they will tend to reduce their bets slightly. Pacific Stock Exchange Google the Blackjack Hall of Fame write-ups for more on Francesco, Hyland, Uston, and others in this chapter. the blackjack community As the popularity of the game surged, a blackjack community evolved. Special newsletters for counters were circulated and, later, websites appeared. Stanford Wong’s newsletters and websites provided cutting edge information on how and where to play for best advantage. Arnold Snyder’s quarterly Blackjack Forum had articles from leading players and theorists over its twenty years or so of publication and gave a good informal history of the war with the casinos. Anthony Curtis’s Las Vegas Advisor was a monthly guide to events, and to best deals on food, accommodations, and playing conditions. The Gambler’s Book Store, managed by Howard Schwartz, continued to offer the latest books and systems. Richard Reid’s website www.bjmath.com was a treasury of articles, workshops and discussions. The network that arose accelerated the development of new methods for advantageous play. The second line Later more exact calculations changed these numbers somewhat. They also vary slightly with the number of decks. nearest whole number Thirteen was chosen because it gave a good fit. count was to use The strengths of various card counting systems are thoroughly discussed in The Theory of Blackjack by Peter Griffin, 6th Edition, Huntington Press, 1999. in the discard tray An easy way to do this is by estimating how many half decks are left, as described in Professional Blackjack by Stanford Wong (pseud.), Pi Yee Press, 1994. the point count If you divide the ultimate strategy values in table 1 by eight and round off to the nearest whole number, you get the values of 0 or 1 that make up the complete point count. However the values for sevens and nines are about as close to 1 and −1, respectively, as they are to zero. Choosing them thusly gives the alternative point count I was using in Puerto Rico. “what I’ve got.” The quote is from Thorp (1966), pp. 84–85. 1970s, several people Notably, Keith Taft. a powerful advantage I explained the idea in Thorp, Edward O., “Non-random Shuffling With Applications to the Game of Faro,” Journal of the American Statistical Association, pp. 842–47, December 1973. A much expanded version appears in Gambling and Society, edited by W. Eadington, Charles C. Thomas, Springfield, IL, 1975, as: “Probabilities and Strategies for the Game of Faro,” pp. 531–60. cheating, the statutes N.R.S. 465.015. success much more difficult Legends of Blackjack by Kevin Blackwood and Larry Barker, Kindle eBook, April 5, 2009, tells the stories of some of the leading professionals. the Blackjack Ball “The Smartest Guy in the Room,” by R.M. Schneiderman, Newsweek, February 20, 2012, pp. 56–57. in the movie 21 Inspired by the Ben Mezrich book Bringing Down the House. rate the games Stanford Wong’s monthly newsletter is thorough. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:67:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 9 exploitable patterns Pearson, Karl, The chances of death and other studies in evolution, London, New York, E. Arnold, 1897*.* sailed the Caribbean Los Angeles Times, February 27, 2003, page B12, obituary of Albert Hibbs. See also Wilson (1965, 1970). Hibbs later wrote Caltech obituary for Hibbs at http://pr.caltech.edu/periodicals . a thing of the past Decades later, professional gambler Billy Walters found and exploited effective roulette wheels, as reported in Beating the Wheel, by Russell T. Barnhart, Carol Publishing, New York, 1992. Walters is interviewed in Gambling Wizards, by Richard W. Munchkin, Huntington Press, Las Vegas, Nevada, 2002, pp. 16–18. With a plastic playing card and a few minutes at a roulette wheel, I can tell you whether any of the dividers, or frets, separating the pockets on the rotor, are high or low, loose or tight, and which numbers will be affected. For good measure I would also check to see if the wheel was level and that the rotor hung true on its spindle. odds in his…favor Thanks to Richard Cohen for supplying this. a 1956 paper Kelly, J. L., “A New Interpretation of Information Rate,” Bell System Technical Journal, Vol. 35, 1956, pp. 917–26. and the stock market Thorp, Edward O., “Optimal Gambling Systems for Favorable Games,” Review of the International Statistical Institute, Vol. 37, 1969, pp. 273–93; Thorp, Edward O., “The Kelly Criterion in Blackjack, Sports Betting, and the Stock Market,” Handbook of Asset and Liability Management, Vol. 1, S.A. Zenios and W.T. Ziemba, editors. Elsevier, New York, 2006. called wearable computers “A Brief History of Wearable Computing” timeline—MIT Media Lab, www.media.mit.edu/wearables/lizzy/timeline.html . only one person O’Neil, Paul, “The Professor Who Breaks the Bank,” Life, March 27, 1964, pp. 80–91. our roulette system Thorp, Edward O., Beat The Dealer, 2nd Edition, Vintage, New York, 1966. the details later Thorp, Edward O., “Systems for Roulette I,” Gambling Times, January/February 1979; Thorp, Edward O., “Physical Prediction of Roulette I, II, III, IV,” Gambling Times, May, July, August, October 1979; Thorp, Edward O., The Mathematics of Gambling, Lyle Stuart, Secaucus, New Jersey, 1984. by hardware problems Bass, Thomas A., The Endaemonic Pie, Houghton Mifflin, New York, 1985. May 30, 1985 One of the uninformed people writing for Wikipedia claimed our computer was used to “cheat” at roulette. This is false. We and the many others who subsequently used roulette and blackjack computers could not be prosecuted under the strong anti-(player) cheating statutes already in place in Nevada. That’s why the Nevada state legislature had to pass a law specifically outlawing “devices.” ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:68:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 10 thermonuclear weapons Ulam, S. M., Adventures of a Mathematician, Scribner’s, New York, 1976. to a set of rules These rules are designed to make the casino’s edge roughly the same on the two bets. for the two bets Thorp and Walden (1966). Walden and I proved Thorp and Walden (1973). is in blackjack Griffin (1995), Thorp (1984), Vancura (1996). so the effect Griffin (1995), Thorp (1984), Vancura (1996). as many tables Vancura (1996). ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:69:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 11 can be ruinous The Kelly Criterion highlights the perils of overbetting even when you have the edge. waiting fifteen years The NASDAQ Composite finally exceeded its March 2000 peak in April of 2015. However, investors were still behind more than 20 percent after adjusting for inflation. weren’t available yet Bogle on Mutual Funds, pp. 169–70 says “The indexing concept was…introduced…to the mutual fund industry in 1976.” and in fact by the author himself, John C. Bogle. or the stock market According to a Fidelity Research Institute Report dated February 2007 stocks averaged about 10 percent a year, beating residential real estate by more than 4 percent a year for 1963–2005 and by more than 5.5 percent annualized over 1835–2005. Bonds also did better than residential real estate. to Mr. Market See The Warren Buffett Way, by Robert G. Hagstrom, Jr., Wiley, New York, 1994, pp. 50–51, and The Warren Buffett Portfolio, by Roberg G. Hagstrom, Jr., Wiley, New York, 1999, pp. 143–44. thesis on the subject Kassouf, Sheen T., Evaluation of Convertible Securities, Analytical Publishers Co., 602 Vanderbilt Street, Brooklyn, New York 11218, 1962. A brief summary of hedging with warrants and convertible bonds. Beat the Market You can read more about our theories and investments in Beat the Market at www.edwardothorp.com . ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:70:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 12 researcher and biologist You can read about his remarkable scientific aptitude and career on The National Academics Press website, www.nap.edu/books , in Biographical Memoirs v.53 (1982), National Academy of Sciences. In 1974 Gerard, who became one of my first investors using money paid out to him by Buffett, passed away. His wife, Frosty, survived him for several years. When she died, part of their Princeton Newport investment went to support the University of California, Irvine. analysis of common stocks The classic 1940 second edition was reissued by McGraw-Hill in 2002. was his contribution Some biographies of Buffett say $105,000. My figure, which I recall from conversations with Warren, is confirmed in the definitive biography of Buffett, The Snowball, by Alice Schroeder, Random House, New York, 2008. a manic 150 percent Returns on equities are from Ibbotson Associates (2007). Peter Minuit Minuit, Peter (1580–1638), Dutch colonial governor in America, helped establish New Amsterdam, the settlement that became New York City. He joined the Dutch West India Company and set out for the company’s settlement in America. He reached Manhattan Island in 1626 and became the first director general of the colony. Minuit purchased the island from one of the Algonquian-speaking tribes with trinkets valued at 60 Dutch guilders, a sum later calculated at $24. with all the improvements From 1626 to 1968 is 342 years. Twenty-four dollars compounding at 8 percent would become $6.47 trillion, 1⁄8 or more of the net worth of the USA at that time. By 2013 this grows at 8 percent to $206 trillion, enough to buy half the world: estimating US 2013 net worth as $100 trillion ($77 billion for households plus $23 billion for government) and assuming the US has about 25 percent of the globe’s total net worth gives an estimate of $400 trillion for the market value of the world. Discover magazine article “May the Best Man Lose,” Discover, Nov. 2000, pp. 85–91. For more on voting paradoxes, see Poundstone, William, Gaming the Vote: Why Elections Aren’t Fair (and What We Can Do About It), Hill and Wang, New York, 2008, and Saari, Donald G, “A Chaotic Exploration of Aggregation Paradoxes,” SIAM Review, Vol. 37, pp. 37–52, March 1995, and A Mathematician Looks at Voting, American Mathematical Society, 2001. are typically stumped For more on nontransitive dice see Gardner, Martin, The Colossal Book of Mathematics, Norton, New York, 2001, and Finkelstein, Mark and Thorp, Edward, “Nontransitive Dice with Equal Means, in Optimal Play: Mathematical Studies of Games and Gambling, Stewart N. Ethier and William R. Eadington, editors, University of Nevada, Reno, 2007. early as 1936 See Schroeder, loc. cit. the last ten years Loomis, Carol, “The Jones Nobody Keeps Up With,” Personal Investing, Fortune, April 1966. few existing hedge funds The hedge fund world at the start of 1968 was tiny, almost insignificant. The combined capital in dollars was less than one one-thousandth of what it was in 2016. Back in 1968, the top twenty funds ranged in size from $80 million down to $12 million. Altogether, there were about 150 funds, with aggregate capital of a billion or two. This grew to more than two trillion dollars a half a century later. Since the dollar value of GNP was about a tenth of what it was forty-eight years later, hedge funds compared to GNP grew more than a hundred times from 1968 to 2016. and were closing down The decline is catalogued in Robertson, Wyndham and Haines, Angela, “The Hedge Funds’ Dubious Prospects, A Report on Twenty-Eight Funds,” Personal Investing, Fortune, October 1970. The group consisted of the largest hedge funds as of December 31, 1968. The big winner was Buffett Partnership, Ltd., which was closing after a spectacular twelve years because stock prices were too high compared with the underlying value of the companies. The only other fund whose assets had increased was Steinhardt, Fine, Berkowitz \u0026 Co. Wall Street Letter The ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:71:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 13 never before tried The year before, Arbitrage Management Company was set up to exploit the hedging ideas in Beat the Market. Among others it involved Harry Markowitz, who later won a Nobel Prize in Economics, and John Shelton, a leading finance professor and warrant theorist. Though profitable, the gains were not enough to keep it from disappearing from the scene after three years. would transform physics For a full account see the inspiring Annus Mirabilis: 1905, Albert Einstein and the Theory of Relativity, by John and Mary Gribbin, Penguin, New York, 2005. of stock price changes See the article by Case M. Sprenkle in The Random Character of Stock Market Prices, Paul H. Cootner, editor, MIT Press, Cambridge, MA, 1964. riskless interest rate Academic economists and financial theorists have long assumed, as in the Black-Scholes formula, that US Treasury bonds and their short-term version, bills, are riskless. The argument goes that the government can always print the money it needs in order to pay the interest and to redeem them at maturity. Congressional battles over whether to raise the debt ceiling, such as the clash in 2013, exposed the fallacy. The US can pay its debts but it may choose not to. Default is possible. Since investors generally demand to be paid a higher rate of interest to purchase risky debt, this dispute over the debt ceiling has led to higher borrowing costs for the US. Thus, those opposing an increase in the debt ceiling have made the debt itself larger. warrant expiration date For an account of what I did, see my articles in Wilmott magazine, Sept. 2002, Dec. 2002, and Jan. 2003. They are also available on my website at www.edwardothorp.com . For an introduction to the methods of plausible reasoning see Mathematics and Plausible Reasoning, Vols. I and II, by George Polya, Princeton University Press, 1954, and his more elementary How to Solve It, 2nd Edition, Doubleday, 1957. I began using it For a background discussion, see Derivatives: Models on Models, by Haug, Espen Gaarder, Wiley, New York, 2007, pp. 27–44. Beat the Market They acknowledge this in their famous paper, Black, F. and Scholes, M., “The Pricing of Options and Corporate Liabilities,” Journal of Political Economy, Vol. 81, May–June 1973, pp. 637–54. the identical formula The fact that their formula was identical to the one I was using verified that my plausible reasoning led to the correct result. down when he had Buffet: The Making of an American Capitalist, by Roger Lowenstein, Random House, New York, 1995, page 156. customers high fees In a massive replay, hundreds of trillions of dollars’ worth of derivatives contracts now trade Over-the-Counter (OTC). Again, the banks and brokers love the high fees and are resistant to standardizing the contracts. The OTC contracts are under-collateralized and could easily precipitate a financial collapse similar to what we saw in 2008–09. Exchange-traded standardized contracts could eliminate this threat. from the formula Years later I heard that one trader had consulted Black and also was using formula prices when trading began at the CBOE. The Wall Street Journal “Computer Formulas Are One Man’s Secret to Success in Market,” by Jonathan R. Laing, Wall Street Journal, September 23, 1974, page 1. running my hedge fund Fortune’s Formula, page 172, incorrectly reports that I was making as much then as Paul Newman was. One of my stories Beat the Dealer, 1966 edition, pp. 167ff. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:72:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 14 where I was speaking “Extensions of the Black-Scholes Option Model,” Thorp, Edward O., Contributed Papers 39th Session of the International Statistical Institute, Vienna, Austria, August 1973, pp. 1029–36. model for stock prices This is the so-called lognormal model for stock price changes. A different but important situation which it fails to cover for option valuation is the bimodal, or two-peaked, payoff distribution which arises when one company makes a tender offer for another. and applied finance Fischer Black and the Revolutionary Idea of Finance by Mehrling, Perry, Wiley, New York, 2005. were eventually published See “Option Pricing: The American Put,” by Parkinson, Michael, Journal of Business 1977, v50(1), pp. 21–36, and “The Valuation of American Put Options,” by Brennan, Michael J. and Schwartz, Eduardo, Journal of Finance 1977, v32(2), pp. 449–62. than $1 million Andrew Tobias used my account of this trade and several others we did in his book Money Angles, Simon and Schuster, New York, 1984, pp. 68–72. is a general rule If you make 20 percent in year one and 30 percent in year two the wealth and relatives are 1.20 and 1.30. Multiplying gives 1.56, which is the wealth relative for the two successive years, the amount a dollar grows to if reinvested. Thus the two-year gain is 56 percent, not 20 percent+30 percent or 50 percent. If you simply add the numbers in the table you get +11.7 percent for the market, which isn’t nearly as bad. But to find out what a dollar invested at the start of 1973 grows to, or diminishes to, you need to multiply together the results for successive periods, which produces the –0.5 percent figure. The result from investing $1 for one period is called the “wealth relative” (wealth at the end of the period). For instance, if you make 12 percent during the year, the wealth relative for that twelve months is 1.12. When we add the numbers for the returns for PNP limited partners the result is 42.1 percent, significantly less, in this case, than the actual 48.9 percent figure obtained by multiplying successive wealth relatives. only month-end values Month-end figures for the S\u0026P 500 are from Ibbotson. Because the Great Depression was, on average, deflationary, the results based on inflation adjusted or “real” returns are less extreme. quality and maturity More precisely, “duration,” which is the discounted weighted average time for the payment stream. hundred billion dollars During the early stages of the run-up in interest rates, S\u0026Ls were raising money by selling puts at bargain prices on Government National Mortgage Association (GNMA) bonds. Bonds usually trade in thousand-dollar denominations or “par” amount and are quoted as a percent of par, so these bonds—quoted at 98—were currently selling at $980 each. The puts we bought from the S\u0026L which issued them gave us the right to sell the bonds to that S\u0026L at this price for the life of the put, which in our case ranged from twelve to eighteen months. If the bonds fell, we could buy the bonds below 98 and “put them” to the bank, which was required to pay us 98 under the terms of the contract. If the bonds rose instead, the puts would expire worthless. As the price of a put tends to move in a direction opposite to that of the underlying security, we hedged the risk of loss on the put by buying GNMA (“Ginnie Mae”) futures, i.e., contracts to buy GNMA bonds at a certain price and time in the future. In futures markets, contracts require daily settlement of gains and losses. If the bonds fell in price we would have temporarily to lay out cash for our losses in the futures markets, even though we would ultimately get the money back when we cashed in our puts. Since our cash and borrowing power was finite, this limited how big a hedge we could safely carry to completion. To estimate the maximum safe size for our hedge we needed to consider the lowest price anyone imagined GNMA bonds could drop to during the next eighteen months. The number","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:73:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 15 at UC, Irvine It was then the Graduate School of Management. were earnings yield Earnings yield is E/P, Where E is annual earnings (either trailing twelve months or predicted next twelve months—you choose). This is the inverse of P/E, the famous price/earnings ratio, but E/P is better because of problems in the interpretation of P/E when E is zero or negative. several billion dollars Market Neutral Strategies, Bruce I. Jacobs and Kenneth N. Levy editors, Wiley, New Jersey, 2005. when it opened Hours for the N.Y.S.E. were from 10 A.M. to 4 P.M. Monday through Friday, from October 1, 1974, until September 30, 1985, when the opening time changed to 9:30 A.M. of which we pioneered Among them were the interest rate swap (the object here was to hedge away interest rate risk in our positions), the bond cash and carry, the commodity cash and carry, capturing a profit when closed-end funds could be purchased below their liquidation value, and special deals. historical stock price data I learned only recently, while reading the interview of Harry Markowitz in Masters of Finance, IMCA, Greenwood Village, CO, 2014, page 109, that Markowitz and Usmen got the same answer for daily S\u0026P 500 Index price changes as we did for a much larger data set of two hundred individual stocks. Their work, done sometime before 1987, and submitted before he won the Nobel Prize for Economics in 1990, was initially rejected for publication (!), appearing elsewhere only in 1996. of the impact Market impact refers to the fact that “market orders” to buy are, on average, filled at or above the last previous price and “market orders” to sell tend to be filled at or below the last previous price. gained 9 percent The accounting period with an odd length of five months arose here for PNP because the fiscal year end for PNP changed in 1987 from October 31 to December 31. statistics confirmed Common metrics include the Sharpe ratio, the Sortino ratio, the distribution of drawdowns, and the MAR ratio (annual return divided by maximum drawdown). See, for instance, the three-part series by William Ziemba in Wilmott magazine: “The Great Investors,” March, May and July 2006. or losing quarters For comparison, the S\u0026P 500 was down in eleven of the thirty-two full quarters and small company stocks lost in thirteen. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:74:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 16 associates, and clients Den of Thieves by James B. Stewart, Simon and Schuster, New York, 1991. Ken Griffin The Quants by Scott Patterson, Crown, New York, 2010. first limited partner Conversation with Citadel’s Scott Rafferty. $5.6 billion The Forbes 400 list likely misses people that should be on it, the numbers are estimates, and wealth fluctuates, so the rankings are not exact. For example, Warren Buffett qualified when the list was started in 1982 but they didn’t discover him until 1985! Also missing from the Forbes list are “The $13 Billion Mystery Angels,” of the article by Zachary Mider, Bloomberg Businessweek, May 8, 2014. Mider reveals that a group of former PNP employees made at least $13 billion in the twenty-five years after starting their own firm in 1989, with the aid of our quantitative methods and computer algorithms. Malibu, California For some of his thoughts, see J. Paul Getty, How to Be Rich: The Success of a Billionaire Businessman, Playboy Press, New York, 1965. Corporation, said Los Angeles Times Magazine, January 23, 2000, pp. 10ff and page 35. D. E. Shaw When D. E. Shaw hired one of our key employees the first thing they reportedly did was debrief him for six hours to find out everything he could tell them about PNP. convertible bonds The program had methods for incorporating quality deterioration and credit default risk which were, I believe, unique at the time. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:75:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 17 I’ve got enough The New Yorker, May 16, 2005. buyers and sellers We chose a sample of ten option trades from the forty which we had not already proven to be fakes. a legal mess For the classic story of a legal mess, see Jarndyce versus Jarndyce in Bleak House by Charles Dickens*.* his best investment If Madoff is really gaining 20 percent a year and their best alternatives give, say, 16 percent a year, then they’re only out 4 percent a year. to destroy documents Rothfeld, Michael and Strasburg, Jenny, “SEC Accused of Destroying Files,” Wall Street Journal, August 18, 2011, page C2. the headline article Arvedlund, Erin E., “Don’t Ask, Don’t Tell,” Barron’s, May 7, 2001. in the early 1990s “Bernard Madoff Gets 150 Years in Jail for Epic Fraud (Update 7), Bloomberg.com , June 29, 2009. $65 billion News Release, “Bernard L. Madoff Charged in Eleven-Count Criminal Information,” U.S. Attorney for the Southern District of New York, March 10, 2009. One individual reportedly One Jeffry M. Picower, according to The New York Times, Sunday, July 5, 2009, page B2. According to a later report in The New York Times by Diana B. Henriques, October 2, 2009, page B5, the trustee liquidating the Madoff assets, Irving H. Picard, “reported that one Picower account had been overdrawn by $6 billion when Mr. Madoff was arrested.” the individual guesses If half of the crowd’s guesses are on each side of the average it is a mathematical fact that the average will be closer to the correct value than at least half of the individual estimates. What is interesting is that the crowd consensus is often much better yet. hedging Japanese warrants “The Money Man: A Three-time Winner,” Forbes, November 25, 1991, pp. 96–97. so-called secretary or marriage problem Today one might retitle it as “The Significant Other Problem.” ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:76:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 18 to gain a profit A couple of weeks after the EMLX hoax, the Los Angeles Times reported that the SEC, in a “web fraud sweep,” accused thirty-three companies and individuals of illegally using the Internet to make more than $10 million in profits by driving up the prices of more than seventy small thinly traded stocks, hyping them on chat rooms, websites, and in email messages. before the Internet Two readable and entertaining accounts are the famous Extraordinary Popular Delusions and the Madness of Crowds, by Charles MacKay, and its more current sequel, Ponzi Schemes, Invaders from Mars and Other Extraordinary Popular Delusions, by Joseph Bulgatz. Hell on one’s feet See page 71 of Haugen, The New Finance: The Case Against Efficient Markets, Second Edition (1999). Recent reports “Toxic Equity Trading Order Flow on Wall Street,” by Arnuk, Sal and Saluzzi, Joseph, A Themis Trading LLC White Paper, www.themistrading.com , and “Algo Traders Take $21bn in Annual Profits,” by Tom Fairless of Financial News, quoting the research firm Tabb group. for the A shares Reported as of 1:22 P.M. New York time on July 24, 2009. one of the mechanisms “Traders Profit With Computers Set at High Speed,” by Charles Duhigg, New York Times, Friday, July 24, 2009, Page A1, and “SEC Starts Crackdown on ‘Flash’ Trading Techniques,” by Charles Duhigg, New York Times, Wednesday, August 5, 2009, Page B1. See also: (1) Patterson, Scott, and Geoffrey Rogow, “What’s Behind High-Frequency Trading,” Wall Street Journal, Saturday/Sunday, August 1–2, 2009, page B1. (2) Wilmott, Paul, “Hurrying Into the Next Panic?”, New York Times, Wednesday, July 29, 2009, page A19. Krugman disagrees sharply Krugman, Paul, “Rewarding Bad Actors,” New York Times, Monday, August 3, 2009, page A19. See also O’Brien, Matthew, “High Speed Trading Isn’t About Efficiency—It’s About Cheating.” The Atlantic, February 2014. cut a trading rate The dollar value of all trades in U.S. equities varies from year to year, as does the portion created by the high-frequency traders. Business Day headline New York Times, September 28, 2000. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:77:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 19 billion shares annually The Medallion Fund, a hedge fund closed to new investors, run by mathematician James Simons, includes a similar and far larger trading operation than ours with a higher rate of turnover and a vast annual trading volume. Now an investment vehicle for Simons and his associates in his firm Renaissance Technologies Corporation, it is probably the most successful hedge fund in history. of our researchers David Gelbaum. Gerry Bamberger For this and much more, see A Demon of Our Design, by Richard Bookstaber, Wiley, New York, 2008. in the securities industry See the book The Quants: How a New Breed of Math Whizzes Conquered Wall Street and Nearly Destroyed It, by Scott Patterson, Crown, New York, 2010. and more powerful It was based on the statistical notion of “principal components”; We called it ETS, for “Equity Trading System.” Japanese warrant hedging See Forbes, November 25, 1991, pp. 96–99, “A Three Time Winner,” and the article by Shaw, Thorp and Ziemba, “Risk Arbitrage in the Nikkei Put Warrant Market of 1989–1990,” App. Math. Fin. 2, 243–71 (1995). trend is greater The graph makes returns in the second epoch appear even more variable relative to the first than they really were. The proper display for comparing variability and growth rates is a so-called lognormal graph. For that graph of XYZ’s performance, see Thorp, Edward O., “Statistical Arbitrage, Part VI,” Wilmott, July 2005, pp. 34–36. had ever experienced Reportedly, Simons’s secretive Renaissance Partners had a similar experience in August of 2008, losing 8 percent or so in a few days, then rebounding to make more than 100 percent for the year. employees, only six Since the six people in my office also had other responsibilities, we had only 3.5 “full-time equivalents” on the project. in statistical arbitrage Firms doing statistical arbitrage, such as the hedge fund group Citadel, already had in place most of the technology, talent, and expertise needed later to create and implement high frequency trading (HFT). For an account of HFT, see the book Flash Boys by Michael Lewis; In 2005, three years after we went out of the statistical arbitrage business, Steve and I worked with Jerry Baesel, who was then at Morgan Stanley Asset Management, to see if it was worth restarting. We concluded it was marginal because simulation showed a recent unlevered return of 10 percent or so, not attractive enough when compared to other investment opportunities then available to us. Meanwhile, we put the “shrink-wrapped” software on our shelf with a tag saying “add people and data to reactivate.” Had we been running the program during the 2008–09 economic crisis, I suspect that we would have had a rerun of our “miraculous” 1998–99 results. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:78:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 20 or ten thousand shares The number of shares for which a depositor can apply varies from one offering to another, but between ¼ percent and 1 percent of the issue is common. deals aren’t as good If, for example, the business value in the MW deal were $5 instead of $10, management helped themselves to $3 of the final enterprise value, and selling costs were $1, buyers of the IPO would get a value of $5+$10−$3−$1=$11 for their $10. The market won’t recognize this value right away, so the stock will start trading below $11 a share. Meanwhile, market prices for the group of S\u0026Ls as a whole could drop, taking the new stock’s price down, too. Or, if the market sees management as greedy—$3 is a lot to take out for themselves—or not competent to put the new capital to work, the stock will decline. so-called opportunity cost Opportunity cost refers to the cost of the opportunity that was given up. In addition to the cost of redirecting our capital from other investments, it includes the value of whatever didn’t get done because we redirected our personal time to the S\u0026L project. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:79:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 21 never did, either After BPL closed, Buffett accumulated Berkshire stock whenever he could, often from his friends, associates, and former partners. According to Schroeder, The Snowball, pp. 341–42, the stock was unregistered then so it had to be traded privately. As Forbes Forbes 400, October 22, 1990, page 122. growth, decided Quietly. made my first purchase Buffett was so uninformative about his plans that his own children sold their stock fairly early. I started buying about the time his daughter Susie sold off the last of hers. $74,000 a share “Ordinary” investors tend to switch their money from securities that drop in price to those that have gone up, a strategy sometimes described as chasing returns. An academic study of all domestic US equity mutual funds covering 1991 through 2004 showed that this behavior by the fund investors reduced their annual returns by an average of 1.6 percent; Friesen, Geoffrey C. and Sapp, Travis R.A., “Mutual Fund Flows and Investor Returns: An Empirical Examination of Fund Investor Timing Ability,” Journal of Banking and Finance, September 2007. Summarized in “Buying High and Selling Low,” Mark Hulbert, New York Times, July 12, 2009, Mutual Fund Report, page 18. a famous interchange Paul Marx told me this story. principles of security analysis As practiced by Graham, Dodd, Buffett, Munger, Fisher and others. Three Successive Periods I chose the dates for the first three periods because the price graphs suggested that they were natural divisions. The fourth and last period covers the aftermath of the Great Recession. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:80:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 22 fund in the country The woes of Harvard’s endowment fund are chronicled in “Rich Harvard, Poor Harvard,” by Nina Munk, Vanity Fair, August 2009, pp. 106ff. average 18 percent This loss figure of 18 percent is commonly cited by the industry and I use it throughout the book. However, it substantially understates the drop because some funds did not fully discount their toxic assets, others with illiquid assets reported too late to be counted, reports are voluntary so losers are less likely to respond, and the impact of funds which disappeared during the year may not have been included. billion in 2007 New York Times, March 25, 2009, page B1. Management fees Incentive fees in 2015 averaged 17.7 percent of any profit, compared to 19.3 percent in 2008, according to The Wall Street Journal, September 10, 2015. Management fees had declined to an average of 1.54 percent. hedge fund returns The studies encountered difficulties obtaining clean long-term data and in correcting for survivorship bias: funds that died early and may not be in the database are expected to have performed more poorly. Omitting them and studying only the survivors overstates the results. Later analyses Dichev, Ilia D. and Yu, Gwen, “Higher risk, lower returns: What hedge fund investors really earn,” Journal of Financial Economics, 100 (2011) 248–63; Lack, Simon, The Hedge Fund Mirage, Wiley, New York, 2012. One study Bloomberg Businessweek, “Buzzkill Profs: Hedge Funds Do Half as Well as You Think,” August 17, 2015, reports on a study by Getmansky, Lo, and Lee. Using data from 1996 through 2014, they conclude that a reported average return of 12.6 percent was really 6.3 percent when the losers, who tend not to report, are included. As Shakespeare Julius Caesar, Act I, Scene II, lines 140–41. in an article International Fund Investment, April 2000, page 64. managers that year New York Times, March 25, 2009, page B1. playing this game New York Times, September 9, 1999, National Edition, page C10. tend to fade Consider the statistical phenomenon of regression toward the mean. of their stake Numerous books and articles give accounts of what happened, including Roger Lowenstein, When Genius Failed, Random House, New York (2001), “Failed Wizards of Wall Street,” Business Week, September 21, 1998, pp. 114–20, and “Hedge Fund’s Star Power Lulled Big Financiers Into Complacency,” by Timothy L. O’Brien and Laura M. Holson, New York Times, October 23, 1998. For some of my comments, see Tim O’Brien’s story “When Economic Bombs Drop, Risk Models Fail” in the New York Times, October 4, 1998. A sensational Nova program on LTCM that aired in February 2000, asserted that the total LTCM contracts outstanding amounted to a trillion dollars. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:81:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 23 good to him Perhaps too good. He later stole millions from his firm and fled to Brazil. following classification Forbes, October 11, 1999, page 60. $83 trillion Orange County Register, March 7, 2014, Business, page 3. is not available How big is the underground economy? What’s the value of privately held non-traded businesses? How much of the national wealth consisting of patents, copyrights, and innovation is being counted? Most household labor is not monetized, hence is not customarily counted in national income. $1.36 million Source: Money magazine report on a University of Michigan study, the lead author being Fabian T. Pfeffer. Much of the increase in inequality comes from the fact that housing prices were about the same in 2014 as they were in 2003, whereas US stocks, as represented by the S\u0026P 500, doubled. The rich have a higher proportion of their wealth in stocks and less in housing than poorer people. for large stocks Ibbotson Associates yearbook. saves $6 each day He saves more each day in later years assuming the price of cigarettes increases along with inflation. An article http://quickenloans.quicken.com/Articles/fthbc_afford_budget.asp . $10,000 difference grows Mentally calculated by the rule of 240 in Appendix C. The formula Assume the power law N = AW–B, where W is a high enough wealth level to exclude most people, and N is the number having wealth at least W, and A and B are unknowns. The two facts I used to find A and B were (1) when N = 400, W = $1.3 billion, and (2) the total wealth of the 400 was $1.2 trillion, giving an average value of three times the cutoff. The result is N = 400 [(1.3 billion)/W]4/3. The value 4/3 for B seems to be roughly the same from year to year, as the average wealth seems to be about three times the cutoff. So you can recalibrate the formula each year that this is true by simply replacing 1.3 billion by the current cutoff. Using 1999 data, in “How Rich Is Rich?—Part 1,” Wilmott magazine, July 2003, pp. 44–45, I found the slightly different value 1.43 instead of 4/3. Perhaps coincidentally, the 2009 Forbes 400 edition offers a calculator using this formula on page 20 at forbes.com/baldwin . Forbes, page 20, says the formula uses the exponent 0.7, which equals 1/B and, therefore, a value for B of 1.43. Their formula inverts mine and expresses W as a function of N. For an extended discussion of formulas for estimating wealth and income, including evidence for the Pareto equation, see Inhaber and Carroll (1992) and the many further references therein as well as Scafetta, Picozzi and West, “An out-of-equilibrium model of the distribution of wealth,” Quantitative Finance, Vol. 4 (2004) pp. 353–64. billion dollars or more The Orange County Business Journal listed 36, with Lakers basketball star Kobe Bryant in 36th place at $250 million. Since I know of people they missed, the figure of 49 may be closer to the truth. $81 billion The Gates household had as much wealth as 150 thousand average US households. In other words, one one-thousandth of all the private wealth in the United States. large increases in wealth Carrying this a step further, you could guess someone’s future wealth, in current dollars, by estimating the present value of their future savings, plus their current net worth. This is similar to some methods used by analysts to calculate the value of a company in current dollars, and hence the fair price for its stock. Using an estimated future inflation rate you could then arrive at a value for their wealth on any future date. $37 million each Bloomberg, August 17, 2009, citing UC–Berkeley economics professor Emmanuel Saez, noted for his continuing studies of, and statistics on, the distribution of income and wealth in America. Note that the average of $37 million, divided by the cutoff of $11.5 million, is 3.2, very close to the result of the same calculation for the wealth distribution of the Forbes 400, suggesting that 2007 superrich taxable income followed the","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:82:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 24 disputed origin The claimed sources include Benjamin Franklin, various Rothschilds, Albert Einstein, Bernard Baruch, and “unknown.” $22 million result These figures do not include trading costs or income taxes. A buy-and-hold investor loses little to trading costs and is taxed only on dividends. Taxes, if any, vary with the investor. less than the last So-called decreasing marginal utility. as one day less Adults with the same chronological age vary widely in their fitness age. Qualifiers for the Senior Olympic Games have a functional and fitness age averaging twenty-five years less than their calendar age, as reported in “Older Athletes Have a Strikingly Young Fitness Age,” by Gretchen Reynolds, in the New York Times, July 1, 2015. Studies of identical twins give persuasive evidence of the longevity benefits of exercise. See, for instance, “One Twin Exercises, the Other Doesn’t,” Gretchen Reynolds, New York Times, March 4, 2015. forty hours of commuting Assuming he works on average about twenty days a month. a month to operate Costs include fuel, maintenance, insurance, license fees, and depreciation, plus the owner’s time spent taking care of these. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:83:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 25 the US stocks The Vanguard Total Stock Market Index Admiral Shares, ticker symbol VTSAX, does this. Actually, it invests in each stock proportionally to the market value of the so-called float, which is the estimated fraction of freely trading shares, as opposed to shares being held which are not available for trading. The difference in performance between the two methods has been negligible. it first appeared Justin Fox, The Myth of the Rational Market, page 119, reports that Ben Graham, in 1962, pointed out that investment funds as a whole shouldn’t expect to beat the market “because in a significant sense they…are the market.” the clearest exposition Sharpe, William, “The Arithmetic of Active Management,” Financial Analyst’s Journal, Vol. 47, No. 1, pp. 7–9, January/February, 1991. all areas of investing According to Lipper, Inc., Wall Street Journal, July 6, 2009, section R, the average equity mutual fund expense ratio alone was 1.22 percent in 2007, compared to 0.20 percent for Vanguard’s no-load equity index funds. As the expense ratio is only a part of the fees investors pay, the “helpers” collect substantially more than 1 percent per year and, with trading costs, active investors lag passives by well over 2 percent annually. five and ten years Fund Track, by Sam Mamudi, Wall Street Journal, October 8, 2009, page C9. corresponding economic loss These extra taxable gains or losses will be offset later if you liquidate your investment. to catch up Taxes leave me with 70 percent of my sales price. To get back to $100, $70 has to increase by $30 or 42.6 percent. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:84:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 26 beat the market This sounds nonsensical at first. What it means is that no one has any information whatsoever that has predictive value. to the contrary They display the well-known characteristic known as cognitive dissonance. and hundreds of books An excellent history of these meanderings is Justin Fox’s book The Myth of the Rational Market. all the future earnings Interpreted as net value paid out or accumulated for the benefit of a sole owner. on inside information As chronicled by James Stewart in Den of Thieves, Connie Bruck in The Predators’ Ball, and others. this type profitably Tobias, Andrew, Money Angles, Simon and Schuster, New York, 1984, pp. 71–72. Often, management would offer to redeem shares at an intermediate price, thus cashing out the dissidents and retaining an asset base of unredeemed shares which they could continue to be paid to manage. in a few months Buying SPACs was not without risk because the assets were not protected from creditors in bankruptcy. Jeff, who discovered the strategy, researched the risks in each case before investing. or 82 percent I have omitted details such as how the actual cash required for the investment might vary from the $9,000 of the example because of the interaction of margin regulations with the investor’s preexisting portfolio, and also because of time-varying marks to the market on the short position. The Wall Street Journal Wall Street Journal, March 3, 2000, page C19, “Palm Soars As 3Com Unit Makes Its Trading Debut.” the EMH explained Malkiel, Burton G., A Random Walk Down Wall Street, Norton \u0026 Co., New York, 2007. The New York Times New York Times, March 3, 2000, page A1, “Offspring Upstages Parent In Palm Inc.’s Initial Trading.” academic literature documents It often takes weeks or months for the stock price to fully adjust after announcements of unexpected earnings, stock buybacks, and spin-offs. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:85:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 27 already been counted Mutual fund management companies and hedge fund general partnership interests have a separate and often considerable market value but they have already been counted as part of the private equity subcategory. between asset classes For a highly mathematical discussion of this effect, sometimes called “volatility pumping,” see The Kelly Capital Growth Investment Criterion: Theory and Practice, editors Leonard C. MacLean, Edward O. Thorp, and William T. Ziemba, World Scientific, 2010*.* The results For a comprehensive history of returns for world stock markets, see Triumph of the Optimists, by Dimson [et al]. 1940–2004 period “Causes of the United States Housing Bubble,” from Wikipedia, 09/16/09 version; Ziemba, William, “What Signals Worked and What Did Not 1980–2009, Parts I, II, III, Wilmott magazine, May, July and Sept. 2009. owning your home On average, significant money in excess of inflation is made in commercial real estate but not in residential real estate. Homeowners and would-be buyers often don’t understand the distinction. They also are misled by the stories of big winners at various times and in different localities. They share this error with stock market investors, which is no surprise as many are the same people. Behavioral finance theorists have analyzed this human tendency. median large endowment “Princeton’s Endowment Declines 23 percent,” by John Hechinger, Wall Street Journal, September 30, 2009, page C3. Fortune’s Formula The title, Fortune’s Formula, should sound familiar, as it was the title of the talk I gave on blackjack to the American Mathematical Society in 1961. Bill Poundstone considerately asked if he might use it as his title. In Beat the Dealer I called this, naturally enough, the Kelly Gambling System. Since 1966 I’ve called it the Kelly Criterion and the name has stuck. larger the bet You can read about the details in articles I have written, most of which are available on my website at www.edwardothorp.com . considerable controversy In addition to Poundstone’s history, mathematically trained readers can study some of the burgeoning modern developments in The Kelly Capital Growth Investment Criterion: Theory and Practice, editors Leonard C. MacLean, Edward O. Thorp, and William T. Ziemba, World Scientific, 2010. As he told The Wall Street Journal “Old Pros Size Up the Game,” by Scott Patterson, Wall Street Journal, March 22, 2008, page A9. Gross left PIMCO in 2014 and went to Janus to manage money. out in Wilmott magazine See “Understanding the Kelly Criterion,” by Edward O. Thorp, Wilmott, May 2008, pp. 57–59, and http://undergroundvalue.blogspot.com/2008/02/notes-from-buffett-meeting-2152008_23.html . Computer simulations The simulations were done by mathematician Art Quaife. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:86:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 28 stock index fund Why not continue to hold Berkshire? One reason is that I can’t foresee who will be managing the endowment in the distant future and believe it best to lock them into the well-understood mechanical approach of indexing. This avoids the waste that occurs from active investing. H. W. Brands Fortune, August 11, 2003. resilient than universities As quoted in Sample, Steven B. and Bermis, Warren, Los Angeles Times Book Review, July 13, 2003, page R9. criteria for selection I thank Professor Ronald Stern for encouraging and facilitating the creation of the chair, Paul Marx of the University Foundation for legal help and for many insightful conversations, and my wife, Vivian, for her part in creating the conditions that made our contribution possible. early card counters See The Bond King, by Timothy Middleton, Wiley, New Jersey, 2004, and Everything You’ve Heard About Investing is Wrong, by William H. Gross, Random House, New York, 1997, as well as the revised version Bill Gross on Investing, by William H. Gross, Wiley, New York, 1997, 1998. past twenty-five years Gross, 1997, op. cit. page 90. causes, so a group The idea of meeting with Bill Gross started with Professor Stern, then Dean of the School of Physical Sciences, and Greg Gissendanner, who was UCI’s University Advancement Officer at the School. My friend and attorney, Paul Marx, knew Bill and set up a lunch. ahead of schedule Sue and Bill Gross later gave an additional $4 million to complete the fourth floor conference center and laboratories. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:87:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 29 all-time closing high Source: www.finance.yahoo.com , daily closing prices, adjusted for splits and dividends. Figures are truncated from 1565.15 and 676.53. $48.5 trillion The Federal Reserve, as reported in the Los Angeles Times, March 12, 2010. $161 per share The successive 10 percent increases over the previous stock price compound, moving the stock from $100 to $110, then to $121, $133.10, etc., reaching $161.05 after the fifth increase. more than ten times Month-end returns from the end of 1925 to the end of August 1929 show no decline in the index from a previous peak of 10 percent, suggesting that conditions were encouraging for borrowing as stock prices increased. maintenance margin For a discussion of maintenance margin, see, for instance, Beat the Market, by Thorp, Edward O. and Kassouf, Sheen T., Random House, New York, 1967, Chapter 11. impact of speculation Wikipedia. Frontline Frontline: The Warning, October 20, 2009, pbs.org , available on DVD. gross national product What would have been produced by workers who are unemployed never gets “made up.” Social waste included deteriorating empty untended houses and the impact on society of shattered lives. Krugman pointed out Krugman, Paul, “How Did Economists Get It So Wrong?” New York Times Magazine, September 6, 2009, pp. 36–43. in his book The Quants: How a Small Band of Math Wizards Took Over Wall Street and Nearly Destroyed It, by Scott Patterson, Crown, New York, 2010. remain solvent He also said, on the same point, “In the long run we are all dead.” anyone, later told Patterson, Scott, Wall Street Journal. Krugman points out Krugman, Paul, “Good and Boring,” New York Times Op-Ed, February 1, 2010. 411 to 1 Hiltzik, Michael, “Echoes of Bell in CEO Salaries,” Los Angeles Times, page 31, October 3, 2010. Wall Street Journal (as reprinted in The Orange County Register, May 11, 2014, Business, page 3) says a study by the Economic Policy Institute found that for the 350 companies with the largest sales, CEOs received 18 times the pay of their workers in 1965 but were compensated 201 times as much, on average, in 2012. as Moshe Adler Adler, Moshe, “Overthrowing the Overpaid,” Los Angeles Times Opinion, page A15, January 4, 2010. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:88:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"CHAPTER 30 Garrett Hardin Hardin, Garrett, “The Tragedy of the Commons,” Science, Vol. 162, No. 3859 December 13, 1968, pp. 1243–48. protects my neighbors Vaccination is a positive externality as it protects others from contracting a disease from the recipient. collection of insights Poor Charlie’s Almanack: The Wit and Wisdom of Charles T. Munger, Foreward by Warren Buffett, edited by Peter Kaufman. Expanded third edition, 2008. much of their income Read the evolving discussion of the taxation of so-called carried interest in Wikipedia and elsewhere on the Internet. The 2012 Republican presidential candidate, Mitt Romney, was a substantial beneficiary. of national income The top 1 percent have about a third of taxable income, the next 9 percent have another third, and the bottom 90 percent have the remaining third. 20 percent or so To get a simplistic feel for the numbers, government received $3.25 trillion in income in 2015 and GNP was $18 trillion. If we exempt $2 trillion for the very poorest citizens and tax the remaining $16 trillion of GNP at a single rate, the result is 3.25/16 or 20 percent. a year by 2015 According to the UC Admissions Office, this is mitigated by the fact that more than half of undergraduates pay no tuition and more than two-thirds receive grants and scholarships averaging $16,300. of them Chinese My grandson Edward, while a high school senior, was taking an advanced mathematics class (partial differential equations) at UCI. Thirty-one of the thirty-six students were Chinese. As they were unaware that Edward speaks Mandarin fluently, he overheard many candid conversations. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:89:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"APPENDIX A dollar has changed For an insightful discussion of why the inflation index from the 1970s may be much too low as a result of a series of government revisions in the method of calculation, and the consequences to investors and consumers, see “Fooling With Inflation” by Bill Gross (June 2008) at www.pimco.com . For updated Consumer Price Index numbers and for month-by-month values, go to www.bls.gov/cpi or do the usual Google search. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:90:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"APPENDIX C $2 billion in cash Los Angeles Times, Thursday, September 7, 2000, page C5. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:91:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"BIBLIOGRAPHY Bass, Thomas A. The Endaemonic Pie. New York: Houghton Mifflin, 1985. Black, Fischer, and Myron Scholes. “The Pricing of Options and Corporate Liabilities.” Journal of Political Economy 81.3 (1973): 637–54. Blackwood, Kevin, and Larry Barker. Legends of Blackjack: True Stories of Players Who Crushed the Casinos. Kindle EBook, April 5, 2009. Bogle, John C. Bogle on Mutual Funds: New Perspectives for the Intelligent Investor. Burr Ridge, IL: Irwin, 1994. Edwardothorp.com . View articles written by the author. Feller, William. An Introduction to Probability Theory and Its Applications, Volume I. New York: Wiley, 1957, 1968. Fox, Justin. The Myth of the Rational Market: A History of Risk, Reward, and Delusion on Wall Street. New York: Harper Business, 2009. Griffin, Peter A. Introduction. The Theory of Blackjack: The Compleat Card Counter’s Guide to the Casino Game of 21. Las Vegas, NV: Huntington, 1995, 1999. Gross, William H. Bill Gross on Investing. New York: Wiley, 1998. Ibbotson SBBI 2014 Classic Yearbook: Market Results for Stocks, Bonds, Bills, and Inflation, 1926–2013. Chicago, IL: Morningstar, 2014. Kelly, J. L. “A New Interpretation of Information Rate.” Bell System Technical Journal 35.4 (1956): 917–26. Lack, Simon. The Hedge Fund Mirage: The Illusion of Big Money and Why It’s Too Good to Be True. Hoboken, NJ: Wiley, 2012. MacLean, L. C., Edward O. Thorp, and W. T. Ziemba. The Kelly Capital Growth Investment Criterion: Theory and Practice. World Scientific, 2011. Malkiel, Burton Gordon. A Random Walk Down Wall Street: The Time-tested Strategy for Successful Investing. New York: W. W. Norton, 2007. Mezrich, Ben. Bringing Down the House: The Inside Story of Six MIT Students Who Took Vegas for Millions. New York: Free Press, 2002. Munchkin, Richard W. Gambling Wizards: Conversations with the World’s Greatest Gamblers. Las Vegas, NV: Huntington, 2002. Munger, Charles T., and Peter D. Kaufman. Poor Charlie’s Almanack: The Wit and Wisdom of Charles T. Munger. Foreword by Warren Buffett. Virginia Beach, VA: Donning Publishers, 2008. O’Neil, Paul. “The Professor Who Breaks the Bank.” Life 27 March 1964: 80–91. Patterson, Scott. The Quants: How a New Breed of Math Whizzes Conquered Wall Street and Nearly Destroyed It. New York: Crown, 2010. Poundstone, William. Fortune’s Formula: The Untold Story of the Scientific Betting System That Beat the Casinos and Wall Street. New York: Hill and Wang, 2005. Schroeder, Alice. The Snowball: Warren Buffett and the Business of Life. New York: Bantam, 2008. Segel, Joel. Recountings: Conversations with MIT Mathematicians. Wellesley, MA: A K Peters/CRC Press, 2009. Siegel, Jeremy J. Stocks for the Long Run: The Definitive Guide to Financial Market Returns and Long-Term Investment Strategies. New York: McGraw-Hill, 2008. Taleb, Nassim Nicholas. The Black Swan: The Impact of the Highly Improbable. New York: Random House, 2007. ———. Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets. New York: Random House, 2005. Thorp, Edward O., and Sheen T. Kassouf. Beat the Market: A Scientific Stock Market System. New York: Random House, 1967. Available at www.edwardothorp.com . ———. Beat the Dealer: A Winning Strategy for the Game of Twenty-One. New York: Random House, 1962, Rev. 1966, Rev. 2016. ———. “A Favorable Strategy for Twenty-One.” Proceedings of the National Academy of Sciences 47.1 (1961): 110–12. ———. “Optimal Gambling Systems for Favorable Games.” Review of the International Statistical Institute 37.3 (1969): 273–93. ———. “The Kelly Criterion in Blackjack, Sports Betting, and the Stock Market.” Handbook of Asset and Liability Management, Volume 1, Zenios, Stavros Andrea, and W. T. Ziemba, Editors. Amsterdam: Elsevier, 2006. Wong, Stanford. Professional Blackjack. La Jolla, CA: Pi Yee, 1994. ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:92:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"BY EDWARD O. THORP A Man for All Markets Beat the Dealer Beat the Market Elementary Probability The Mathematics of Gambling ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:93:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["probability"],"content":"ABOUT THE AUTHOR EDWARD O. THORP is the author of the bestseller Beat the Dealer: A Winning Strategy for the Game of Twenty-One (Random House 1962, 1966). It presented the first scientific system ever devised for a major casino gambling game and revolutionized the game of blackjack. His book Beat the Market (Random House 1967, co-authored with Sheen T. Kassouf) helped start the derivatives revolution that transformed world securities markets. Based on his work, he launched the first market-neutral hedge fund in 1969. Dr. Thorp, with Claude Shannon, also invented the first wearable computer in 1961 to win at roulette. He has also written Elementary Probability (1966), The Mathematics of Gambling (1984), and numerous mathematical papers on probability, game theory, and functional analysis. He completed undergraduate and graduate work at UCLA, receiving the BA and MA in physics, and the PhD in mathematics in 1958. He has taught at UCLA, MIT, and New Mexico State University, and was Professor of Mathematics and Finance at the University of California, Irvine. edwardothorp.com amanforallmarkets.com ","date":"2022-09-06","objectID":"/b01_a_man_for_all_markets/:94:0","tags":["probability"],"title":"A Man for All Markets","uri":"/b01_a_man_for_all_markets/"},{"categories":["self-learning"],"content":"Preface PEOPLE GENERALLY ARE going about learning in the wrong ways. Empirical research into how we learn and remember shows that much of what we take for gospel about how to learn turns out to be largely wasted effort. Even college and medical students—whose main job is learning—rely on study techniques that are far from optimal. At the same time, this field of research, which goes back 125 years but has been particularly fruitful in recent years, has yielded a body of insights that constitute a growing science of learning: highly effective, evidence-based strategies to replace less effective but widely accepted practices that are rooted in theory, lore, and intuition. But there’s a catch: the most effective learning strategies are not intuitive. Two of us, Henry Roediger and Mark McDaniel, are cognitive scientists who have dedicated our careers to the study of learning and memory. Peter Brown is a storyteller. We have teamed up to explain how learning and memory work, and we do this less by reciting the research than by telling stories of people who have found their way to mastery of complex knowledge and skills. Through these examples we illuminate the principles of learning that the research shows are highly effective. This book arose in part from a collaboration among eleven cognitive psychologists. In 2002, the James S. McDonnell Foundation of St. Louis, Missouri, in an effort to better bridge the gap between basic knowledge on learning in cognitive psychology and its application in education, awarded a research grant “Applying Cognitive Psychology to Enhance Educational Practice” to Roediger and McDaniel and nine others, with Roediger as the principal investigator. The team collaborated for ten years on research to translate cognitive science into educational science, and in many respects this book is a direct result of that work. The researchers and many of their studies are cited in the book, the notes, and our acknowledgments. Roediger’s and McDaniel’s work is also supported by several other funders, and McDaniel is the co-director of Washington University’s Center for Integrative Research in Learning and Memory. Most books deal with topics serially—they cover one topic, move on to the next, and so on. We follow this strategy in the sense that each chapter addresses new topics, but we also apply two of the primary learning principles in the book: spaced repetition of key ideas, and the interleaving of different but related topics. If learners spread out their study of a topic, returning to it periodically over time, they remember it better. Similarly, if they interleave the study of different topics, they learn each better than if they had studied them one at a time in sequence. Thus we unabashedly cover key ideas more than once, repeating principles in different contexts across the book. The reader will remember them better and use them more effectively as a result. This is a book about what people can do for themselves right now in order to learn better and remember longer. The responsibility for learning rests with every individual. Teachers and coaches, too, can be more effective right now by helping students understand these principles and by designing them into the learning experience. This is not a book about how education policy or the school system ought to be reformed. Clearly, though, there are policy implications. For example, college professors at the forefront of applying these strategies in the classroom have experimented with their potential for narrowing the achievement gap in the sciences, and the results of those studies are eye opening. We write for students and teachers, of course, and for all readers for whom effective learning is a high priority: for trainers in business, industry, and the military; for leaders of professional associations offering in-service training to their members; and for coaches. We also write for lifelong learners nearing middle age or older who want to hone their skills","date":"2022-09-05","objectID":"/b10_make_it_stick/:1:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"1 Learning Is Misunderstood EARLY IN HIS CAREER as a pilot, Matt Brown was flying a twin-engine Cessna northeast out of Harlingen, Texas, when he noticed a drop in oil pressure in his right engine. He was alone, flying through the night at eleven thousand feet, making a hotshot freight run to a plant in Kentucky that had shut down its manufacturing line awaiting product parts for assembly. He reduced altitude and kept an eye on the oil gauge, hoping to fly as far as a planned fuel stop in Louisiana, where he could service the plane, but the pressure kept falling. Matt has been messing around with piston engines since he was old enough to hold a wrench, and he knew he had a problem. He ran a mental checklist, figuring his options. If he let the oil pressure get too low he risked the engine’s seizing up. How much further could he fly before shutting it down? What would happen when he did? He’d lose lift on the right side, but could he stay aloft? He reviewed the tolerances he’d memorized for the Cessna 401. Loaded, the best you could do on one engine was slow your descent. But he had a light load, and he’d burned through most of his fuel. So he shut down the ailing right engine, feathered the prop to reduce drag, increased power on the left, flew with opposite rudder, and limped another ten miles toward his intended stop. There, he made his approach in a wide left-hand turn, for the simple but critical reason that without power on his right side it was only from a left-hand turn that he still had the lift needed to level out for a touchdown. While we don’t need to understand each of the actions Matt took, he certainly needed to, and his ability to work himself out of a jam illustrates what we mean in this book when we talk about learning: we mean acquiring knowledge and skills and having them readily available from memory so you can make sense of future problems and opportunities. There are some immutable aspects of learning that we can probably all agree on: First, to be useful, learning requires memory, so what we’ve learned is still there later when we need it. Second, we need to keep learning and remembering all our lives. We can’t advance through middle school without some mastery of language arts, math, science, and social studies. Getting ahead at work takes mastery of job skills and difficult colleagues. In retirement, we pick up new interests. In our dotage, we move into simpler housing while we’re still able to adapt. If you’re good at learning, you have an advantage in life. Third, learning is an acquired skill, and the most effective strategies are often counterintuitive. ","date":"2022-09-05","objectID":"/b10_make_it_stick/:2:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Claims We Make in This Book You may not agree with the last point, but we hope to persuade you of it. Here, more or less unadorned in list form, are some of the principal claims we make in support of our argument. We set them forth more fully in the chapters that follow. Learning is deeper and more durable when it’s effortful. Learning that’s easy is like writing in sand, here today and gone tomorrow. We are poor judges of when we are learning well and when we’re not. When the going is harder and slower and it doesn’t feel productive, we are drawn to strategies that feel more fruitful, unaware that the gains from these strategies are often temporary. Rereading text and massed practice of a skill or new knowledge are by far the preferred study strategies of learners of all stripes, but they’re also among the least productive. By massed practice we mean the single-minded, rapid-fire repetition of something you’re trying to burn into memory, the “practice-practice-practice” of conventional wisdom. Cramming for exams is an example. Rereading and massed practice give rise to feelings of fluency that are taken to be signs of mastery, but for true mastery or durability these strategies are largely a waste of time. Retrieval practice—recalling facts or concepts or events from memory—is a more effective learning strategy than review by rereading. Flashcards are a simple example. Retrieval strengthens the memory and interrupts forgetting. A single, simple quiz after reading a text or hearing a lecture produces better learning and remembering than rereading the text or reviewing lecture notes. While the brain is not a muscle that gets stronger with exercise, the neural pathways that make up a body of learning do get stronger, when the memory is retrieved and the learning is practiced. Periodic practice arrests forgetting, strengthens retrieval routes, and is essential for hanging onto the knowledge you want to gain. When you space out practice at a task and get a little rusty between sessions, or you interleave the practice of two or more subjects, retrieval is harder and feels less productive, but the effort produces longer lasting learning and enables more versatile application of it in later settings. Trying to solve a problem before being taught the solution leads to better learning, even when errors are made in the attempt. The popular notion that you learn better when you receive instruction in a form consistent with your preferred learning style, for example as an auditory or visual learner, is not supported by the empirical research. People do have multiple forms of intelligence to bring to bear on learning, and you learn better when you “go wide,” drawing on all of your aptitudes and resourcefulness, than when you limit instruction or experience to the style you find most amenable. When you’re adept at extracting the underlying principles or “rules” that differentiate types of problems, you’re more successful at picking the right solutions in unfamiliar situations. This skill is better acquired through interleaved and varied practice than massed practice. For instance, interleaving practice at computing the volumes of different kinds of geometric solids makes you more skilled at picking the right solution when a later test presents a random solid. Interleaving the identification of bird types or the works of oil painters improves your ability both to learn the unifying attributes within a type and to differentiate between types, improving your skill at categorizing new specimens you encounter later. We’re all susceptible to illusions that can hijack our judgment of what we know and can do. Testing helps calibrate our judgments of what we’ve learned. A pilot who is responding to a failure of hydraulic systems in a flight simulator discovers quickly whether he’s on top of the corrective procedures or not. In virtually all areas of learning, you build better mastery when you use testing as a tool to identify and bring up your areas","date":"2022-09-05","objectID":"/b10_make_it_stick/:2:1","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Empirical Evidence versus Theory, Lore, and Intuition Much of how we structure training and schooling is based on learning theories that have been handed down to us, and these are shaped by our own sense of what works, a sensibility drawn from our personal experiences as teachers, coaches, students, and mere humans at large on the earth. How we teach and study is largely a mix of theory, lore, and intuition. But over the last forty years and more, cognitive psychologists have been working to build a body of evidence to clarify what works and to discover the strategies that get results. Cognitive psychology is the basic science of understanding how the mind works, conducting empirical research into how people perceive, remember, and think. Many others have their hands in the puzzle of learning as well. Developmental and educational psychologists are concerned with theories of human development and how they can be used to shape the tools of education—such as testing regimes, instructional organizers (for example topic outlines and schematic illustrations), and resources for special groups like those in remedial and gifted education. Neuroscientists, using new imaging techniques and other tools, are advancing our understanding of brain mechanisms that underlie learning, but we’re still a very long way from knowing what neuroscience will tell us about how to improve education. How is one to know whose advice to take on how best to go about learning? It’s wise to be skeptical. Advice is easy to find, only a few mouse-clicks away. Yet not all advice is grounded in research—far from it. Nor does all that passes as research meet the standards of science, such as having appropriate control conditions to assure that the results of an investigation are objective and generalizable. The best empirical studies are experimental in nature: the researcher develops a hypothesis and then tests it through a set of experiments that must meet rigorous criteria for design and objectivity. In the chapters that follow, we have distilled the findings of a large body of such studies that have stood up under review by the scientific community before being published in professional journals. We are collaborators in some of these studies, but not the lion’s share. Where we’re offering theory rather than scientifically validated results, we say so. To make our points we use, in addition to tested science, anecdotes from people like Matt Brown whose work requires mastery of complex knowledge and skills, stories that illustrate the underlying principles of how we learn and remember. Discussion of the research studies themselves is kept to a minimum, but you will find many of them cited in the notes at the end of the book if you care to dig further. People Misunderstand Learning It turns out that much of what we’ve been doing as teachers and students isn’t serving us well, but some comparatively simple changes could make a big difference. People commonly believe that if you expose yourself to something enough times—say, a textbook passage or a set of terms from an eighth grade biology class—you can burn it into memory. Not so. Many teachers believe that if they can make learning easier and faster, the learning will be better. Much research turns this belief on its head: when learning is harder, it’s stronger and lasts longer. It’s widely believed by teachers, trainers, and coaches that the most effective way to master a new skill is to give it dogged, single-minded focus, practicing over and over until you’ve got it down. Our faith in this runs deep, because most of us see fast gains during the learning phase of massed practice. What’s apparent from the research is that gains achieved during massed practice are transitory and melt away quickly. The finding that rereading textbooks is often labor in vain ought to send a chill up the spines of educators and learners, because it’s the number one study strategy of most people—including more than 80 percent of coll","date":"2022-09-05","objectID":"/b10_make_it_stick/:2:2","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Knowledge: Not Sufficient, but Necessary Albert Einstein declared “creativity is more important than knowledge,” and the sentiment appears to be widely shared by college students, if their choice in t-shirt proclamations is any indication. And why wouldn’t they seize on the sentiment? It embodies an obvious and profound truth, for without creativity where would our scientific, social, or economic breakthroughs come from? Besides which, accumulating knowledge can feel like a grind, while creativity sounds like a lot more fun. But of course the dichotomy is false. You wouldn’t want to see that t-shirt on your neurosurgeon or on the captain who’s flying your plane across the Pacific. But the sentiment has gained some currency as a reaction to standardized testing, fearing that this kind of testing leads to an emphasis on memorization at the expense of high-level skills. Notwithstanding the pitfalls of standardized testing, what we really ought to ask is how to do better at building knowledge and creativity, for without knowledge you don’t have the foundation for the higher-level skills of analysis, synthesis, and creative problem solving. As the psychologist Robert Sternberg and two colleagues put it, “one cannot apply what one knows in a practical manner if one does not know anything to apply.”12 Mastery in any field, from cooking to chess to brain surgery, is a gradual accretion of knowledge, conceptual understanding, judgment, and skill. These are the fruits of variety in the practice of new skills, and of striving, reflection, and mental rehearsal. Memorizing facts is like stocking a construction site with the supplies to put up a house. Building the house requires not only knowledge of countless different fittings and materials but conceptual understanding, too, of aspects like the load-bearing properties of a header or roof truss system, or the principles of energy transfer and conservation that will keep the house warm but the roof deck cold so the owner doesn’t call six months later with ice dam problems. Mastery requires both the possession of ready knowledge and the conceptual understanding of how to use it. When Matt Brown had to decide whether or not to kill his right engine he was problem solving, and he needed to know from memory the procedures for flying with a dead engine and the tolerances of his plane in order to predict whether he would fall out of the air or be unable to straighten up for landing. The would-be neurosurgeon in her first year of med school has to memorize the whole nervous system, the whole skeletal system, the whole muscular system, the humeral system. If she can’t, she’s not going to be a neurosurgeon. Her success will depend on diligence, of course, but also on finding study strategies that will enable her to learn the sheer volume of material required in the limited hours available. ","date":"2022-09-05","objectID":"/b10_make_it_stick/:3:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Testing: Dipstick versus Learning Tool There are few surer ways to raise the hackles of many students and educators than talking about testing. The growing focus over recent years on standardized assessment, in particular, has turned testing into a lightning rod for frustration over how to achieve the country’s education goals. Online forums and news articles are besieged by readers who charge that emphasis on testing favors memorization at the expense of a larger grasp of context or creative ability; that testing creates extra stress for students and gives a false measure of ability; and so on. But if we stop thinking of testing as a dipstick to measure learning—if we think of it as practicing retrieval of learning from memory rather than “testing,” we open ourselves to another possibility: the use of testing as a tool for learning. One of the most striking research findings is the power of active retrieval—testing—to strengthen memory, and that the more effortful the retrieval, the stronger the benefit. Think flight simulator versus PowerPoint lecture. Think quiz versus rereading. The act of retrieving learning from memory has two profound benefits. One, it tells you what you know and don’t know, and therefore where to focus further study to improve the areas where you’re weak. Two, recalling what you have learned causes your brain to reconsolidate the memory, which strengthens its connections to what you already know and makes it easier for you to recall in the future. In effect, retrieval—testing—interrupts forgetting. Consider an eighth grade science class. For the class in question, at a middle school in Columbia, Illinois, researchers arranged for part of the material covered during the course to be the subject of low-stakes quizzing (with feedback) at three points in the semester. Another part of the material was never quizzed but was studied three times in review. In a test a month later, which material was better recalled? The students averaged A- on the material that was quizzed and C+ on the material that was not quizzed but reviewed.13 In Matt Brown’s case, even after ten years piloting the same business jet, his employer reinforces his mastery every six months in a battery of tests and flight simulations that require him to retrieve the information and maneuvers that are essential to stay in control of his plane. As Matt points out, you hardly ever have an emergency, so if you don’t practice what to do, there’s no way to keep it fresh. Both of these cases—the research in the classroom and the experience of Matt Brown in updating his knowledge—point to the critical role of retrieval practice in keeping our knowledge accessible to us when we need it. The power of active retrieval is the topic of Chapter 2 .14 ","date":"2022-09-05","objectID":"/b10_make_it_stick/:4:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"The Takeaway For the most part, we are going about learning in the wrong ways, and we are giving poor advice to those who are coming up behind us. A great deal of what we think we know about how to learn is taken on faith and based on intuition but does not hold up under empirical research. Persistent illusions of knowing lead us to labor at unproductive strategies; as recounted in Chapter 3 , this is true even of people who have participated in empirical studies and seen the evidence for themselves, firsthand. Illusions are potent persuaders. One of the best habits a learner can instill in herself is regular self-quizzing to recalibrate her understanding of what she does and does not know. Second Lieutenant Kiley Hunkler, a 2013 graduate of West Point and winner of a Rhodes Scholarship, whom we write about in Chapter 8 , uses the phrase “shooting an azimuth” to describe how she takes practice tests to help refocus her studying. In overland navigation, shooting an azimuth means climbing to a height, sighting an object on the horizon in the direction you’re traveling, and adjusting your compass heading to make sure you’re still gaining on your objective as you beat through the forest below. The good news is that we now know of simple and practical strategies that anybody can use, at any point in life, to learn better and remember longer: various forms of retrieval practice, such as low-stakes quizzing and self-testing, spacing out practice, interleaving the practice of different but related topics or skills, trying to solve a problem before being taught the solution, distilling the underlying principles or rules that differentiate types of problems, and so on. In the chapters that follow we describe these in depth. And because learning is an iterative process that requires that you revisit what you have learned earlier and continually update it and connect it with new knowledge, we circle through these topics several times along the way. At the end, in Chapter 8 , we pull it all together with specific tips and examples for putting these tools to work. 2 To Learn, Retrieve MIKE EBERSOLD GOT CALLED into a hospital emergency room one afternoon late in 2011 to examine a Wisconsin deer hunter who’d been found lying unconscious in a cornfield. The man had blood at the back of his head, and the men who’d found and brought him in supposed he’d maybe stumbled and cracked his skull on something. Ebersold is a neurosurgeon. The injury had brain protruding, and he recognized it as a gunshot wound. The hunter regained consciousness in the ER, but when asked how he’d hurt himself, he had no idea. Recounting the incident later, Ebersold said, “Somebody from some distance away must have fired what appeared to be a 12-gauge shotgun, which arced over God only knows what distance, hit this guy in the back of his head, fractured his skull, and lodged into the brain about an inch. It must have been pretty much spent, or it would have gone deeper.”1 Ebersold is tall, slender, and counts among his forebears the Dakota chiefs named Wapasha and the French fur traders named Rocque who populated this part of the Mississippi River Valley where the Mayo brothers would later found their famous clinic. Ebersold’s formal training included four years of college, four years of medical school, and seven years of neurosurgery training—building a foundation of knowledge and skills that has been broadened and deepened through continuing medical education classes, consultations with his colleagues, and his practice at the Mayo Clinic and elsewhere. He carries himself with a midwestern modesty that belies a career that counts a long list of high-profile patients who have sought out his services. When President Ronald Reagan needed treatment for injuries after a fall from his horse, Ebersold participated in the surgery and postsurgical care. When Sheikh Zayed bin Sultan Al Nahyan, president of the United Arab Emirates, needed delicate spinal repair, he and what seemed","date":"2022-09-05","objectID":"/b10_make_it_stick/:5:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Reflection Is a Form of Practice What inferences can we draw from this story about how we learn and remember? In neurosurgery (and, arguably, in all aspects of life from the moment you leave the womb), there’s an essential kind of learning that comes from reflection on personal experience. Ebersold described it this way: A lot of times something would come up in surgery that I had difficulty with, and then I’d go home that night thinking about what happened and what could I do, for example, to improve the way a suturing went. How can I take a bigger bite with my needle, or a smaller bite, or should the stitches be closer together? What if I modified it this way or that way? Then the next day back, I’d try that and see if it worked better. Or even if it wasn’t the next day, at least I’ve thought through this, and in so doing I’ve not only revisited things that I learned from lectures or from watching others performing surgery but also I’ve complemented that by adding something of my own to it that I missed during the teaching process. Reflection can involve several cognitive activities that lead to stronger learning: retrieving knowledge and earlier training from memory, connecting these to new experiences, and visualizing and mentally rehearsing what you might do differently next time. It was this kind of reflection that originally had led Ebersold to try a new technique for repairing the sinus vein at the back of the head, a technique he practiced in his mind and in the operating room until it became the kind of reflexive maneuver you can depend on when your patient is spouting blood at two hundred cubic centimeters a minute. To make sure the new learning is available when it’s needed, Ebersold points out, “you memorize the list of things that you need to worry about in a given situation: steps A, B, C, and D,” and you drill on them. Then there comes a time when you get into a tight situation and it’s no longer a matter of thinking through the steps, it’s a matter of reflexively taking the correct action. “Unless you keep recalling this maneuver, it will not become a reflex. Like a race car driver in a tight situation or a quarterback dodging a tackle, you’ve got to act out of reflex before you’ve even had time to think. Recalling it over and over, practicing it over and over. That’s just so important.” ","date":"2022-09-05","objectID":"/b10_make_it_stick/:6:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"The Testing Effect A child stringing cranberries on a thread goes to hang them on the tree, only to find they’ve slipped off the other end. Without the knot, there’s no making a string. Without the knot there’s no necklace, there’s no beaded purse, no magnificent tapestry. Retrieval ties the knot for memory. Repeated retrieval snugs it up and adds a loop to make it fast. Since as far back as 1885, psychologists have been plotting “forgetting curves” that illustrate just how fast our cranberries slip off the string. In very short order we lose something like 70 percent of what we’ve just heard or read. After that, forgetting begins to slow, and the last 30 percent or so falls away more slowly, but the lesson is clear: a central challenge to improving the way we learn is finding a way to interrupt the process of forgetting.2 The power of retrieval as a learning tool is known among psychologists as the testing effect. In its most common form, testing is used to measure learning and assign grades in school, but we’ve long known that the act of retrieving knowledge from memory has the effect of making that knowledge easier to call up again in the future. In his essay on memory, Aristotle wrote: “exercise in repeatedly recalling a thing strengthens the memory.” Francis Bacon wrote about this phenomenon, as did the psychologist William James. Today, we know from empirical research that practicing retrieval makes learning stick far better than reexposure to the original material does. This is the testing effect, also known as the retrieval-practice effect.3 To be most effective, retrieval must be repeated again and again, in spaced out sessions so that the recall, rather than becoming a mindless recitation, requires some cognitive effort. Repeated recall appears to help memory consolidate into a cohesive representation in the brain and to strengthen and multiply the neural routes by which the knowledge can later be retrieved. In recent decades, studies have confirmed what Mike Ebersold and every seasoned quarterback, jet pilot, and teenaged texter knows from experience—that repeated retrieval can so embed knowledge and skills that they become reflexive: the brain acts before the mind has time to think. Yet despite what research and personal experience tell us about the power of testing as a learning tool, teachers and students in traditional educational settings rarely use it as such, and the technique remains little understood or utilized by teachers or students as a learning tool in traditional educational settings. Far from it. In 2010 the New York Times reported on a scientific study that showed that students who read a passage of text and then took a test asking them to recall what they had read retained an astonishing 50 percent more of the information a week later than students who had not been tested. This would seem like good news, but here’s how it was greeted in many online comments: “Once again, another author confuses learning with recalling information.” “I personally would like to avoid as many tests as possible, especially with my grade on the line. Trying to learn in a stressful environment is no way to help retain information.” “Nobody should care whether memorization is enhanced by practice testing or not. Our children cannot do much of anything anymore.”4 Forget memorization, many commenters argued; education should be about high-order skills. Hmmm. If memorization is irrelevant to complex problem solving, don’t tell your neurosurgeon. The frustration many people feel toward standardized, “dipstick” tests given for the sole purpose of measuring learning is understandable, but it steers us away from appreciating one of the most potent learning tools available to us. Pitting the learning of basic knowledge against the development of creative thinking is a false choice. Both need to be cultivated. The stronger one’s knowledge about the subject at hand, the more nuanced one’s creativity can be in addressing a new probl","date":"2022-09-05","objectID":"/b10_make_it_stick/:7:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Studying the Testing Effect in the Lab The testing effect has a solid pedigree in empirical research. The first large-scale investigation was published in 1917. Children in grades 3, 5, 6, and 8 studied brief biographies from Who’s Who in America. Some of them were directed to spend varying lengths of the study time looking up from the material and silently reciting to themselves what it contained. Those who did not do so simply continued to reread the material. At the end of the period, all the children were asked to write down what they could remember. The recall test was repeated three to four hours later. All the groups who had engaged in the recitation showed better retention than those who had not done so but had merely continued to review the material. The best results were from those spending about 60 percent of the study time in recitation. A second landmark study, published in 1939, tested over three thousand sixth graders across Iowa. The kids studied six-hundred-word articles and then took tests at various times before a final test two months later. The experiment showed a couple of interesting results: the longer the first test was delayed, the greater the forgetting, and second, once a student had taken a test, the forgetting nearly stopped, and the student’s score on subsequent tests dropped very little.5 Around 1940, interest turned to the study of forgetting, and investigating the potential of testing as a form of retrieval practice and as a learning tool fell out of favor. So did the use of testing as a research tool: since testing interrupts forgetting, you can’t use it to measure forgetting because that “contaminates” the subject. Interest in the testing effect resurfaced in 1967 with the publication of a study showing that research subjects who were presented with lists of thirty-six words learned as much from repeated testing after initial exposure to the words as they did from repeated studying. These results—that testing led to as much learning as studying did—challenged the received wisdom, turned researchers’ attention back to the potential of testing as a learning tool, and stimulated a boomlet in testing research. In 1978, researchers found that massed studying (cramming) leads to higher scores on an immediate test but results in faster forgetting compared to practicing retrieval. In a second test two days after an initial test, the crammers had forgotten 50 percent of what they had been able to recall on the initial test, while those who had spent the same period practicing retrieval instead of studying had forgotten only 13 percent of the information recalled initially. A subsequent study was aimed at understanding what effect taking multiple tests would have on subjects’ long-term retention. Students heard a story that named sixty concrete objects. Those students who were tested immediately after exposure recalled 53 percent of the objects on this initial test but only 39 percent a week later. On the other hand, a group of students who learned the same material but were not tested at all until a week later recalled 28 percent. Thus, taking a single test boosted performance by 11 percent after a week. But what effect would three immediate tests have relative to one? Another group of students were tested three times after initial exposure and a week later they were able to recall 53 percent of the objects—the same as on the initial test for the group receiving one test. In effect, the group that received three tests had been “immunized” against forgetting, compared to the one-test group, and the one-test group remembered more than those who had received no test immediately following exposure. Thus, and in agreement with later research, multiple sessions of retrieval practice are generally better than one, especially if the test sessions are spaced out.6 In another study, researchers showed that simply asking a subject to fill in a word’s missing letters resulted in better memory of the word. Consid","date":"2022-09-05","objectID":"/b10_make_it_stick/:8:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Studying the Testing Effect “In the Wild” In 2005, we and our colleagues approached Roger Chamberlain, the principal of a middle school in nearby Columbia, Illinois, with a proposition. The positive effects of retrieval practice had been demonstrated many times in controlled laboratory settings but rarely in a regular classroom setting. Would the principal, teachers, kids, and parents of Columbia Middle School be willing subjects in a study to see how the testing effect would work “in the wild”? Chamberlain had concerns. If this was just about memorization, he wasn’t especially interested. His aim is to raise the school’s students to higher forms of learning—analysis, synthesis, and application, as he put it. And he was concerned about his teachers, an energetic faculty with curricula and varied instructional methods he was loath to disrupt. On the other hand, the study’s results could be instructive, and participation would bring enticements in the form of smart boards and “clickers”—automated response systems—for the classrooms of participating teachers. Money for new technology is famously tight. A sixth grade social studies teacher, Patrice Bain, was eager to give it a try. For the researchers, a chance to work in the classroom was compelling, and the school’s terms were accepted: the study would be minimally intrusive by fitting within existing curricula, lesson plans, test formats, and teaching methods. The same textbooks would be used. The only difference in the class would be the introduction of occasional short quizzes. The study would run for three semesters (a year and a half), through several chapters of the social studies textbook, covering topics such as ancient Egypt, Mesopotamia, India, and China. The project was launched in 2006. It would prove to be a good decision. For the six social studies classes a research assistant, Pooja Agarwal, designed a series of quizzes that would test students on roughly one-third of the material covered by the teacher. These quizzes were for “no stakes,” meaning that scores were not counted toward a grade. The teacher excused herself from the classroom for each quiz so as to remain unaware of which material was being tested. One quiz was given at the start of class, on material from assigned reading that hadn’t yet been discussed. A second was given at the end of class after the teacher had covered the material for the day’s lesson. And a review quiz was given twenty-four hours before each unit exam. There was concern that if students tested better in the final exam on material that had been quizzed than on material not quizzed, it could be argued that the simple act of reexposing them to the material in the quizzes was responsible for the superior learning, not the retrieval practice. To counter this possibility, some of the nonquizzed material was interspersed with the quiz material, provided as simple review statements, like “The Nile River has two major tributaries: the White Nile and the Blue Nile,” with no retrieval required. The facts were quizzed for some classes but just restudied for others. The quizzes took only a few minutes of classroom time. After the teacher stepped out of the room, Agarwal projected a series of slides onto the board at the front of the room and read them to the students. Each slide presented either a multiple choice question or a statement of fact. When the slide contained a question, students used clickers (handheld, cell-phone-like remotes) to indicate their answer choice: A, B, C, or D. When all had responded, the correct answer was revealed, so as to provide feedback and correct errors. (Although teachers were not present for these quizzes, under normal circumstances, with teachers administering quizzes, they would see immediately how well students are tracking the study material and use the results to guide further discussion or study.) Unit exams were the normal pencil-and-paper tests given by the teacher. Exams were also given at the end o","date":"2022-09-05","objectID":"/b10_make_it_stick/:9:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Exploring Nuances Andy Sobel’s example is anecdotal and likely reflects a variety of beneficial influences, not least being the cumulative learning effects that accrue like compounded interest when course material is carried forward in a regime of quizzes across an entire semester. Nonetheless, his experience squares with empirical research designed to tease apart the effects and nuances of testing. For example, in one experiment college students studied prose passages on various scientific topics like those taught in college and then either took an immediate recall test after the initial exposure or restudied the material. After a delay of two days, the students who took the initial test recalled more of the material than those who simply restudied it (68 v. 54 percent), and this advantage was sustained a week later (56 v. 42 percent). Another experiment found that after one week a study-only group showed the most forgetting of what they initially had been able to recall, forgetting 52 percent, compared to a repeated-testing group, who forgot only 10 percent.11 How does giving feedback on wrong answers to test questions affect learning? Studies show that giving feedback strengthens retention more than testing alone does, and, interestingly, some evidence shows that delaying the feedback briefly produces better long-term learning than immediate feedback. This finding is counterintuitive but is consistent with researchers’ discoveries about how we learn motor tasks, like making layups or driving a golf ball toward a distant green. In motor learning, trial and error with delayed feedback is a more awkward but effective way of acquiring a skill than trial and correction through immediate feedback; immediate feedback is like the training wheels on a bicycle: the learner quickly comes to depend on the continued presence of the correction. In the case of learning motor skills, one theory holds that when there’s immediate feedback it comes to be part of the task, so that later, in a real-world setting, its absence becomes a gap in the established pattern that disrupts performance. Another idea holds that frequent interruptions for feedback make the learning sessions too variable, preventing establishment of a stabilized pattern of performance.12 In the classroom, delayed feedback also yields better long-term learning than immediate feedback does. In the case of the students studying prose passages on science topics, some were shown the passage again even while they were asked to answer questions about it, in effect providing them with continuous feedback during the test, analogous to an open-book exam. The other group took the test without the study material at hand and only afterward were given the passage and instructed to look over their responses. Of course, the open-book group performed best on the immediate test, but those who got corrective feedback after completing the test retained the learning better on a later test. Delayed feedback on written tests may help because it gives the student practice that’s spaced out in time; as discussed in the next chapter, spacing practice improves retention.13 Are some kinds of retrieval practice more effective for long-term learning than others? Tests that require the learner to supply the answer, like an essay or short-answer test, or simply practice with flashcards, appear to be more effective than simple recognition tests like multiple choice or true/false tests. However, even multiple choice tests like those used at Columbia Middle School can yield strong benefits. While any kind of retrieval practice generally benefits learning, the implication seems to be that where more cognitive effort is required for retrieval, greater retention results. Retrieval practice has been studied extensively in recent years, and an analysis of these studies shows that even a single test in a class can produce a large improvement in final exam scores, and gains in learning continue to increase as the num","date":"2022-09-05","objectID":"/b10_make_it_stick/:10:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"The Takeaway Practice at retrieving new knowledge or skill from memory is a potent tool for learning and durable retention. This is true for anything the brain is asked to remember and call up again in the future—facts, complex concepts, problem-solving techniques, motor skills. Effortful retrieval makes for stronger learning and retention. We’re easily seduced into believing that learning is better when it’s easier, but the research shows the opposite: when the mind has to work, learning sticks better. The greater the effort to retrieve learning, provided that you succeed, the more that learning is strengthened by retrieval. After an initial test, delaying subsequent retrieval practice is more potent for reinforcing retention than immediate practice, because delayed retrieval requires more effort. Repeated retrieval not only makes memories more durable but produces knowledge that can be retrieved more readily, in more varied settings, and applied to a wider variety of problems. While cramming can produce better scores on an immediate exam, the advantage quickly fades because there is much greater forgetting after rereading than after retrieval practice. The benefits of retrieval practice are long-term. Simply including one test (retrieval practice) in a class yields a large improvement in final exam scores, and gains continue to increase as the frequency of classroom testing increases. Testing doesn’t need to be initiated by the instructor. Students can practice retrieval anywhere; no quizzes in the classroom are necessary. Think flashcards—the way second graders learn the multiplication tables can work just as well for learners at any age to quiz themselves on anatomy, mathematics, or law. Self-testing may be unappealing because it takes more effort than rereading, but as noted already, the greater the effort at retrieval, the more will be retained. Students who take practice tests have a better grasp of their progress than those who simply reread the material. Similarly, such testing enables an instructor to spot gaps and misconceptions and adapt instruction to correct them. Giving students corrective feedback after tests keeps them from incorrectly retaining material they have misunderstood and produces better learning of the correct answers. Students in classes that incorporate low-stakes quizzing come to embrace the practice. Students who are tested frequently rate their classes more favorably. What about Principal Roger Chamberlain’s initial concerns about practice quizzing at Columbia Middle School—that it might be nothing more than a glorified path to rote learning? When we asked him this question after the study was completed, he paused for a moment to gather his thoughts. “What I’ve really gained a comfort level with is this: for kids to be able to evaluate, synthesize, and apply a concept in different settings, they’re going to be much more efficient at getting there when they have the base of knowledge and the retention, so they’re not wasting time trying to go back and figure out what that word might mean or what that concept was about. It allows them to go to a higher level.” 3 Mix Up Your Practice IT MAY NOT BE INTUITIVE that retrieval practice is a more powerful learning strategy than repeated review and rereading, yet most of us take for granted the importance of testing in sports. It’s what we call “practice-practice-practice.” Well, here’s a study that may surprise you. A group of eight-year-olds practiced tossing beanbags into buckets in gym class. Half of the kids tossed into a bucket three feet away. The other half mixed it up by tossing into buckets two feet and four feet away. After twelve weeks of this they were all tested on tossing into a three-foot bucket. The kids who did the best by far were those who’d practiced on two- and four-foot buckets but never on three-foot buckets.1 Why is this? We will come back to the beanbags, but first a little insight into a widely held myth about how we learn. ","date":"2022-09-05","objectID":"/b10_make_it_stick/:11:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"The Myth of Massed Practice Most of us believe that learning is better when you go at something with single-minded purpose: the practice-practice-practice that’s supposed to burn a skill into memory. Faith in focused, repetitive practice of one thing at a time until we’ve got it nailed is pervasive among classroom teachers, athletes, corporate trainers, and students. Researchers call this kind of practice “massed,” and our faith rests in large part on the simple fact that when we do it, we can see it making a difference. Nevertheless, despite what our eyes tell us, this faith is misplaced. If learning can be defined as picking up new knowledge or skills and being able to apply them later, then how quickly you pick something up is only part of the story. Is it still there when you need to use it out in the everyday world? While practicing is vital to learning and memory, studies have shown that practice is far more effective when it’s broken into separate periods of training that are spaced out. The rapid gains produced by massed practice are often evident, but the rapid forgetting that follows is not. Practice that’s spaced out, interleaved with other learning, and varied produces better mastery, longer retention, and more versatility. But these benefits come at a price: when practice is spaced, interleaved, and varied, it requires more effort. You feel the increased effort, but not the benefits the effort produces. Learning feels slower from this kind of practice, and you don’t get the rapid improvements and affirmations you’re accustomed to seeing from massed practice. Even in studies where the participants have shown superior results from spaced learning, they don’t perceive the improvement; they believe they learned better on the material where practice was massed. Almost everywhere you look, you find examples of massed practice: summer language boot camps, colleges that offer concentration in a single subject with the promise of fast learning, continuing education seminars for professionals where training is condensed into a single weekend. Cramming for exams is a form of massed practice. It feels like a productive strategy, and it may get you through the next day’s midterm, but most of the material will be long forgotten by the time you sit down for the final. Spacing out your practice feels less productive for the very reason that some forgetting has set in and you’ve got to work harder to recall the concepts. It doesn’t feel like you’re on top of it. What you don’t sense in the moment is that this added effort is making the learning stronger.2 ","date":"2022-09-05","objectID":"/b10_make_it_stick/:12:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Spaced Practice The benefits of spacing out practice sessions are long established, but for a vivid example consider this study of thirty-eight surgical residents. They took a series of four short lessons in microsurgery: how to reattach tiny vessels. Each lesson included some instruction followed by some practice. Half the docs completed all four lessons in a single day, which is the normal in-service schedule. The others completed the same four lessons but with a week’s interval between them.3 In a test given a month after their last session, those whose lessons had been spaced a week apart outperformed their colleagues in all areas—elapsed time to complete a surgery, number of hand movements, and success at reattaching the severed, pulsating aortas of live rats. The difference in performance between the two groups was impressive. The residents who had taken all four sessions in a single day not only scored lower on all measures, but 16 percent of them damaged the rats’ vessels beyond repair and were unable to complete their surgeries. Why is spaced practice more effective than massed practice? It appears that embedding new learning in long-term memory requires a process of consolidation, in which memory traces (the brain’s representations of the new learning) are strengthened, given meaning, and connected to prior knowledge—a process that unfolds over hours and may take several days. Rapid-fire practice leans on short-term memory. Durable learning, however, requires time for mental rehearsal and the other processes of consolidation. Hence, spaced practice works better. The increased effort required to retrieve the learning after a little forgetting has the effect of retriggering consolidation, further strengthening memory. We explore some of the theories about this process in the next chapter. ","date":"2022-09-05","objectID":"/b10_make_it_stick/:13:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Interleaved Practice Interleaving the practice of two or more subjects or skills is also a more potent alternative to massed practice, and here’s a quick example of that. Two groups of college students were taught how to find the volumes of four obscure geometric solids (wedge, spheroid, spherical cone, and half cone). One group then worked a set of practice problems that were clustered by problem type (practice four problems for computing the volume of a wedge, then four problems for a spheroid, etc.). The other group worked the same practice problems, but the sequence was mixed (interleaved) rather than clustered by type of problem. Given what we’ve already presented, the results may not surprise you. During practice, the students who worked the problems in clusters (that is, massed) averaged 89 percent correct, compared to only 60 percent for those who worked the problems in a mixed sequence. But in the final test a week later, the students who had practiced solving problems clustered by type averaged only 20 percent correct, while the students whose practice was interleaved averaged 63 percent. The mixing of problem types, which boosted final test performance by a remarkable 215 percent, actually impeded performance during initial learning.4 Now, suppose you’re a trainer in a company trying to teach employees a complicated new process that involves ten procedures. The typical way of doing this is to train up in procedure 1, repeating it many times until the trainees really seem to have it down cold. Then you go to procedure 2, you do many repetitions of 2, you get that down, and so on. That appears to produce fast learning. What would interleaved practice look like? You practice procedure 1 just a few times, then switch to procedure 4, then switch to 3, then to 7, and so on. (Chapter 8 tells how Farmers Insurance trains new agents in a spiraling series of exercises that cycle back to key skillsets in a seemingly random sequence that adds layers of context and meaning at each turn.) The learning from interleaved practice feels slower than learning from massed practice. Teachers and students sense the difference. They can see that their grasp of each element is coming more slowly, and the compensating long-term advantage is not apparent to them. As a result, interleaving is unpopular and seldom used. Teachers dislike it because it feels sluggish. Students find it confusing: they’re just starting to get a handle on new material and don’t feel on top of it yet when they are forced to switch. But the research shows unequivocally that mastery and long-term retention are much better if you interleave practice than if you mass it. ","date":"2022-09-05","objectID":"/b10_make_it_stick/:14:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Varied Practice Okay, what about the beanbag study where the kids who did best had never practiced the three-foot toss that the other kids had only practiced? The beanbag study focused on mastery of motor skills, but much evidence has shown that the underlying principle applies to cognitive learning as well. The basic idea is that varied practice—like tossing your beanbags into baskets at mixed distances—improves your ability to transfer learning from one situation and apply it successfully to another. You develop a broader understanding of the relationships between different conditions and the movements required to succeed in them; you discern context better and develop a more flexible “movement vocabulary”—different movements for different situations. Whether the scope of variable training (e.g., the two- and four-foot tosses) must encompass the particular task (the three-foot toss) is subject for further study. The evidence favoring variable training has been supported by recent neuroimaging studies that suggest that different kinds of practice engage different parts of the brain. The learning of motor skills from varied practice, which is more cognitively challenging than massed practice, appears to be consolidated in an area of the brain associated with the more difficult process of learning higher-order motor skills. The learning of motor skills from massed practice, on the other hand, appears to be consolidated in a different area of the brain that is used for learning more cognitively simple and less challenging motor skills. The inference is that learning gained through the less challenging, massed form of practice is encoded in a simpler or comparatively impoverished representation than the learning gained from the varied and more challenging practice which demands more brain power and encodes the learning in a more flexible representation that can be applied more broadly.5 Among athletes, massed practice has long been the rule: take your hook shot, knock the twenty-foot putt, work your backhand return, throw the pass while rolling out: again and again and again—to get it right and train your “muscle memory.” Or so the notion holds. The benefits of variable training for motor learning have been gaining broader acceptance, albeit slowly. Consider the one-touch pass in hockey. That’s where you receive the puck and immediately pass it to a teammate who’s moving down the ice, keeping the opposition off balance and unable to put pressure on the puck carrier. Jamie Kompon, when he was assistant coach of the Los Angeles Kings, was in the habit of running team practice on one-touch passes from the same position on the rink. Even if this move is interleaved with a sequence of other moves in practice, if you only do it at the same place on the rink or in the same sequence of moves, you are only, as it were, throwing your beanbags into the three-foot bucket. Kompon is onto the difference now and has changed up his drills. Since we talked, he’s gone over to the Chicago Blackhawks. We would have said “Keep an eye on those Blackhawks” here, but as we revise to go into production, Kompon and team have already won the Stanley Cup. Perhaps no coincidence? The benefits of variable practice for cognitive as opposed to motor skills learning were shown in a recent experiment that adapted the beanbag test to verbal learning: in this case, the students solved anagrams–that is, they rearranged letters to form words (tmoce becomes comet). Some subjects practiced the same anagram over and over, whereas others practiced multiple anagrams for the word. When they were all tested on the same anagram that the former group had practiced on, the latter group performed better on it! The same benefits will apply whether you are practicing to identify tree species, differentiate the principles of case law, or master a new computer program.6 ","date":"2022-09-05","objectID":"/b10_make_it_stick/:15:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Developing Discrimination Skills Compared to massed practice, a significant advantage of interleaving and variation is that they help us learn better how to assess context and discriminate between problems, selecting and applying the correct solution from a range of possibilities. In math education, massing is embedded in the textbook: each chapter is dedicated to a particular kind of problem, which you study in class and then practice by working, say, twenty examples for homework before you move on. The next chapter has a different type of problem, and you dive into the same kind of concentrated learning and practice of that solution. On you march, chapter by chapter, through the semester. But then, on the final exam, lo and behold, the problems are all mixed up: you’re staring at each one in turn, asking yourself Which algorithm do I use? Was it in chapter 5, 6, or 7? When you have learned under conditions of massed or blocked repetition, you have had no practice on that critical sorting process. But this is the way life usually unfolds: problems and opportunities come at us unpredictably, out of sequence. For our learning to have practical value, we must be adept at discerning “What kind of problem is this?” so we can select and apply an appropriate solution. Several studies have demonstrated the improved powers of discrimination to be gained through interleaved and varied practice. One study involved learning to attribute paintings to the artists who created them, and another focused on learning to identify and classify birds. Researchers initially predicted that massed practice in identifying painters’ works (that is, studying many examples of one painter’s works before moving on to study many examples of another’s works) would best help students learn the defining characteristics of each artist’s style. Massed practice of each artist’s works, one artist at a time, would better enable students to match artworks to artists later, compared to interleaved exposure to the works of different artists. The idea was that interleaving would be too hard and confusing; students would never be able to sort out the relevant dimensions. The researchers were wrong. The commonalities among one painter’s works that the students learned through massed practice proved less useful than the differences between the works of multiple painters that the students learned through interleaving. Interleaving enabled better discrimination and produced better scores on a later test that required matching the works with their painters. The interleaving group was also better able to match painters’ names correctly to new examples of their work that the group had never viewed during the learning phase. Despite these results, the students who participated in these experiments persisted in preferring massed practice, convinced that it served them better. Even after they took the test and could have realized from their own performance that interleaving was the better strategy for learning, they clung to their belief that the concentrated viewing of paintings by one artist was better. The myths of massed practice are hard to exorcise, even when you’re experiencing the evidence yourself.7 The power of interleaving practice to improve discriminability has been reaffirmed in studies of people learning bird classification. The challenge here is more complex than it might seem. One study addressed twenty different bird families (thrashers, swallows, wrens, finches, and so on). Within each family, students were presented with a dozen species (brown thrasher, curve-billed thrasher, Bendire’s thrasher, etc.). To identify a bird’s family, you consider a wide range of traits like size, plumage, behavior, location, beak shape, iris color, and so on. A problem in bird identification is that members of a family share many traits in common but not all. For instance, many but not all thrashers have a long, slightly hooked beak. There are traits that are typical of a family ","date":"2022-09-05","objectID":"/b10_make_it_stick/:16:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Improving Complex Mastery for Medical Students The distinction between straightforward knowledge of facts and deeper learning that permits flexible use of the knowledge may be a little fuzzy, but it resonates with Douglas Larsen at Washington University Medical School in St. Louis, who says that the skills required for bird classification are similar to those required of a doctor diagnosing what’s wrong with a patient. “The reason variety is important is it helps us see more nuances in the things that we can compare against,” he says. “That comes up a lot in medicine, in the sense that every patient visit is a test. There are many layers of explicit and implicit memory involved in the ability to discriminate between symptoms and their interrelationships.” Implicit memory is your automatic retrieval of past experience in interpreting a new one. For example, the patient comes in and gives you a story. As you listen, you’re consciously thinking through your mental library to see what fits, while also unconsciously polling your past experiences to help interpret what the patient is telling you. “Then you’re left with making a judgment call,” Larsen says.9 Larsen is a pediatric neurologist seeing patients in the university clinic and hospital. He’s a busy guy: in addition to practicing medicine, he supervises the work of physicians in training, he teaches, and as time permits, he conducts research into medical education, working in collaboration with cognitive psychologists. He’s drawing on all of these roles to redesign and strengthen the school’s training curriculum in pediatric neurology. As you’d expect, the medical school employs a wide spectrum of instructional techniques. Besides classroom lectures and labs, students practice resuscitations and other procedures on high-tech mannequins in three simulation centers the school maintains. Each “patient” is hooked up to monitors, has a heartbeat, blood pressure, pupils that dilate and constrict, and the ability to listen and speak, thanks to a controller who observes and operates the mannequin from a back room. The school also makes use of “standardized patients,” actors who follow scripts and exhibit symptoms the students are required to diagnose. The center is set up like a regular medical clinic, and students must show proficiency in all aspects of a patient encounter, from bedside manner, physical exam skills, and remembering to ask the full spectrum of pertinent questions to arriving at a diagnosis and treatment plan. From studies of these teaching methods, Larsen has drawn some interesting conclusions. First—and this may seem self-evident: you do better on a test to demonstrate your competency at seeing patients in a clinic if your learning experience has involved seeing patients in a clinic. Simply reading about patients is not enough. However, on written final exams, medical students who have examined patients and those who have learned via written tests do equally well. The reason is that in a written test the student is being given considerable structure and being asked for specific information. When examining the patient, you have to come up on your own with the right mental model and the steps to follow. Having practiced these steps on patients or simulated patients improves performance relative to just reading about how to do it. In other words, the kind of retrieval practice that proves most effective is one that reflects what you’ll be doing with the knowledge later. It’s not just what you know, but how you practice what you know that determines how well the learning serves you later. As the sports adage goes, “practice like you play and you will play like you practice.” This conclusion lines up with other research into learning, and with some of the more sophisticated training practices in science and industry, including the increasingly broad use of simulators—not just for jet pilots and medical students but for cops, towboat pilots, and people in almost any field","date":"2022-09-05","objectID":"/b10_make_it_stick/:17:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"These Principles Are Broadly Applicable College football might seem an incongruous place to look for a learning model, but a conversation with Coach Vince Dooley about the University of Georgia’s practice regime provides an intriguing case. Dooley is authoritative on the subject. As head coach of Bulldogs football from 1964–1988, he piled up an astonishing 201 wins with only 77 losses and 10 tied games, winning six conference titles and a national championship. He went on to serve as the university’s athletic director, where he built one of the most impressive athletics programs in the country. We asked Coach Dooley how players go about mastering all the complexities of the game. His theories of coaching and training revolve around the weekly cycle of one Saturday game to the next. In that short period there’s a lot to learn: studying the opposition’s type of game in the classroom, discussing offensive and defensive strategies for opposing it, taking the discussion onto the playing field, breaking the strategies down to the movements of individual positions and trying them out, knitting the parts into a whole, and then repeating the moves until they run like clockwork. While all this is going on, the players must also keep their fundamental skills in top form: blocking, tackling, catching the ball, bringing the ball in, carrying the ball. Dooley believes that (1) you have to keep practicing the fundamentals from time to time, forever, so you keep them sharp, otherwise you’re cooked, but (2) you need to change it up in practice because too much repetition is boring. The position coaches work with players individually on specific skills and then on how they’re playing their positions during team practice. What else? There’s practicing the kicking game. There’s the matter of each player’s mastery of the playbook. And there are the special plays from the team’s repertoire that often make the difference between winning and losing. In Dooley’s narrative, the special plays stand as exemplars of spaced learning: they’re practiced only on Thursdays, so there’s always a week between sessions, and the plays are run in a varied sequence. With all this to be done, it’s not surprising that a critical aspect of the team’s success is a very specific daily and weekly schedule that interleaves the elements of individual and team practice. The start of every day’s practice is strictly focused on the fundamentals of each player’s position. Next, players practice in small groups, working on maneuvers involving several positions. These parts are gradually brought together and run as a team. Play is speeded up and slowed down, rehearsed mentally as well as physically. By midweek the team is running the plays in real time, full speed. “You’re coming at it fast, and you’ve got to react fast,” Dooley said. “But as you get closer to game time, you slow it down again. Now it’s a kind of rehearsal without physical contact. The play basically starts out the same each time, but then what the opponent does changes it. So you’ve got to be able to adjust to that. You start into the motion and say, ‘If they react like this, then this is what you would do.’ You practice adjustments. If you do it enough times in different situations, then you’re able to do it pretty well in whatever comes up on the field.”11 How does a player get on top of his playbook? He takes it home and goes over the plays in his mind. He may walk through them. Everything in practice can’t be physically strenuous, Dooley said, or you’d wear yourself out, “so if the play calls for you to step this way and then go the other way, you can rehearse that in your mind, maybe just lean your body as if to go that way. And then if something happens where you have to adjust, you can do that mentally. By reading the playbook, rehearsing it in your mind, maybe taking a step or two to walk through it, you simulate something happening. So that kind of rehearsal is added to what you get in the classroom and ","date":"2022-09-05","objectID":"/b10_make_it_stick/:18:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"The Takeaway Here’s a quick rundown of what we know today about massed practice and its alternatives. Scientists will continue to deepen our understanding. We harbor deep convictions that we learn better through single-minded focus and dogged repetition, and these beliefs are validated time and again by the visible improvement that comes during “practice-practice-practice.” But scientists call this heightened performance during the acquisition phase of a skill “momentary strength” and distinguish it from “underlying habit strength.” The very techniques that build habit strength, like spacing, interleaving, and variation, slow visible acquisition and fail to deliver the improvement during practice that helps to motivate and reinforce our efforts.12 Cramming, a form of massed practice, has been likened to binge-and-purge eating. A lot goes in, but most of it comes right back out in short order. The simple act of spacing out study and practice in installments and allowing time to elapse between them makes both the learning and the memory stronger, in effect building habit strength. How big an interval, you ask? The simple answer: enough so that practice doesn’t become a mindless repetition. At a minimum, enough time so that a little forgetting has set in. A little forgetting between practice sessions can be a good thing, if it leads to more effort in practice, but you do not want so much forgetting that retrieval essentially involves relearning the material. The time periods between sessions of practice let memories consolidate. Sleep seems to play a large role in memory consolidation, so practice with at least a day in between sessions is good. Something as simple as a deck of flashcards can provide an example of spacing. Between repetitions of any individual card, you work through many others. The German scientist Sebastian Leitner developed his own system for spaced practice of flashcards, known as the Leitner box. Think of it as a series of four file-card boxes. In the first are the study materials (be they musical scores, hockey moves, or Spanish vocabulary flashcards) that must be practiced frequently because you often make mistakes in them. In the second box are the cards you’re pretty good at, and that box gets practiced less often than the first, perhaps by a half. The cards in the third box are practiced less often than those in the second, and so on. If you miss a question, make mistakes in the music, flub the one-touch pass, you move it up a box so you will practice it more often. The underlying idea is simply that the better your mastery, the less frequent the practice, but if it’s important to retain, it will never disappear completely from your set of practice boxes. Beware of the familiarity trap: the feeling that you know something and no longer need to practice it. This familiarity can hurt you during self-quizzing if you take shortcuts. Doug Larsen says, “You have to be disciplined to say, ‘All right, I’m going to make myself recall all of this and if I don’t, what did I miss, how did I not know that?’ Whereas if you have an instructor-generated test or quiz, suddenly you have to do it, there’s an expectation, you can’t cheat, you can’t take mental shortcuts around it, you simply have to do that.” The nine quizzes Andy Sobel administers over the twenty-six meetings of his political economics course are a simple example of spaced retrieval practice, and of interleaving—because he rolls forward into each successive quiz questions pertaining to work from the beginning of the semester. Interleaving two or more subjects during practice also provides a form of spacing. Interleaving can also help you develop your ability to discriminate later between different kinds of problems and select the right tool from your growing toolkit of solutions. In interleaving, you don’t move from a complete practice set of one topic to go to another. You switch before each practice is complete. A friend of ours describes his own experi","date":"2022-09-05","objectID":"/b10_make_it_stick/:19:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"How Learning Occurs To help you understand how difficulty can be desirable, we’ll briefly describe here how learning occurs. Encoding Let’s imagine you’re Mia, standing in a gravel pit watching a jump instructor explain and demonstrate the parachute landing fall. The brain converts your perceptions into chemical and electrical changes that form a mental representation of the patterns you’ve observed. This process of converting sensory perceptions into meaningful representations in the brain is still not perfectly understood. We call the process encoding, and we call the new representations within the brain memory traces. Think of notes jotted or sketched on a scratchpad, our short-term memory. Much of how we run our day-to-day lives is guided by the ephemera that clutter our short-term memory and are, fortunately, soon forgotten—how to jigger the broken latch on the locker you used when you suited up at the gym today; remembering to stop for an oil change after your workout. But the experiences and learning that we want to salt away for the future must be made stronger and more durable—in Mia’s case, the distinctive moves that will enable her to hit the ground without breaking an ankle, or worse.3 Consolidation The process of strengthening these mental representations for long-term memory is called consolidation. New learning is labile: its meaning is not fully formed and therefore is easily altered. In consolidation, the brain reorganizes and stabilizes the memory traces. This may occur over several hours or longer and involves deep processing of the new material, during which scientists believe that the brain replays or rehearses the learning, giving it meaning, filling in blank spots, and making connections to past experiences and to other knowledge already stored in long-term memory. Prior knowledge is a prerequisite for making sense of new learning, and forming those connections is an important task of consolidation. Mia’s considerable athletic skills, physical self-awareness, and prior experience represent a large body of knowledge to which the elements of a successful PLF would find many connections. As we’ve noted, sleep seems to help memory consolidation, but in any case, consolidation and transition of learning to long-term storage occurs over a period of time. An apt analogy for how the brain consolidates new learning may be the experience of composing an essay. The first draft is rangy, imprecise. You discover what you want to say by trying to write it. After a couple of revisions you have sharpened the piece and cut away some of the extraneous points. You put it aside to let it ferment. When you pick it up again a day or two later, what you want to say has become clearer in your mind. Perhaps you now perceive that there are three main points you are making. You connect them to examples and supporting information familiar to your audience. You rearrange and draw together the elements of your argument to make it more effective and elegant. Similarly, the process of learning something often starts out feeling disorganized and unwieldy; the most important aspects are not always salient. Consolidation helps organize and solidify learning, and, notably, so does retrieval after a lapse of some time, because the act of retrieving a memory from long-term storage can both strengthen the memory traces and at the same time make them modifiable again, enabling them, for example, to connect to more recent learning. This process is called reconsolidation. This is how retrieval practice modifies and strengthens learning. Suppose that on day 2 of jump school, you’re put on the spot to execute your parachute landing fall and you struggle to recall the correct posture and compose yourself—feet and knees together, knees slightly bent, eyes on the horizon—but in the reflex to break your fall you throw your arm out, forgetting to pull your elbows tight to your sides. You could have broken the arm or dislocated your shoulder if this wer","date":"2022-09-05","objectID":"/b10_make_it_stick/:20:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Extending Learning: Updating Retrieval Cues There’s virtually no limit to how much learning we can remember as long as we relate it to what we already know. In fact, because new learning depends on prior learning, the more we learn, the more possible connections we create for further learning. Our retrieval capacity, though, is severely limited. Most of what we’ve learned is not accessible to us at any given moment. This limitation on retrieval is helpful to us: if every memory were always readily to hand, you would have a hard time sorting through the sheer volume of material to put your finger on the knowledge you need at the moment: where did I put my hat, how do I sync my electronic devices, what goes into a perfect brandy Manhattan? Knowledge is more durable if it’s deeply entrenched, meaning that you have firmly and thoroughly comprehended a concept, it has practical importance or keen emotional weight in your life, and it is connected with other knowledge that you hold in memory. How readily you can recall knowledge from your internal archives is determined by context, by recent use, and by the number and vividness of cues that you have linked to the knowledge and can call on to help bring it forth.5 Here’s the tricky part. As you go through life, you often need to forget cues associated with older, competing memories so as to associate them successfully with new ones. To learn Italian in middle age, you may have to forget your high school French, because every time you think “to be” and hope to come up with the Italian essere, up pops etre, despite your most earnest intentions. Traveling in England, you have to suppress your cues to drive on the right side of the road so you can establish reliable cues to stay on the left. Knowledge that is well entrenched, like real fluency in French or years of experience driving on the right side of the road, is easily relearned later, after a period of disuse or after being interrupted by competition for retrieval cues. It’s not the knowledge itself that has been forgotten, but the cues that enable you to find and retrieve it. The cues for the new learning, driving on the left, displace those for the old, driving on the right (if we are lucky). The paradox is that some forgetting is often essential for new learning.6 When you change from a PC to a Mac, or from one Windows platform to another, you have to do enormous forgetting in order to learn the architecture of the new system and become adept at manipulating it so readily that your attention can focus on doing your work and not on working the machine. Jump school training provides another example: After their military service, many paratroopers take an interest in smoke jumping. Smokejumpers use different airplanes, different equipment, and different jump protocols. Having trained at the army’s jump school is cited as a distinct disadvantage for smoke jumping, because you have to unlearn one set of procedures that you have practiced to the point of reflex and replace them with another. Even in cases where both bodies of learning seem so similar to the uninitiated—jumping out of an airplane with a parachute on your back—you may have to forget the cues to a complex body of learning that you possess if you are to acquire a new one. We know this problem of reassigning cues to memory from our own lives, even on the simplest levels. When our friend Jack first takes up with Joan, we sometimes call the couple “Jack and Jill,” as the cue “Jack and” pulls up the old nursery rhyme that’s so thoroughly embedded in memory. About the time we have “Jack and” reliably cuing “Joan,” alas, Joan throws him over, and he takes up with Jenny. Good grief! Half of the time that we mean to say Jack and Jenny we catch ourselves saying Jack and Joan. It would have been easier had Jack picked up with Katie, so that the trailing K sound in his name handed us off to the initiating K in hers, but no such luck. Alliteration can be a handy cue, or a subversive ","date":"2022-09-05","objectID":"/b10_make_it_stick/:21:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Easier Isn’t Better Psychologists have uncovered a curious inverse relationship between the ease of retrieval practice and the power of that practice to entrench learning: the easier knowledge or a skill is for you to retrieve, the less your retrieval practice will benefit your retention of it. Conversely, the more effort you have to expend to retrieve knowledge or skill, the more the practice of retrieval will entrench it. Not long ago the California Polytechnic State University baseball team, in San Luis Obispo, became involved in an interesting experiment in improving their batting skills. They were all highly experienced players, adept at making solid contact with the ball, but they agreed to take extra batting practice twice a week, following two different practice regimens, to see which type of practice produced better results. Hitting a baseball is one of the hardest skills in sports. It takes less than half a second for a ball to reach home plate. In this instant, the batter must execute a complex combination of perceptual, cognitive, and motor skills: determining the type of pitch, anticipating how the ball will move, and aiming and timing the swing to arrive at the same place and moment as the ball. This chain of perceptions and responses must be so deeply entrenched as to become automatic, because the ball is in the catcher’s mitt long before you can even begin to think your way through how to connect with it. Part of the Cal Poly team practiced in the standard way. They practiced hitting forty-five pitches, evenly divided into three sets. Each set consisted of one type of pitch thrown fifteen times. For example, the first set would be fifteen fastballs, the second set fifteen curveballs, and the third set fifteen changeups. This was a form of massed practice. For each set of 15 pitches, as the batter saw more of that type, he got gratifyingly better at anticipating the balls, timing his swings, and connecting. Learning seemed easy. The rest of the team were given a more difficult practice regimen: the three types of pitches were randomly interspersed across the block of forty-five throws. For each pitch, the batter had no idea which type to expect. At the end of the forty-five swings, he was still struggling somewhat to connect with the ball. These players didn’t seem to be developing the proficiency their teammates were showing. The interleaving and spacing of different pitches made learning more arduous and feel slower. The extra practice sessions continued twice weekly for six weeks. At the end, when the players’ hitting was assessed, the two groups had clearly benefited differently from the extra practice, and not in the way the players expected. Those who had practiced on the randomly interspersed pitches now displayed markedly better hitting relative to those who had practiced on one type of pitch thrown over and over. These results are all the more interesting when you consider that these players were already skilled hitters prior to the extra training. Bringing their performance to an even higher level is good evidence of a training regimen’s effectiveness. Here again we see the two familiar lessons. First, that some difficulties that require more effort and slow down apparent gains—like spacing, interleaving, and mixing up practice—will feel less productive at the time but will more than compensate for that by making the learning stronger, precise, and enduring. Second, that our judgments of what learning strategies work best for us are often mistaken, colored by illusions of mastery. When the baseball players at Cal Poly practiced curveball after curveball over fifteen pitches, it became easier for them to remember the perceptions and responses they needed for that type of pitch: the look of the ball’s spin, how the ball changed direction, how fast its direction changed, and how long to wait for it to curve. Performance improved, but the growing ease of recalling these perceptions and responses led to lit","date":"2022-09-05","objectID":"/b10_make_it_stick/:22:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"How Effort Helps Reconsolidating Memory Effortful recall of learning, as happens in spaced practice, requires that you “reload” or reconstruct the components of the skill or material anew from long-term memory rather than mindlessly repeating them from short-term memory.10 During this focused, effortful recall, the learning is made pliable again: the most salient aspects of it become clearer, and the consequent reconsolidation helps to reinforce meaning, strengthen connections to prior knowledge, bolster the cues and retrieval routes for recalling it later, and weaken competing routes. Spaced practice, which allows some forgetting to occur between sessions, strengthens both the learning and the cues and routes for fast retrieval when that learning is needed again, as when the pitcher tries to surprise the batter with a curveball after pitching several fastballs. The more effort that is required to recall a memory or to execute a skill, provided that the effort succeeds, the more the act of recalling or executing benefits the learning.11 Massed practice gives us the warm sensation of mastery because we’re looping information through short-term memory without having to reconstruct the learning from long-term memory. But just as with rereading as a study strategy, the fluency gained through massed practice is transitory, and our sense of mastery is illusory. It’s the effortful process of reconstructing the knowledge that triggers reconsolidation and deeper learning. Creating Mental Models With enough effortful practice, a complex set of interrelated ideas or a sequence of motor skills fuse into a meaningful whole, forming a mental model somewhat akin to a “brain app”. Learning to drive a car involves a host of simultaneous actions that require all of our powers of concentration and dexterity while we are learning them. But over time, these combinations of cognition and motor skills—for example, the perceptions and maneuvers required to parallel park or manipulate a stick shift—become ingrained as sets of mental models associated with driving. Mental models are forms of deeply entrenched and highly efficient skills (seeing and unloading on a curveball) or knowledge structures (a memorized sequence of chess moves) that, like habits, can be adapted and applied in varied circumstances. Expert performance is built through thousands of hours of practice in your area of expertise, in varying conditions, through which you accumulate a vast library of such mental models that enables you to correctly discern a given situation and instantaneously select and execute the correct response. Broadening Mastery Retrieval practice that you perform at different times and in different contexts and that interleaves different learning material has the benefit of linking new associations to the material. This process builds interconnected networks of knowledge that bolster and support mastery of your field. It also multiplies the cues for retrieving the knowledge, increasing the versatility with which you can later apply it. Think of an experienced chef who has internalized the complex knowledge of how flavors and textures interact; how ingredients change form under heat; the differing effects to be achieved with a saucepan versus a wok, with copper versus cast iron. Think of the fly fisher who can sense the presence of trout and accurately judge the likely species, make the right choice of dry fly, nymph, or streamer, judge the wind, and know how and where to drop that fly to make the trout rise. Think of the kid on the BMX bike who can perform bunnyhops, tail whips, 180s, and wall taps off the features of an unfamiliar streetscape. Interleaving and variation mix up the contexts of practice and the other skills and knowledge with which the new material is associated. This makes our mental models more versatile, enabling us to apply our learning to a broader range of situations. Fostering Conceptual Learning How do humans learn concepts, for example th","date":"2022-09-05","objectID":"/b10_make_it_stick/:23:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Other Learning Strategies That Incorporate Desirable Difficulties We usually think of interference as a detriment to learning, but certain kinds of interference can produce learning benefits, and the positive effects are sometimes surprising. Would you rather read an article that has normal type or type that’s somewhat out of focus? Almost surely you would opt for the former. Yet when text on a page is slightly out of focus or presented in a font that is a little difficult to decipher, people recall the content better. Should the outline of a lecture follow the precise flow of a chapter in a textbook, or is it better if the lecture mismatches the text in some ways? It turns out that when the outline of a lecture proceeds in a different order from the textbook passage, the effort to discern the main ideas and reconcile the discrepancy produces better recall of the content. In another surprise, when letters are omitted from words in a text, requiring the reader to supply them, reading is slowed, and retention improves. In all of these examples, the change from normal presentation introduces a difficulty—disruption of fluency—that makes the learner work harder to construct an interpretation that makes sense. The added effort increases comprehension and learning. (Of course, learning will not improve if the difficulty completely obscures the meaning or cannot be overcome.)13 The act of trying to answer a question or attempting to solve a problem rather than being presented with the information or the solution is known as generation. Even if you’re being quizzed on material you’re familiar with, the simple act of filling in a blank has the effect of strengthening your memory of the material and your ability to recall it later. In testing, being required to supply an answer rather than select from multiple choice options often provides stronger learning benefits. Having to write a short essay makes them stronger still. Overcoming these mild difficulties is a form of active learning, where students engage in higher-order thinking tasks rather than passively receiving knowledge conferred by others. When you’re asked to supply an answer or a solution to something that’s new to you, the power of generation to aid learning is even more evident. One explanation for this effect is the idea that as you cast about for a solution, retrieving related knowledge from memory, you strengthen the route to a gap in your learning even before the answer is provided to fill it and, when you do fill it, connections are made to the related material that is fresh in your mind from the effort. For example, if you’re from Vermont and are asked to name the capital of Texas you might start ruminating on possibilities: Dallas? San Antonio? El Paso? Houston? Even if you’re unsure, thinking about alternatives before you hit on (or are given) the correct answer will help you. (Austin, of course.) Wrestling with the question, you rack your brain for something that might give you an idea. You may get curious, even stumped or frustrated and acutely aware of the hole in your knowledge that needs filling. When you’re then shown the solution, a light goes on. Unsuccessful attempts to solve a problem encourage deep processing of the answer when it is later supplied, creating fertile ground for its encoding, in a way that simply reading the answer cannot. It’s better to solve a problem than to memorize a solution. It’s better to attempt a solution and supply the incorrect answer than not to make the attempt.14 The act of taking a few minutes to review what has been learned from an experience (or in a recent class) and asking yourself questions is known as reflection. After a lecture or reading assignment, for example, you might ask yourself: What are the key ideas? What are some examples? How do these relate to what I already know? Following an experience where you are practicing new knowledge or skills, you might ask: What went well? What could have gone better? What mi","date":"2022-09-05","objectID":"/b10_make_it_stick/:24:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Failure and the Myth of Errorless Learning In the 1950s and 1960s, the psychologist B. F. Skinner advocated the adoption of “errorless learning” methods in education in the belief that errors by learners are counterproductive and result from faulty instruction. The theory of errorless learning gave rise to instructional techniques in which learners were spoonfed new material in small bites and immediately quizzed on them while they still remained on the tongue, so to speak, fresh in short-term memory and easily spit out onto the test form. There was virtually no chance of making an error. Since those days we’ve come to understand that retrieval from short-term memory is an ineffective learning strategy and that errors are an integral part of striving to increase one’s mastery over new material. Yet in our Western culture, where achievement is seen as an indicator of ability, many learners view errors as failure and do what they can to avoid committing them. The aversion to failure may be reinforced by instructors who labor under the belief that when learners are allowed to make errors it’s the errors that they will learn.16 This is a misguided impulse. When learners commit errors and are given corrective feedback, the errors are not learned. Even strategies that are highly likely to result in errors, like asking someone to try to solve a problem before being shown how to do it, produce stronger learning and retention of the correct information than more passive learning strategies, provided there is corrective feedback. Moreover, people who are taught that learning is a struggle that often involves making errors will go on to exhibit a greater propensity to tackle tough challenges and will tend to see mistakes not as failures but as lessons and turning points along the path to mastery. To see the truth of this, look no further than the kid down the hall who is deeply absorbed in working his avatar up through the levels of an action game on his Xbox video console. A fear of failure can poison learning by creating aversions to the kinds of experimentation and risk taking that characterize striving, or by diminishing performance under pressure, as in a test setting. In the latter instance, students who have a high fear of making errors when taking tests may actually do worse on the test because of their anxiety. Why? It seems that a significant portion of their working memory capacity is expended to monitor their performance (How am I doing? Am I making mistakes?), leaving less working memory capacity available to solve the problems posed by the test. “Working memory” refers to the amount of information you can hold in mind while working through a problem, especially in the face of distraction. Everyone’s working memory is severely limited, some more than others, and larger working memory capacities correlate with higher IQs. To explore this theory about how fear of failure reduces test performance, sixth graders in France were given very difficult anagram problems that none of them could solve. After struggling unsuccessfully with the problems, half of the kids received a ten-minute lesson in which they were taught that difficulty is a crucial part of learning, errors are natural and to be expected, and practice helps, just as in learning to ride a bicycle. The other kids were simply asked how they had gone about trying to solve the anagrams. Then both groups were given a difficult test whose results provided a measure of working memory. The kids who had been taught that errors are a natural part of learning showed significantly better use of working memory than did the others. These children did not expend their working memory capacity in agonizing over the difficulty of the task. The theory was further tested in variations of the original study. The results support the finding that difficulty can create feelings of incompetence that engender anxiety, which in turn disrupts learning, and that “students do better when given room","date":"2022-09-05","objectID":"/b10_make_it_stick/:25:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"An Example of Generative Learning As we said earlier, the process of trying to solve a problem without the benefit of having been taught how is called generative learning, meaning that the learner is generating the answer rather than recalling it. Generation is another name for old-fashioned trial and error. We’re all familiar with the stories of skinny kids in Silicon Valley garages messing around with computers and coming out billionaires. We would like to serve up a different kind of example here: Minnesota’s Bonnie Blodgett. Bonnie is a writer and a self-taught ornamental gardener in a constant argument with a voice in her head that keeps nattering about all the ways her latest whim is sure to go haywire and embarrass her. While she is a woman of strong aesthetic sensibilities, she is also one of epic doubts. Her “learning style” might be called leap-before-you-look-because-if-you-look-first-you-probably-won’t-like-what-you-see. Her garden writing appears under the name “The Blundering Gardener.” This moniker is a way of telling her voices of doubt to take a hike, because whatever the consequences of the next whim, she’s already rolling up her sleeves. “Blundering means that you get going on your project before you have figured out how to do it in the proper way, before you know what you’re getting into. For me, the risk of knowing what you’re getting into is that it becomes an overwhelming obstacle to getting started.”18 Bonnie’s success shows how struggling with a problem makes for strong learning, and how a sustained commitment to advancing in a particular field of endeavor through trial-and-error effort leads to complex mastery and greater knowledge of the interrelationships of things. When we spoke, she had just traveled to southern Minnesota to meet with a group of farmers who wanted her gardening insights on a gamut of issues ranging from layout and design to pest control and irrigation. In the years since she first sank her spade, Bonnie’s garden writing has won national recognition and found a devoted following far and wide through many outlets, and her garden has become a destination for other gardeners. She came to ornamental gardening about the time she found herself eyeballing middle age. She had no training, just a burning desire to get her hands dirty making beautiful spaces on the corner lot of the home she shares with her husband in a historic neighborhood of St. Paul. “The experience of creating beauty calms me down,” she says, but it’s strictly a discovery process. She has always been a writer, and some years after having launched herself into the garden, she began publishing the Garden Letter, a quarterly for northern gardeners in which she chronicles her exploits, mishaps, lessons, and successes. She writes the same way that she gardens, with boldness and self-effacing humor, passing along the entertaining snafus and unexpected insights that are the fruits of experience. In calling herself the Blundering Gardener, she is giving herself and us, her readers, permission to make mistakes and get on with it. Note that in writing about her experiences, Bonnie is engaging two potent learning processes beyond the act of gardening itself. She is retrieving the details and the story of what she has discovered—say, about an experiment in grafting two species of fruit trees—and then she is elaborating by explaining the experience to her readers, connecting the outcome to what she already knows about the subject or has learned as a result. Her leap-taking impulses have taken her through vast swaths of the plant kingdom, of course, and deeply into the Latin nomenclature and the classic horticultural literature. These impulses have also drawn her into the aesthetics of space and structure and the mechanics thereof: building stone walls; digging and wiring water features; putting a cupola on the garage; building paths, stairs, and gates; ripping out a Gothic picket fence and reusing the wood to create something more o","date":"2022-09-05","objectID":"/b10_make_it_stick/:26:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Undesirable Difficulties Elizabeth and Robert Bjork, who coined the phrase “desirable difficulties,” write that difficulties are desirable because “they trigger encoding and retrieval processes that support learning, comprehension, and remembering. If, however, the learner does not have the background knowledge or skills to respond to them successfully, they become undesirable difficulties.”19 Cognitive scientists know from empirical studies that testing, spacing, interleaving, variation, generation, and certain kinds of contextual interference lead to stronger learning and retention. Beyond that, we have an intuitive sense of what kinds of difficulties are undesirable but, for lack of the needed research, we cannot yet be definitive. Clearly, impediments that you cannot overcome are not desirable. Outlining a lesson in a sequence different from the one in the textbook is not a desirable difficulty for learners who lack the reading skills or language fluency required to hold a train of thought long enough to reconcile the discrepancy. If your textbook is written in Lithuanian and you don’t know the language, this hardly represents a desirable difficulty. To be desirable, a difficulty must be something learners can overcome through increased effort. Intuitively it makes sense that difficulties that don’t strengthen the skills you will need, or the kinds of challenges you are likely to encounter in the real-world application of your learning, are not desirable. Having somebody whisper in your ear while you read the news may be essential training for a TV anchor. Being heckled by role-playing protestors while honing your campaign speech may help train up a politician. But neither of these difficulties is likely to be helpful for Rotary Club presidents or aspiring YouTube bloggers who want to improve their stage presence. A cub towboat pilot on the Mississippi might be required in training to push a string of high-riding empty barges into a lock against a strong side wind. A baseball player might practice hitting with a weight on his bat to strengthen his swing. You might teach a football player some of the principles of ballet for learning balance and movement, but you probably would not teach him the techniques for an effective golf drive or backhand tennis serve. Is there an overarching rule that determines the kinds of impediments that make learning stronger? Time and further research may yield an answer. But the kinds of difficulties we’ve just described, whose desirability is well documented, offer a large and diverse toolkit already at hand. ","date":"2022-09-05","objectID":"/b10_make_it_stick/:27:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"The Takeaway Learning is at least a three-step process: initial encoding of information is held in short-term working memory before being consolidated into a cohesive representation of knowledge in long-term memory. Consolidation reorganizes and stabilizes memory traces, gives them meaning, and makes connections to past experiences and to other knowledge already stored in long-term memory. Retrieval updates learning and enables you to apply it when you need it. Learning always builds on a store of prior knowledge. We interpret and remember events by building connections to what we already know. Long-term memory capacity is virtually limitless: the more you know, the more possible connections you have for adding new knowledge. Because of the vast capacity of long-term memory, having the ability to locate and recall what you know when you need it is key; your facility for calling up what you know depends on the repeated use of the information (to keep retrieval routes strong) and on your establishing powerful retrieval cues that can reactivate the memories. Periodic retrieval of learning helps strengthen connections to the memory and the cues for recalling it, while also weakening routes to competing memories. Retrieval practice that’s easy does little to strengthen learning; the more difficult the practice, the greater the benefit. When you recall learning from short-term memory, as in rapid-fire practice, little mental effort is required, and little long-term benefit accrues. But when you recall it after some time has elapsed and your grasp of it has become a little rusty, you have to make an effort to reconstruct it. This effortful retrieval both strengthens the memory but also makes the learning pliable again, leading to its reconsolidation. Reconsolidation helps update your memories with new information and connect them to more recent learning. Repeated effortful recall or practice helps integrate learning into mental models, in which a set of interrelated ideas or a sequence of motor skills are fused into a meaningful whole that can be adapted and applied in later settings. Examples are the perceptions and manipulations involved in driving a car or in knocking a curveball out of the ballpark. When practice conditions are varied or retrieval is interleaved with the practice of other material, we increase our abilities of discrimination and induction and the versatility with which we can apply the learning in new settings at a later date. Interleaving and variation build new connections, expanding and more firmly entrenching knowledge in memory and increasing the number of cues for retrieval. Trying to come up with an answer rather than having it presented to you, or trying to solve a problem before being shown the solution, leads to better learning and longer retention of the correct answer or solution, even when your attempted response is wrong, so long as corrective feedback is provided. 5 Avoid Illusions of Knowing AT THE ROOT of our effectiveness is our ability to grasp the world around us and to take the measure of our own performance. We’re constantly making judgments about what we know and don’t know and whether we’re capable of handling a task or solving a problem. As we work at something, we keep an eye on ourselves, adjusting our thinking or actions as we progress. Monitoring your own thinking is what psychologists call metacognition (meta is Greek for “about”). Learning to be accurate self-observers helps us to stay out of blind alleys, make good decisions, and reflect on how we might do better next time. An important part of this skill is being sensitive to the ways we can delude ourselves. One problem with poor judgment is that we usually don’t know when we’ve got it. Another problem is the sheer scope of the ways our judgment can be led astray.1 In this chapter we discuss perceptual illusions, cognitive biases, and distortions of memory that commonly mislead people. Then we suggest techniques for keeping your ","date":"2022-09-05","objectID":"/b10_make_it_stick/:28:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Two Systems of Knowing In his book Thinking, Fast and Slow, Daniel Kahneman describes our two analytic systems. What he calls System 1 (or the automatic system) is unconscious, intuitive, and immediate. It draws on our senses and memories to size up a situation in the blink of an eye. It’s the running back dodging tackles in his dash for the end zone. It’s the Minneapolis cop, walking up to a driver he’s pulled over on a chilly day, taking evasive action even before he’s fully aware that his eye has seen a bead of sweat run down the driver’s temple. System 2 (the controlled system) is our slower process of conscious analysis and reasoning. It’s the part of thinking that considers choices, makes decisions, and exerts self-control. We also use it to train System 1 to recognize and respond to particular situations that demand reflexive action. The running back is using System 2 when he walks through the moves in his playbook. The cop is using it when he practices taking a gun from a shooter. The neurosurgeon is using it when he rehearses his repair of the torn sinus. System 1 is automatic and deeply influential, but it is susceptible to illusion, and you depend on System 2 to help you manage yourself: by checking your impulses, planning ahead, identifying choices, thinking through their implications, and staying in charge of your actions. When a guy in a restaurant walks past a mother with an infant and the infant cries out “Dada!” that’s System 1. When the blushing mother says, “No, dear, that’s not Dada, that’s a man,” she is acting as a surrogate System 2, helping the infant refine her System 1. System 1 is powerful because it draws on our accumulated years of experience and our deep emotions. System 1 gives us the survival reflex in moments of danger, and the astonishing deftness earned through thousands of hours of deliberate practice in a chosen field of expertise. In the interplay between Systems 1 and 2—the topic of Malcolm Gladwell’s book *Blink—*your instantaneous ability to size up a situation plays against your capacity for skepticism and thoughtful analysis. Of course, when System 1’s conclusions arise out of misperception or illusion, they can steer you into trouble. Learning when to trust your intuition and when to question it is a big part of how you improve your competence in the world at large and in any field where you want to be expert. It’s not just the dullards who fall victim. We all do, to varying degrees. Pilots, for example, are susceptible to a host of perceptual illusions. They are trained to beware of them and to use their instruments to know that they’re getting things right. A frightening example with a happy ending is China Airlines Flight 006 on a winter day in 1985. The Boeing 747 was 41,000 feet above the Pacific, almost ten hours into its eleven-hour flight from Taipei to LA, when engine number 4 lost power. The plane began to lose airspeed. Rather than taking manual control and descending below 30,000 feet to restart the engine, as prescribed in the flight book, the crew held at 41,000 with the autopilot engaged and attempted a restart. Meanwhile, loss of the outboard engine gave the plane asymmetrical thrust. The autopilot tried to correct for this and keep the plane level, but as the plane continued to slow it also began to roll to the right. The captain was aware of the deceleration, but not the extent to which the plane had entered a right bank; his System 1 clue would have been his vestibular reflex—how the inner ear senses balance and spatial orientation—but because of the plane’s trajectory, he had the sensation of flying level. His System 2 clues would have been a glimpse at the horizon and his instruments. Correct procedure called for applying left rudder to help raise the right wing, but his System 2 focus was on the airspeed indicator and on the efforts of the first officer and engineer to restart the engine. As its bank increased, the plane descended through 37,000 feet into high c","date":"2022-09-05","objectID":"/b10_make_it_stick/:29:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Illusions and Memory Distortions The filmmaker Errol Morris, in a series of articles on illusion in the New York Times, quotes the social psychologist David Dunning on humans’ penchant for “motivated reasoning,” or, as Dunning put it, the “sheer genius people have at convincing themselves of congenial conclusions while denying the truth of inconvenient ones.”4 (The British prime minister Benjamin Disraeli once said of a political opponent that his conscience was not his guide but his accomplice.) There are many ways that our System 1 and System 2 judgments can be led astray: perceptual illusions like those experienced by pilots, faulty narrative, distortions of memory, failure to recognize when a new kind of problem requires a new kind of solution, and a variety of cognitive biases to which we’re prone. We describe a number of these hazards here, and then we offer measures you can take, akin to scanning the cockpit instruments, to help keep your thinking aligned with reality. Our understanding of the world is shaped by a hunger for narrative that rises out of our discomfort with ambiguity and arbitrary events. When surprising things happen, we search for an explanation. The urge to resolve ambiguity can be surprisingly potent, even when the subject is inconsequential. In a study where participants thought they were being measured for reading comprehension and their ability to solve anagrams, they were exposed to the distraction of a background phone conversation. Some heard only one side of a conversation, and others heard both sides. The participants, not knowing that the distraction itself was the subject of the study, tried to ignore what they were hearing so as to stay focused on the reading and anagram solutions. The results showed that overhearing one side of a conversation proved more distracting than overhearing both sides, and the content of those partial conversations was better recalled later by the unintentional eavesdroppers. Why was this? Presumably, those overhearing half a conversation were strongly compelled to try to infer the missing half in a way that made for a complete narrative. As the authors point out, the study may help explain why we find one-sided cell phone conversations in public spaces so intrusive, but it also reveals the ineluctable way we are drawn to imbue the events around us with rational explanations. The discomfort with ambiguity and arbitrariness is equally powerful, or more so, in our need for a rational understanding of our own lives. We strive to fit the events of our lives into a cohesive story that accounts for our circumstances, the things that befall us, and the choices we make. Each of us has a different narrative that has many threads woven into it from our shared culture and experience of being human, as well as many distinct threads that explain the singular events of one’s personal past. All these experiences influence what comes to mind in a current situation and the narrative through which you make sense of it: Why nobody in my family attended college until me. Why my father never made a fortune in business. Why I’d never want to work in a corporation, or, maybe, Why I would never want to work for myself. We gravitate to the narratives that best explain our emotions. In this way, narrative and memory become one. The memories we organize meaningfully become those that are better remembered. Narrative provides not only meaning but also a mental framework for imbuing future experiences and information with meaning, in effect shaping new memories to fit our established constructs of the world and ourselves. No reader, when asked to account for the choices made under pressure by a novel’s protagonist, can keep her own life experience from shading her explanation of what must have been going on in the character’s interior world. The success of a magician or politician, like that of a novelist, relies on the seductive powers of narrative and on the audience’s willing suspension of","date":"2022-09-05","objectID":"/b10_make_it_stick/:30:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Mental Models As we develop mastery in the various areas of our lives, we tend to bundle together the incremental steps that are required to solve different kinds of problems. To use an analogy from a previous chapter, you could think of them as something like smart-phone apps in the brain. We call them mental models. Two examples in police work are the choreography of the routine traffic stop and the moves to take a weapon from an assailant at close quarters. Each of these maneuvers involves a set of perceptions and actions that cops can adapt with little conscious thought in response to context and situation. For a barista, a mental model would be the steps and ingredients to produce a perfect sixteen-ounce decaf frappuccino. For the receptionist at urgent care, it’s triage and registration. The better you know something, the more difficult it becomes to teach it. So says physicist and educator Eric Mazur of Harvard. Why? As you get more expert in complex areas, your models in those areas grow more complex, and the component steps that compose them fade into the background of memory (the curse of knowledge). A physicist, for example, will create a mental library of the principles of physics she can use to solve the various kinds of problems she encounters in her work: Newton’s laws of motion, for example, or the laws of conservation of momentum. She will tend to sort problems based on their underlying principles, whereas a novice will group them by similarity of surface features, like the apparatus being manipulated in the problem (pulley, inclined plane, etc.). One day, when she goes to teach an intro physics class, she explains how a particular problem calls for something from Newtonian mechanics, forgetting that her students have yet to master the underlying steps she has long ago bundled into one unified mental model. This presumption by the professor that her students will readily follow something complex that appears fundamental in her own mind is a metacognitive error, a misjudgment of the matchup between what she knows and what her students know. Mazur says that the person who knows best what a student is struggling with in assimilating new concepts is not the professor, it’s another student.15 This problem is illustrated through a very simple experiment in which one person plays a common tune inside her head and taps the rhythm with her knuckles and another person hearing the rhythmic taps must guess the tune. Each tune comes from a fixed set of twenty-five, so the statistical chance of guessing it is 4 percent. Tellingly, the participants who have the tune in mind estimate that the other person will guess correctly 50 percent of the time, but in fact the listeners guess correctly only 2.5 percent of the time, no better than chance.16 Like Coach Dooley’s football players memorizing their playbooks, we all build mental libraries of myriad useful solutions that we can call on at will to help us work our way from one Saturday game to the next. But we can be tripped by these models, too, when we fail to recognize a new problem that appears to be a familiar one is actually something quite different and we pull out a solution to address it that doesn’t work or makes things worse. The failure to recognize when your solution doesn’t fit the problem is another form of faulty self-observation that can lead you into trouble. Mike Ebersold, the neurosurgeon, was called into the operating room one day to help a surgical resident who, in the midst of removing a brain tumor, was losing the patient. The usual model for cutting out a tumor calls for taking your time, working carefully around the growth, getting a clean margin, saving the surrounding nerves. But when the growth is in the brain, and if you get bleeding behind it, pressure on the brain can turn fatal. Instead of slow-and-careful, you need just the opposite, cutting the growth out very quickly so the blood can drain, and then working to repair the bleeding. “Initially y","date":"2022-09-05","objectID":"/b10_make_it_stick/:31:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Unskilled and Unaware of It Incompetent people lack the skills to improve because they are unable to distinguish between incompetence and competence. This phenomenon, of particular interest for metacognition, has been named the Dunning-Kruger effect after the psychologists David Dunning and Justin Kruger. Their research showed that incompetent people overestimate their own competence and, failing to sense a mismatch between their performance and what is desirable, see no need to try to improve. (The title of their initial paper on the topic was “Unskilled and Unaware of It.”) Dunning and Kruger have also shown that incompetent people can be taught to raise their competence by learning the skills to judge their own performance more accurately, in short, to make their metacognition more accurate. In one series of studies that demonstrate this finding, they gave students a test of logic and asked them to rate their own performance. In the first experiment the results confirmed expectations that the least competent students were the most out of touch with their performance: students who scored at the twelfth percentile on average believed that their general logical reasoning ability fell at the sixty-eighth percentile. In a second experiment, after taking an initial test and rating their own performance, the students were shown the other students’ answers and then their own answers and asked to reestimate the number of test questions they had answered correctly. The students whose performance was in the bottom quartile failed to judge their own performance more accurately after seeing the more competent choices of their peers and in fact tended to raise their already inflated estimates of their own ability. A third experiment explored whether poor performers could learn to improve their judgment. The students were given ten problems in logical reasoning and after the test were asked to rate their logical reasoning skills and test performance. Once again, the students in the bottom quartile grossly overestimated their performance. Next, half the students received ten minutes of training in logic (how to test the accuracy of a syllogism); the other half of the students were given an unrelated task. All the students were then asked to estimate again how well they had performed on the test. Now the students in the bottom quartile who had received the training were much more accurate estimators of the number of questions they got right and of how they performed compared to the other students. Those in the bottom quartile who didn’t receive the training held to their mistaken conviction that they had performed well. How is it that incompetent people fail to learn through experience that they are unskilled? Dunning and Kruger offer several theories. One is that people seldom receive negative feedback about their skills and abilities from others in everyday life, because people don’t like to deliver the bad news. Even if people get negative feedback, they must come to an accurate understanding of why the failure occurred. For success everything must go right, but by contrast, failure can be attributed to any number of external causes: it’s easy to blame the tool for what the hand cannot do. Finally, Dunning and Kruger suggest that some people are just not astute at reading how other people are performing and are therefore less able to spot competence when they see it, making them less able to make comparative judgments of their own performance. These effects are more likely to occur in some contexts and with some skills than with others. In some domains, the revelation of one’s incompetence can be brutally frank. The authors can all remember from their childhoods when a teacher would appoint two boys to pick other kids for softball teams. The good players are picked first, the worst last. You learn your peers’ judgments of your softball abilities in a very public manner, so it would be hard for the last-picked player to think “I must be r","date":"2022-09-05","objectID":"/b10_make_it_stick/:32:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Tools and Habits for Calibrating Your Judgment Most important is to make frequent use of testing and retrieval practice to verify what you really do know versus what you think you know. Frequent low-stakes quizzes in class help the instructor verify that students are in fact learning as well as they appear to be and reveal the areas where extra attention is needed. Doing cumulative quizzing, as Andy Sobel does in his political economics course, is especially powerful for consolidating learning and knitting the concepts from one stage of a course into new material encountered later. As a learner, you can use any number of practice techniques to self-test your mastery, from answering flashcards to explaining key concepts in your own words, and to peer instruction (see below). Don’t make the mistake of dropping material from your testing regime once you’ve gotten it correct a couple of times. If it’s important, it needs to be practiced, and practiced again. And don’t put stock in momentary gains that result from massed practice. Space your testing, vary your practice, keep the long view. Peer instruction, a learning model developed by Eric Mazur, incorporates many of the foregoing principles. The material to be covered in class is assigned for reading beforehand. In class, the lecture is interspersed with quick tests that present students with a conceptual question and give them a minute or two to grapple with it; they then try, in small groups, to reach a consensus on the correct answer. In Mazur’s experience, this process engages the students in the underlying concepts of the lecture material; reveals students’ problems in reaching understanding; and provides opportunities for them to explain their understanding, receive feedback, and assess their learning compared to other students. Likewise, the process serves as a gauge for the instructor of how well the students are assimilating the material and in what areas more or less work is needed. Mazur tries to pair students who initially had different answers to a question so that they can see another point of view and try to convince one another of who is right. For two more examples of this technique, see the profiles of the professors Mary Pat Wenderoth and Michael D. Matthews in Chapter 8 .20 Pay attention to the cues you’re using to judge what you have learned. Whether something feels familiar or fluent is not always a reliable indicator of learning. Neither is your level of ease in retrieving a fact or a phrase on a quiz shortly after encountering it in a lecture or text. (Ease of retrieval after a delay, however, is a good indicator of learning.) Far better is to create a mental model of the material that integrates the various ideas across a text, connects them to what you already know, and enables you to draw inferences. How ably you can explain a text is an excellent cue for judging comprehension, because you must recall the salient points from memory, put them into your own words, and explain why they are significant—how they relate to the larger subject. Instructors should give corrective feedback, and learners should seek it. In his interview with Errol Morris, the psychologist David Dunning argues that the path to self-insight leads through other people. “So it really depends on what sort of feedback you are getting. Is the world telling you good things? Is the world rewarding you in a way that you would expect a competent person to be rewarded? If you watch other people, you often find there are different ways to do things; there are better ways to do things. ‘I’m not as good as I thought I was, but I have something to work on.’ ” Think of the kids lining up to join the softball team—would you be picked?21 In many fields, the practice of peer review serves as an external gauge, providing feedback on one’s performance. Most medical practice groups have morbidity/mortality conferences, and if a doctor has a bad patient outcome, it will be presented there. The other doc","date":"2022-09-05","objectID":"/b10_make_it_stick/:33:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Active Learning from the Get-Go Part of the secret to Bruce is his sense, from the earliest age, of being the one in charge of Bruce. When he was two his mother, Doris, told him he couldn’t cross the street because a car might hit him. Every day, Bruce crossed the street, and every day Doris gave him a spanking. “He was born aggressive,” Doris told friends. At eight he bought a ball of string at a garage sale for a dime, cut it up, and sold the pieces for a nickel each. At ten he got a paper route. At eleven he added caddying. At twelve he stuffed his pocket with $30 in savings, sneaked out of his bedroom window before dawn with an empty suitcase, and hitchhiked 255 miles to Aberdeen, South Dakota. He stocked up on Black Cats, cherry bombs, and roman candles, illegal in Minnesota, and hitched home before supper. Over the next week, Doris couldn’t figure out why all the paperboys were dropping by the house for a few minutes and leaving. Bruce had struck gold, but the paper route supervisor found out and tipped off Bruce Senior. The father told the son if he ever did it again he’d get the licking of his life. Bruce repeated the buying trip the following summer and got the promised licking. “It was worth it,” he says.2 He was thirteen, and he had learned a lesson about high demand and short supply. The way Bruce figured, rich people were probably no smarter than he was, they just had knowledge he lacked. Looking at how he went after the knowledge he sought will illustrate some of the learning differences that matter. One, of course, is taking charge of your own education, a habit with Bruce from age two that he has exhibited through the years with remarkable persistence. There are other signal behaviors. As he throws himself into one scheme after another, he draws lessons that improve his focus and judgment. He knits what he learns into mental models of investing, which he then uses to size up more complex opportunities and find his way through the weeds, plucking the telling details from masses of irrelevant information to reach the payoff at the end. These behaviors are what psychologists call “rule learning” and “structure building.” People who as a matter of habit extract underlying principles or rules from new experiences are more successful learners than those who take their experiences at face value, failing to infer lessons that can be applied later in similar situations. Likewise, people who single out salient concepts from the less important information they encounter in new material and who link these key ideas into a mental structure are more successful learners than those who cannot separate wheat from chaff and understand how the wheat is made into flour. When he was barely a teenager, Bruce saw a flyer advertising wooded lots on a lake in central Minnesota. Advised that no one ever lost money on real estate, he bought one. Over four subsequent summers, with occasional help from his dad, he built a house on it, confronting each step in the process one at a time, figuring it out for himself or finding someone to show him how. To dig the basement, he borrowed a trailer and hooked it up to his ’49 Hudson. He paid 50 cents for every load his friends excavated, shovel by shovel, and then charged the owner of a nearby lot that needed fill a dollar for it. He learned how to lay block from a friend whose father was in the cement business and then laid himself a foundation. He learned how to frame the walls from the salesman at the lumber yard. He plumbed the house and wired it the same way, a wide-eyed kid asking around how you do that sort of thing. “The electrical inspector disapproved it,” Bruce recalls. “At the time, I figured it was because they wanted a union guy to do it, so I popped for a union guy to come up from the Cities and redo all my wiring. Looking back, I’m sure what I had done was totally dangerous.” He was nineteen and a university student the summer he traded the house for the down payment on a fourplex ","date":"2022-09-05","objectID":"/b10_make_it_stick/:34:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Successful Intelligence Intelligence is a learning difference that we do know matters, but what exactly is it? Every human society has a concept that corresponds to the idea of intelligence in our culture. The problem of how to define and measure intelligence in a way that accounts for people’s intellectual horsepower and provides a fair indicator of their potential has been with us for over a hundred years, with psychologists trying to measure this construct since early in the twentieth century. Psychologists today generally accept that individuals possess at least two kinds of intelligence. Fluid intelligence is the ability to reason, see relationships, think abstractly, and hold information in mind while working on a problem; crystallized intelligence is one’s accumulated knowledge of the world and the procedures or mental models one has developed from past learning and experience. Together, these two kinds of intelligence enable us to learn, reason, and solve problems.7 Traditionally, IQ tests have been used to measure individuals’ logical and verbal potential. These tests assign an Intelligence Quotient, which denotes the ratio of mental age to physical age, times 100. That is, an eight-year-old who can solve problems on a test that most ten-year-olds can solve has an IQ of 125 (10 divided by 8, times 100). It used to be thought that IQ was fixed from birth, but traditional notions of intellectual capacity are being challenged. One countervailing idea, put forward by the psychologist Howard Gardner to account for the broad variety in people’s abilities, is the hypothesis that humans have as many as eight different kinds of intelligence: Logical-mathematical intelligence: ability to think critically, work with numbers and abstractions, and the like; Spatial intelligence: three-dimensional judgment and the ability to visualize with the mind’s eye; Linguistic intelligence: ability to work with words and languages; Kinesthetic intelligence: physical dexterity and control of one’s body; Musical intelligence: sensitivity to sounds, rhythms, tones, and music; Interpersonal intelligence: ability to “read” other people and work with them effectively; Intrapersonal intelligence: ability to understand one’s self and make accurate judgments of one’s knowledge, abilities, and effectiveness; Naturalistic intelligence: the ability to discriminate and relate to one’s natural surroundings (for example, the kinds of intelligence invoked by a gardener, hunter, or chef). Gardner’s ideas are attractive for many reasons, not the least because they attempt to explain human differences that we can observe but cannot account for with modern, Western definitions of intelligence with their focus on language and logic abilities. As with learning styles theory, the multiple intelligences model has helped educators to diversify the kinds of learning experiences they offer. Unlike learning styles, which can have the perverse effect of causing individuals to perceive their learning abilities as limited, multiple intelligences theory elevates the sheer variety of tools in our native toolkit. What both theories lack is an underpinning of empirical validation, a problem Gardner himself recognizes, acknowledging that determining one’s particular mix of intelligences is more an art than a science.8 While Gardner helpfully expands our notion of intelligence, the psychologist Robert J. Sternberg helpfully distills it again. Rather than eight intelligences, Sternberg’s model proposes three: analytical, creative, and practical. Further, unlike Gardner’s theory, Sternberg’s is supported by empirical research.9 One of Sternberg’s studies of particular interest to the question of how we measure intelligence was carried out in rural Kenya, where he and his associates looked at children’s informal knowledge of herbal medicines. Regular use of these medicines is an important part of Kenyans’ daily lives. This knowledge is not taught in schools or assessed by tests, bu","date":"2022-09-05","objectID":"/b10_make_it_stick/:35:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Dynamic Testing Robert Sternberg and Elena Grigorenko have proposed the idea of using testing to assess ability in a dynamic manner. Sternberg’s concept of developing expertise holds that with continued experience in a field we are always moving from a lower state of competence to a higher one. His concept also holds that standardized tests can’t accurately rate our potential because what they reveal is limited to a static report of where we are on the learning continuum at the time the test is given. In tandem with Sternberg’s three-part model of intelligence, he and Grigorenko have proposed a shift away from static tests and replacing them with what they call dynamic testing: determining the state of one’s expertise; refocusing learning on areas of low performance; follow-up testing to measure the improvement and to refocus learning so as to keep raising expertise. Thus, a test may assess a weakness, but rather than assuming that the weakness indicates a fixed inability, you interpret it as a lack of skill or knowledge that can be remedied. Dynamic testing has two advantages over standard testing. It focuses the learner and teacher on areas that need to be brought up rather than on areas of accomplishment, and the ability to measure a learner’s progress from one test to the next provides a truer gauge of his or her learning potential. Dynamic testing does not assume one must adapt to some kind of fixed learning limitation but offers an assessment of where one’s knowledge or performance stands on some dimension and how one needs to move forward to succeed: what do I need to learn in order to improve? That is, where aptitude tests and much of learning styles theory tend to emphasize our strengths and encourage us to focus on them, dynamic testing helps us to discover our weaknesses and correct them. In the school of life experience, setbacks show us where we need to do better. We can steer clear of similar challenges in the future, or we can redouble our efforts to master them, broadening our capacities and expertise. Bruce Hendry’s experiences investing in rental property and in the stock market dealt him setbacks, and the lessons he took away were essential elements of his education: to be skeptical when somebody’s trying to sell him something, to figure out the right questions, and to learn how to go dig out the answers. That’s developing expertise. Dynamic testing has three steps. Step 1: a test of some kind—perhaps an experience or a paper exam—shows me where I come up short in knowledge or a skill. Step 2: I dedicate myself to becoming more competent, using reflection, practice, spacing, and the other techniques of effective learning. Step 3: I test myself again, paying attention to what works better now but also, and especially, to where I still need more work. When we take our first steps as toddlers, we are engaging in dynamic testing. When you write your first short story, put it in front of your writers’ group for feedback, and then revise and bring it back, you’re engaging in dynamic testing, learning the writer’s craft and getting a sense of your potential. The upper limits of your performance in any cognitive or manual skill may be set by factors beyond your control, such as your intelligence and the natural limits of your ability, but most of us can learn to perform nearer to our full potential in most areas by discovering our weaknesses and working to bring them up.12 ","date":"2022-09-05","objectID":"/b10_make_it_stick/:36:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Structure Building There do appear to be cognitive differences in how we learn, though not the ones recommended by advocates of learning styles. One of these differences is the idea mentioned earlier that psychologists call structure building: the act, as we encounter new material, of extracting the salient ideas and constructing a coherent mental framework out of them. These frameworks are sometimes called mental models or mental maps. High structure-builders learn new material better than low structure-builders. The latter have difficulty setting aside irrelevant or competing information, and as a result they tend to hang on to too many concepts to be condensed into a workable model (or overall structure) that can serve as a foundation for further learning. The theory of structure building bears some resemblance to a village built of Lego blocks. Suppose you’re taking a survey course in a new subject. You start with a textbook full of ideas, and you set out to build a coherent mental model of the knowledge they contain. In our Lego analogy, you start with a box full of Lego pieces, and you set out to build the town that’s pictured on the box cover. You dump out the pieces and sort them into a handful of piles. First you lay out the streets and sidewalks that define the perimeter of the city and the distinct places within it. Then you sort the remaining pieces according to the elements they compose: apartment complex, school, hospital, stadium, mall, fire station. Each of these elements is like a central idea in the textbook, and each takes more shape and nuance as added pieces snap into place. Together, these central ideas form the larger structure of the village. Now suppose that your brother has used this Lego set before and dumped some pieces into the box from another set. As you find pieces, some might not fit with your building blocks, and you can put them aside as extraneous. Or you may discover that some of the new pieces can be used to form a substructure of an existing building block, giving it more depth and definition (porches, patios, and back decks as substructures of apartments; streetlights, hydrants, and boulevard trees as substructures of streets). You happily add these pieces to your village, even though the original designers of the set had not planned on this sort of thing. High structure-builders develop the skill to identify foundational concepts and their key building blocks and to sort new information based on whether it adds to the larger structure and one’s knowledge or is extraneous and can be put aside. By contrast, low structure-builders struggle in figuring out and sticking with an overarching structure and knowing what information needs to fit into it and what ought to be discarded. Structure building is a form of conscious and subconscious discipline: stuff fits or it doesn’t; it adds nuance, capacity and meaning, or it obscures and overfreights. A simpler analogy might be a friend who wants to tell you a rare story about this four-year-old boy she knows: she mentions who the mother is, how they became friends in their book club, finally mentioning that the mother, by coincidence, had a large load of manure delivered for her garden on the morning of the boy’s birthday—the mother’s an incredible gardener, her eggplants took a ribbon at the county fair and got her an interview on morning radio, and she gets her manure from that widowed guy in your church who raises the Clydesdale horses and whose son is married to—and so on and so on. Your friend cannot winnow the main ideas from the blizzard of irrelevant associations, and the story is lost on the listener. Story, too, is structure. Our understanding of structure building as a cognitive difference in learning is still in the early stages: is low structure-building the result of a faulty cognitive mechanism, or is structure-building a skill that some pick up naturally and others must be taught? We know that when questions are embedded in texts t","date":"2022-09-05","objectID":"/b10_make_it_stick/:37:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Rule versus Example Learning Another cognitive difference that appears to matter is whether you are a “rule learner” or “example learner,” and the distinction is somewhat akin to the one we just discussed. When studying different kinds of problems in a chemistry class, or specimens in a course on birds and how to identify them, rule learners tend to abstract the underlying principles or “rules” that differentiate the examples being studied. Later, when they encounter a new chemistry problem or bird specimen, they apply the rules as a means to classify it and select the appropriate solution or specimen box. Example learners tend to memorize the examples rather than the underlying principles. When they encounter an unfamiliar case, they lack a grasp of the rules needed to classify or solve it, so they generalize from the nearest example they can remember, even if it is not particularly relevant to the new case. However, example learners may improve at extracting underlying rules when they are asked to compare two different examples rather than focus on studying one example at a time. Likewise, they are more likely to discover the common solution to disparate problems if they first have to compare the problems and try to figure out the underlying similarities. By way of an illustration, consider two different hypothetical problems faced by a learner. These are taken from research into rule learning. In one problem, a general’s forces are set to attack a castle that is protected by a moat. Spies have learned that the bridges over the moat have been mined by the castle’s commander. The mines are set to allow small groups to cross the bridges, so that the occupants of the castle can retrieve food and fuel. How can the general get a large force over the bridges to attack the castle without tripping the mines? The other problem involves an inoperable tumor, which can be destroyed by focused radiation. However, the radiation must also pass through healthy tissue. A beam of sufficient intensity to destroy the tumor will damage the healthy tissue through which it passes. How can the tumor be destroyed without damaging healthy tissue? In the studies, students have difficulty finding the solution to either of these problems unless they are instructed to look for similarities between them. When seeking similarities, many students notice that (1) both problems require a large force to be directed at a target, (2) the full force cannot be massed and delivered through a single route without an adverse outcome, and (3) smaller forces can be delivered to the target, but a small force is insufficient to solve the problem. By identifying these similarities, students often arrive at a strategy of dividing the larger force into smaller forces and sending these in through different routes to converge on the target and destroy it without setting off mines or damaging healthy tissue. Here’s the payoff: after figuring out this common, underlying solution, students are then able to go on to solve a variety of different convergence problems.14 As with high and low structure-builders, our understanding of rule versus example learners is very preliminary. However, we know that high structure-builders and rule learners are more successful in transferring their learning to unfamiliar situations than are low structure-builders and example learners. You might wonder if the tendency to be a high structure-builder is correlated with the tendency to be a rule learner. Unfortunately, research is not yet available to answer this question. You can see the development of structure-building and rule-learning skills in a child’s ability to tell a joke. A three-year-old probably cannot deliver a knock-knock joke, because he lacks an understanding of structure. You reply “Who’s there?” and he jumps to the punch line: “Door is locked, I can’t get in!” He doesn’t understand the importance, after “Who’s there?”, of replying “Doris” to set up the joke. But by the time he’s fi","date":"2022-09-05","objectID":"/b10_make_it_stick/:38:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"The Takeaway Given what we know about learning differences, what’s the takeaway? Be the one in charge. There’s an old truism from sales school that says you can’t shoot a deer from the lodge. The same goes for learning: you have to suit up, get out the door, and find what you’re after. Mastery, especially of complex ideas, skills, and processes, is a quest. It is not a grade on a test, something bestowed by a coach, or a quality that simply seeps into your being with old age and gray hair. Embrace the notion of successful intelligence. Go wide: don’t roost in a pigeonhole of your preferred learning style but take command of your resources and tap all of your “intelligences” to master the knowledge or skill you want to possess. Describe what you want to know, do, or accomplish. Then list the competencies required, what you need to learn, and where you can find the knowledge or skill. Then go get it. Consider your expertise to be in a state of continuing development, practice dynamic testing as a learning strategy to discover your weaknesses, and focus on improving yourself in those areas. It’s smart to build on your strengths, but you will become ever more competent and versatile if you also use testing and trial and error to continue to improve in the areas where your knowledge or performance are not pulling their weight. Adopt active learning strategies like retrieval practice, spacing, and interleaving. Be aggressive. Like those with dyslexia who have become high achievers, develop workarounds or compensating skills for impediments or holes in your aptitudes. Don’t rely on what feels best: like a good pilot checking his instruments, use quizzing, peer review, and the other tools described in Chapter 5 to make sure your judgment of what you know and can do is accurate, and that your strategies are moving you toward your goals. Don’t assume that you’re doing something wrong if the learning feels hard. Remember that difficulties you can overcome with greater cognitive effort will more than repay you in the depth and durability of your learning. Distill the underlying principles; build the structure. If you’re an example learner, study examples two at a time or more, rather than one by one, asking yourself in what ways they are alike and different. Are the differences such that they require different solutions, or are the similarities such that they respond to a common solution? Break your idea or desired competency down into its component parts. If you think you are a low structure-builder or an example learner trying to learn new material, pause periodically and ask what the central ideas are, what the rules are. Describe each idea and recall the related points. Which are the big ideas, and which are supporting concepts or nuances? If you were to test yourself on the main ideas, how would you describe them? What kind of scaffold or framework can you imagine that holds these central ideas together? If we borrowed the winding stair metaphor as a structure for Bruce Hendry’s investment model, it might work something like this. Spiral stairs have three parts: a center post, treads, and risers. Let’s say the center post is the thing that connects us from where we are (down here) to where we want to be (up there): it’s the investment opportunity. Each tread is an element of the deal that protects us from losing money and dropping back, and each riser is an element that lifts us up a notch. Treads and risers must both be present for the stairs to function and for a deal to be attractive. Knowing the scrap value of boxcars is a tread—Bruce knows he won’t get less than that for his investment. Another tread is the guaranteed lease income while his capital is tied up. What are some risers? Impending scarcity, which will raise values. The like-new condition of the cars, which is latent value. A deal that doesn’t have treads and risers will not protect the downside or reliably deliver the upside. Structure is all around us and available t","date":"2022-09-05","objectID":"/b10_make_it_stick/:39:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Neuroplasticity All knowledge and memory are physiological phenomena, held in our neurons and neural pathways. The idea that the brain is not hardwired but plastic, mutable, something that reorganizes itself with each new task, is a recent revelation, and we are just at the frontiers of understanding what it means and how it works. In a helpful review of the neuroscience, John T. Bruer took on this question as it relates to the initial development and stabilization of the brain’s circuitry and our ability to bolster the intellectual ability of our children through early stimulation. We’re born with about 100 billion nerve cells, called neurons. A synapse is a connection between neurons, enabling them to pass signals. For a period shortly before and after birth, we undergo “an exuberant burst of synapse formation,” in which the brain wires itself: the neurons sprout microscopic branches, called axons, that reach out in search of tiny nubs on other neurons, called dendrites. When axon meets dendrite, a synapse is formed. In order for some axons to find their target dendrites they must travel vast distances to complete the connections that make up our neural circuitry (a journey of such daunting scale and precision that Bruer likens it to finding one’s way clear across the United States to a waiting partner on the opposite coast, not unlike Kit Carson’s mission to President Polk for General Fremont). It’s this circuitry that enables our senses, cognition, and motor skills, including learning and memory, and it is this circuitry that forms the possibilities and the limits of one’s intellectual capacity. The number of synapses peaks at the age of one or two, at about 50 percent higher than the average number we possess as adults. A plateau period follows that lasts until around puberty, whereupon this overabundance begins to decline as the brain goes through a period of synaptic pruning. We arrive at our adult complement at around age sixteen with a staggering number, thought to total about 150 trillion connections. We don’t know why the infant brain produces an overabundance of connections or how it subsequently determines which ones to prune. Some neuroscientists believe that the connections we don’t use are the ones that fade and die away, a notion that would seem to manifest the “use it or lose it” principle and argue for the early stimulation of as many connections as possible in hopes of retaining them for life. Another theory suggests the burgeoning and winnowing is determined by genetics and we have little or no influence over which synapses survive and which do not. “While children’s brains acquire a tremendous amount of information during the early years,” the neuroscientist Patricia Goldman-Rakic told the Education Commission of the States, most learning is acquired after synaptic formation stabilizes. “From the time a child enters first grade, through high school, college, and beyond, there is little change in the number of synapses. It is during the time when no, or little, synapse formation occurs that most learning takes place” and we develop adult-level skills in language, mathematics, and logic.3 And it is likely during this period more than during infancy, in the view of the neuroscientist Harry T. Chugani, that experience and environmental stimulation fine-tune one’s circuits and make one’s neuronal architecture unique.4 In a 2011 article, a team of British academics in the fields of psychology and sociology reviewed the evidence from neuroscience and concluded that the architecture and gross structure of the brain appear to be substantially determined by genes but that the fine structure of neural networks appears to be shaped by experience and to be capable of substantial modification.5 That the brain is mutable has become evident on many fronts. Norman Doidge, in his book The Brain That Changes Itself, looks at compelling cases of patients who have overcome severe impairments with the assistance of neurologist","date":"2022-09-05","objectID":"/b10_make_it_stick/:40:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Is IQ Mutable? IQ is a product of genes and environment. Compare it to height: it’s mostly inherited, but over the decades as nutrition has improved, subsequent generations have grown taller. Likewise, IQs in every industrialized part of the world have shown a sustained rise since the start of standardized sampling in 1932, a phenomenon called the Flynn effect after the political scientist who first brought it to wide attention.11 In the United States, the average IQ has risen eighteen points in the last sixty years. For any given age group, an IQ of 100 is the mean score of those taking the IQ tests, so the increase means that having an IQ of 100 today is the intelligence equivalent of those with an IQ 60 years ago of 118. It’s the mean that has risen, and there are several theories why this is so, the principal one being that schools, culture (e.g., television), and nutrition have changed substantially in ways that affect people’s verbal and math abilities as measured by the subtests that make up the IQ test. Richard Nisbett, in his book Intelligence and How to Get It, discusses the pervasiveness of stimuli in modern society that didn’t exist years ago, offering as one simple example a puzzle maze McDonald’s included in its Happy Meals a few years ago that was more difficult than the mazes included in an IQ test for gifted children.12 Nisbett also writes about “environmental multipliers,” suggesting that a tall kid who goes out for basketball develops a proficiency in the sport that a shorter kid with the same aptitudes won’t develop, just as a curious kid who goes for learning gets smarter than the equally bright but incurious kid who doesn’t. The options for learning have expanded exponentially. It may be a very small genetic difference that makes one kid more curious than another, but the effect is multiplied in an environment where curiosity is easily piqued and readily satisfied. Another environmental factor that shapes IQ is socioeconomic status and the increased stimulation and nurturing that are more generally available in families who have more resources and education. On average, children from affluent families test higher for IQ than children from impoverished families, and children from impoverished families who are adopted into affluent families score higher on IQ tests than those who are not, regardless of whether the birth parents were of high or low socioeconomic status. The ability to raise IQ is fraught with controversy and the subject of countless studies reflecting wide disparities of scientific rigor. A comprehensive review published in 2013 of the extant research into raising intelligence in young children sheds helpful light on the issue, in part because of the strict criteria the authors established for determining which studies would qualify for consideration. The eligible studies had to draw from a general, nonclinical population; have a randomized, experimental design; consist of sustained interventions, not of one-shot treatments or simply of manipulations during the testing experience; and use a widely accepted, standardized measure of intelligence. The authors focused on experiments involving children from the prenatal period through age five, and the studies meeting their requirements involved over 37,000 participants. What did they find? Nutrition affects IQ. Providing dietary supplements of fatty acids to pregnant women, breast-feeding women, and infants had the effect of increasing IQ by anywhere from 3.5 to 6.5 points. Certain fatty acids provide building blocks for nerve cell development that the body cannot produce by itself, and the theory behind the results is that these supplements support the creation of new synapses. Studies of other supplements, such as iron and B complex vitamins, strongly suggested benefits, but these need validation through further research before they can be considered definitive. In the realm of environmental effects, the authors found that enrolling poor child","date":"2022-09-05","objectID":"/b10_make_it_stick/:41:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Brain Training? What about “brain training” games? We’ve seen a new kind of business emerge, pitching online games and videos promising to exercise your brain like a muscle, building your cognitive ability. These products are largely founded on the findings of one Swiss study, reported in 2008, which was very limited in scope and has not been replicated.14 The study focused on improving “fluid intelligence”: the facility for abstract reasoning, grasping unfamiliar relationships, and solving new kinds of problems. Fluid intelligence is one of two kinds of intelligence that make up IQ. The other is crystallized intelligence, the storehouse of knowledge we have accumulated through the years. It’s clear that we can increase our crystallized intelligence through effective learning and memory strategies, but what about our fluid intelligence? A key determiner of fluid intelligence is the capacity of a person’s working memory—the number of new ideas and relationships that a person can hold in mind while working through a problem (especially with some amount of distraction). The focus of the Swiss study was to give participants tasks requiring increasingly difficult working memory challenges, holding two different stimuli in mind for progressively longer periods of distraction. One stimulus was a sequence of numerals. The other was a small square of light that appeared in varying locations on a screen. Both the numerals and the locations of the square changed every three seconds. The task was to decide—while viewing a sequence of changed numerals and repositioned squares—for each combination of numeral and square, whether it matched a combination that had been presented n items back in the series. The number n increased during the trials, making the challenge to working memory progressively more arduous. All the participants were tested on fluid intelligence tasks at the outset of the study. Then they were given these increasingly difficult exercises of their working memory over periods ranging up to nineteen days. At the end of the training, they were retested for fluid intelligence. They all performed better than they had before the training, and those who had engaged in the training for the longest period showed the greatest improvement. These results showed for the first time that fluid intelligence can be increased through training. What’s the criticism? The participants were few (only thirty-five) and were all recruited from a similar, highly intelligent population. Moreover, the study focused on only one training task, so it is unclear to what extent it might apply to other working-memory training tasks, or whether the results are really about working memory rather than some peculiarity of the particular training. Finally, the durability of the improved performance is unknown, and the results, as noted, have not been replicated by other studies. The ability to replicate empirical results is the bedrock of scientific theory. The website PsychFileDrawer.org keeps a list of the top twenty psychological research studies that the site’s users would like to see replicated, and the Swiss study is the first on the list. A recent attempt whose results were published in 2013 failed to find any improvements to fluid intelligence as a result of replicating the exercises in the Swiss study. Interestingly, participants in the study believed that their mental capacities had been enhanced, a phenomenon the authors describe as illusory. However, the authors also acknowledge that an increased sense of self-efficacy can lead to greater persistence in solving difficult problems, encouraged by the belief that training has improved one’s abilities.15 The brain is not a muscle, so strengthening one skill does not automatically strengthen others. Learning and memory strategies such as retrieval practice and the building of mental models are effective for enhancing intellectual abilities in the material or skills practiced, but the benefits don’t exten","date":"2022-09-05","objectID":"/b10_make_it_stick/:42:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Growth Mindset Let’s return to the old saw “If you think you can, or you think you can’t, you’re right.” If turns out there is more truth here than wit. Attitude counts for a lot. The studies of the psychologist Carol Dweck have gotten huge attention for showing just how big an impact one simple conviction can have on learning and performance: the belief that your level of intellectual ability is not fixed but rests to a large degree in your own hands.16 Dweck and her colleagues have replicated and expanded on their results in many studies. In one of the early experiments, she ran a workshop for low-performing seventh graders at a New York City junior high school, teaching them about the brain and about effective study techniques. Half the group also received a presentation on memory, but the other half were given an explanation of how the brain changes as a result of effortful learning: that when you try hard and learn something new, the brain forms new connections, and these new connections, over time, make you smarter. This group was told that intellectual development is not the natural unfolding of intelligence but results from the new connections that are formed through effort and learning. After the workshop, both groups of kids filtered back into their classwork. Their teachers were unaware that some had been taught that effortful learning changes the brain, but as the school year unfolded, those students adopted what Dweck calls a “growth mindset,” a belief that their intelligence was largely within their own control, and they went on to become much more aggressive learners and higher achievers than students from the first group, who continued to hold the conventional view, what Dweck calls a “fixed mindset,” that their intellectual ability was set at birth by the natural talents they were born with. Dweck’s research had been triggered by her curiosity over why some people become helpless when they encounter challenges and fail at them, whereas others respond to failure by trying new strategies and redoubling their effort. She found that a fundamental difference between the two responses lies in how a person attributes failure: those who attribute failure to their own inability—“I’m not intelligent”—become helpless. Those who interpret failure as the result of insufficient effort or an ineffective strategy dig deeper and try different approaches. Dweck came to see that some students aim at performance goals, while others strive toward learning goals. In the first case, you’re working to validate your ability. In the second, you’re working to acquire new knowledge or skills. People with performance goals unconsciously limit their potential. If your focus is on validating or showing off your ability, you pick challenges you are confident you can meet. You want to look smart, so you do the same stunt over and over again. But if your goal is to increase your ability, you pick ever-increasing challenges, and you interpret setbacks as useful information that helps you to sharpen your focus, get more creative, and work harder. “If you want to demonstrate something over and over, ‘ability’ feels like something static that lies inside of you, whereas if you want to increase your ability, it feels dynamic and malleable,” Dweck says. Learning goals trigger entirely different chains of thought and action from performance goals.17 Paradoxically, a focus on performance trips up some star athletes. Praised for being “naturals,” they believe their performance is a result of innate gifts. If they’re naturals, the idea goes, they shouldn’t have to work hard to excel, and in fact many simply avoid practicing, because a need to practice is public evidence that their natural gifts are not good enough to cut the mustard after all. A focus on performance instead of on learning and growing causes people to hold back from risk taking or exposing their self-image to ridicule by putting themselves into situations where they have to break a sweat","date":"2022-09-05","objectID":"/b10_make_it_stick/:43:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Deliberate Practice When you see stellar performances by an expert in any field—a pianist, chess player, golfer—perhaps you marvel at what natural talent must underlie their abilities, but expert performance does not usually rise out of some genetic predisposition or IQ advantage. It rises from thousands of hours of what Anders Ericsson calls sustained deliberate practice. If doing something repeatedly might be considered practice, deliberate practice is a different animal: it’s goal directed, often solitary, and consists of repeated striving to reach beyond your current level of performance. Whatever the field, expert performance is thought to be garnered through the slow acquisition of a larger number of increasingly complex patterns, patterns that are used to store knowledge about which actions to take in a vast vocabulary of different situations. Witness a champion chess player. In studying the positions on a board, he can contemplate many alternative moves and the countless different directions each might precipitate. The striving, failure, problem solving, and renewed attempts that characterize deliberate practice build the new knowledge, physiological adaptations, and complex mental models required to attain ever higher levels. When Michelangelo finally completed painting over 400 life size figures on the ceiling of the Sistine Chapel, he is reported to have written, “If people knew how hard I worked to get my mastery, it wouldn’t seem so wonderful after all.” What appeared to his admirers to have flowed from sheer genius had required four torturous years of work and dedication.20 Deliberate practice usually isn’t enjoyable, and for most learners it requires a coach or trainer who can help identify areas of performance that need to be improved, help focus attention on specific aspects, and provide feedback to keep perception and judgment accurate. The effort and persistence of deliberate practice remodel the brain and physiology to accommodate higher performance, but achieving expertise in any field is particular to the field. It does not confer some kind of advantage or head start toward gaining expertise in another domain. A simple example of practice remodeling the brain is the treatment of focal hand dystonia, a syndrome affecting some guitarists and pianists whose repetitive playing has rewired their brains to think that two fingers have been fused into one. Through a series of challenging exercises, they can be helped gradually to retrain their fingers to move separately. One reason that experts are sometimes perceived to possess an uncanny talent is that some can observe a complex performance in their field and later reconstruct from memory every aspect of that performance, in granular detail. Mozart was famous for being able to reconstruct complex musical scores after a single hearing. But this skill, Ericsson says, rises not out of some sixth sense but from an expert’s superior perception and memory within his domain, which are the result of years of acquired skill and knowledge in that domain. Most people who achieve expertise in a field are destined to remain average performers in the other realms of life. Ten thousand hours or ten years of practice was the average time the people Ericsson studied had invested to become expert in their fields, and the best among them had spent the larger percentage of those hours in solitary, deliberate practice. The central idea here is that expert performance is a product of the quantity and the quality of practice, not of genetic predisposition, and that becoming expert is not beyond the reach of normally gifted people who have the motivation, time, and discipline to pursue it. ","date":"2022-09-05","objectID":"/b10_make_it_stick/:44:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Memory Cues Mnemonic devices, as we mentioned, are mental tools to help hold material in memory, cued for ready recall. (Mnemosyne, one of the nine Muses of Greek mythology, was the goddess of memory.) Some examples of simple mnemonic devices are acronyms, like “ROY G BIV” for the colors of the rainbow, and reverse acronyms, as in “I Value Xylophones Like Cows Dig Milk” for the ascending value of Roman numerals from 1 to 1000 (e.g., V = 5; D = 500). A memory palace is a more complex type of mnemonic device that is useful for organizing and holding larger volumes of material in memory. It’s based on the method of loci, which goes back to the ancient Greeks and involves associating mental images with a series of physical locations to help cue memories. For example, you imagine yourself within a space that is very familiar to you, like your home, and then you associate prominent features of the space, like your easy chair, with a visual image of something you want to remember. (When you think of your easy chair you may picture a limber yogi sitting there, to remind you to renew your yoga lessons.) The features of your home can be associated with a countless number of visual cues for retrieving memories later, when you simply take an imaginary walk through the house. If it’s important to recall the material in a certain order, the cues can be sequenced along the route through your house. (The method of loci is also used to associate cues with features you encounter along a very familiar journey, like your walk to the corner store.) As we write this passage, a group of students in Oxford, England, are constructing memory palaces to prepare for their A-level exams in psychology. Every week for six weeks, they and their instructor have visited a different café in town, where they have relaxed over coffee, familiarized themselves with the layout of the place, and discussed how they might imagine it occupied with vivid characters who will cue from memory important aspects of psychology that they will need to write about at exam time. We’ll come back to these students, but first a few more words about this technique, which is surprisingly effective and derives from the way imagery serves to contribute vividness and connective links to memory. Humans remember pictures more easily than words. (For example, the image of an elephant is easier to recall than the word “elephant.”) So it stands to reason that associating vivid mental images with verbal or abstract material makes that material easier to retrieve from memory. A strong mental image can prove as secure and bountiful as a loaded stringer of fish. Tug on it, and a whole day’s catch comes to the surface. When a friend is reminding you of a conversation with somebody the two of you met on a trip, you struggle to recall it. She tells you where the discussion happened, and you picture the place. Ah, yes, it all comes flooding back. Images cue memories.21 Mark Twain wrote about his personal experiences with this phenomenon in an article published by Harper’s. In his days on the speaking circuit, Twain used a list of partial sentences to prompt himself through the different phases of his remarks, but he found the system unsatisfactory—when you glance at snippets of text, they all look alike. He experimented with alternatives, finally hitting on the idea of outlining his speech in a series of crude pencil sketches. The sketches did the job. A haystack with a snake under it told him where to start his story about his adventures in Nevada’s Carson Valley. An umbrella tilted against a stiff wind took him to the next part of his story, the fierce winds that blew down out of the Sierras at about two o’clock every afternoon. And so on. The power of these sketches to evoke memory impressed Twain and gave rise one day to an idea for helping his children, who were still struggling to learn the kings and queens of England, despite long hours invested by their nanny in trying to hammer the names and ","date":"2022-09-05","objectID":"/b10_make_it_stick/:45:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"The Takeaway It comes down to the simple but no less profound truth that effortful learning changes the brain, building new connections and capability. This single fact—that our intellectual abilities are not fixed from birth but are, to a considerable degree, ours to shape—is a resounding answer to the nagging voice that too often asks us “Why bother?” We make the effort because the effort itself extends the boundaries of our abilities. What we do shapes who we become and what we’re capable of doing. The more we do, the more we can do. To embrace this principle and reap its benefits is to be sustained through life by a growth mindset. And it comes down to the simple fact that the path to complex mastery or expert performance does not necessarily start from exceptional genes, but it most certainly entails self-discipline, grit, and persistence; with these qualities in healthy measure, if you want to become an expert, you probably can. And whatever you are striving to master, whether it’s a poem you wrote for a friend’s birthday, the concept of classical conditioning in psychology, or the second violin part in Hayden’s Fifth Symphony, conscious mnemonic devices can help to organize and cue the learning for ready retrieval until sustained, deliberate practice and repeated use form the deeper encoding and subconscious mastery that characterize expert performance. 8 Make It Stick NO MATTER WHAT YOU MAY set your sights on doing or becoming, if you want to be a contender, it’s mastering the ability to learn that will get you in the game and keep you there. In the preceding chapters, we resisted the temptation to become overtly prescriptive, feeling that if we laid out the big ideas from the empirical research and illustrated them well through examples, you could reach your own conclusions about how best to apply them. But early readers of those chapters urged us to get specific with practical advice. So we do that here. We start with tips for students, thinking in particular of high school, college, and graduate school students. Then we speak to lifelong learners, to teachers, and finally to trainers. While the fundamental principles are consistent across these groups, the settings, life stages, and learning materials differ. To help you envision how to apply these tips, we tell the stories of several people who, one way or another, have already found their way to these strategies and are using them to great effect. ","date":"2022-09-05","objectID":"/b10_make_it_stick/:46:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Learning Tips for Students Remember that the most successful students are those who take charge of their own learning and follow a simple but disciplined strategy. You may not have been taught how to do this, but you can do it, and you will likely surprise yourself with the results. Embrace the fact that significant learning is often, or even usually, somewhat difficult. You will experience setbacks. These are signs of effort, not of failure. Setbacks come with striving, and striving builds expertise. Effortful learning changes your brain, making new connections, building mental models, increasing your capability. The implication of this is powerful: Your intellectual abilities lie to a large degree within your own control. Knowing that this is so makes the difficulties worth tackling. Following are three keystone study strategies. Make a habit of them and structure your time so as to pursue them with regularity. Practice Retrieving New Learning from Memory What does this mean? “Retrieval practice” means self-quizzing. Retrieving knowledge and skill from memory should become your primary study strategy in place of rereading. How to use retrieval practice as a study strategy: When you read a text or study lecture notes, pause periodically to ask yourself questions like these, without looking in the text: What are the key ideas? What terms or ideas are new to me? How would I define them? How do the ideas relate to what I already know? Many textbooks have study questions at the ends of the chapters, and these are good fodder for self-quizzing. Generating questions for yourself and writing down the answers is also a good way to study. Set aside a little time every week throughout the semester to quiz yourself on the material in a course, both the current week’s work and material covered in prior weeks. When you quiz yourself, check your answers to make sure that your judgments of what you know and don’t know are accurate. Use quizzing to identify areas of weak mastery, and focus your studying to make them strong. The harder it is for you to recall new learning from memory, the greater the benefit of doing so. Making errors will not set you back, so long as you check your answers and correct your mistakes. What your intuition tells you to do: Most studiers focus on underlining and highlighting text and lecture notes and slides. They dedicate their time to rereading these, becoming fluent in the text and terminology, because this feels like learning. Why retrieval practice is better: After one or two reviews of a text, self-quizzing is far more potent for learning than additional rereading. Why might this be so? This is explained more fully in Chapter 2 , but here are some of the high points. The familiarity with a text that is gained from rereading creates illusions of knowing, but these are not reliable indicators of mastery of the material. Fluency with a text has two strikes against it: it is a misleading indicator of what you have learned, and it creates the false impression that you will remember the material. By contrast, quizzing yourself on the main ideas and the meanings behind the terms helps you to focus on the central precepts rather than on peripheral material or on a professor’s turn of phrase. Quizzing provides a reliable measure of what you’ve learned and what you haven’t yet mastered. Moreover, quizzing arrests forgetting. Forgetting is human nature, but practice at recalling new learning secures it in memory and helps you recall it in the future. Periodically practicing new knowledge and skills through self-quizzing strengthens your learning of it and your ability to connect it to prior knowledge. A habit of regular retrieval practice throughout the duration of a course puts an end to cramming and all-nighters. You will need little studying at exam time. Reviewing the material the night before is much easier than learning it. How it feels: Compared to rereading, self-quizzing can feel awkward and frustrating, espe","date":"2022-09-05","objectID":"/b10_make_it_stick/:47:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Tips for Lifelong Learners The learning strategies we have just outlined for students are effective for anyone at any age. But they are centered around classroom instruction. Lifelong learners are using the same principles in a variety of less-structured settings. In a sense, of course, we’re all lifelong learners. From the moment we’re born we start learning about the world around us through experimentation, trial and error, and random encounters with challenges that require us to recall what we did the last time we found ourselves in a similar circumstance. In other words, the techniques of generation, spaced practice and the like that we present in this book are organic (even if counterintuitive), and it’s not surprising that many people have already discovered their power in the pursuit of interests and careers that require continuous learning. Retrieval Practice Nathaniel Fuller is a professional actor with the Guthrie Theater in Minneapolis. We took an interest in him after a dinner party where the Guthrie’s renowned artistic director, Joe Dowling, on hearing of our work, immediately suggested we interview Fuller. It seems that Fuller has the capacity to so fully learn the lines and movements of a role for which he is understudy that he can go onstage at the last moment with great success, despite not having had the benefit of learning and rehearsing it in the normal way. Fuller is a consummate professional of the stage, having refined his techniques for learning roles over many years. He is often cast in a leading role; at other times, he may play several lesser characters in a play while also understudying the lead. How does he do it? When he starts with a new script, Fuller puts it into a binder, goes through it, and highlights all of his lines. “I figure out how much I’ve got to learn. I try to estimate how much I can learn in a day, and then I try to start early enough to get that learned.”3 Highlighting his lines also makes them easy to find and gives him a sense of the construction, so this use of highlighting is rather different from what students do in class when they highlight merely for purposes of rereading. “You get the shape of the line, and how the back-and-forth works.” Fuller uses retrieval practice in various forms. First, he takes a blank sheet of paper and covers a page of the script. He draws it down, silently rendering the lines of the characters he’s playing opposite, because those lines cue his own, and the emotion in them is reflected one way or another by his own character. He keeps his own line covered and attempts to speak it aloud from memory. He checks his accuracy. If he gets the line wrong, he covers it up and speaks it again. When he has spoken it correctly, he reveals the next passage and goes on. “Half of knowing your part is not just what to say, but knowing when to say it. I don’t have an exceptional brain for memorizing, but one of the keys I’ve found is, I need to try my best to say the line without looking at it. I need to have that struggle in order to make myself remember it. “I’ll work like crazy. When I get to where it feels like diminishing returns, I’ll quit. Then I’ll come back the next day, and I won’t remember it. That’s where a lot of my friends will panic. I just have faith now that it’s in there, it’s going to come back a little bit better the next time. Then I’ll work on a new chunk, until I get to the end of the play.” As he progresses through the script, he’s constantly moving from familiar pages and scenes into newer material, the play taking shape like threads added to a growing tapestry, each scene given meaning by those that came before and extending the story in turn. When he reaches the end, he practices in reverse order, moving from the less familiar last scene to practice the more familiar one that precedes it and then continuing on through the last scene again. Then he goes to the part preceding both of those scenes and practices through to the end. His prac","date":"2022-09-05","objectID":"/b10_make_it_stick/:48:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Tips for Teachers Here again we are leery of being too prescriptive. Every teacher must find what’s right in his or her classroom. Yet specifics can be helpful. So here are some basic strategies that in our judgment will go a long way toward helping students become stronger learners in the classroom. Brief descriptions follow of what some teachers are already doing along these lines. Between the recommendations and the examples, we hope you will find practical ideas you can adapt and put to work. Explain to Students How Learning Works Students labor under many myths and illusions about learning that cause them to make some unfortunate choices about intellectual risk taking and about when and how to study. It’s the proper role of the teacher to explain what empirical studies have discovered about how people learn, so the student can better manage his or her own education. In particular, students must be helped to understand such fundamental ideas as these: • Some kinds of difficulties during learning help to make the learning stronger and better remembered. • When learning is easy, it is often superficial and soon forgotten. • Not all of our intellectual abilities are hardwired. In fact, when learning is effortful, it changes the brain, making new connections and increasing intellectual ability. • You learn better when you wrestle with new problems before being shown the solution, rather than the other way around. • To achieve excellence in any sphere, you must strive to surpass your current level of ability. • Striving, by its nature, often results in setbacks, and setbacks are often what provide the essential information needed to adjust strategies to achieve mastery. These topics, woven throughout the book, are discussed in depth in Chapters 4 and 7 . Teach Students How to Study Students generally are not taught how to study, and when they are, they often get the wrong advice. As a result, they gravitate to activities that are far from optimal, like rereading, massed practice, and cramming. At the beginning of this chapter we present effective study strategies. Students will benefit from teachers who help them understand these strategies and stick with them long enough to experience their benefits, which may initially appear doubtful. Create Desirable Difficulties in the Classroom Where practical, use frequent quizzing to help students consolidate learning and interrupt the process of forgetting. Make the ground rules acceptable to your students and yourself. Students find quizzing more acceptable when it is predictable and the stakes for any individual quiz are low. Teachers find quizzing more acceptable when it is simple, quick, and does not lead to negotiating makeup quizzes. (For one example, consider the way Kathleen McDermott, whose work we describe below, uses daily quizzing in her university class on human learning and memory.) Create study tools that incorporate retrieval practice, generation, and elaboration. These might be exercises that require students to wrestle with trying to solve a new kind of problem before coming to the class where the solution is taught; practice tests that students can download and use to review material and to calibrate their judgments of what they know and don’t know; writing exercises that require students to reflect on past lesson material and relate it to other knowledge or other aspects of their lives; exercises that require students to generate short statements that summarize the key ideas of recent material covered in a text or lecture. Make quizzing and practice exercises count toward the course grade, even if for very low stakes. Students in classes where practice exercises carry consequences for the course grade learn better than those in classes where the exercises are the same but carry no consequences. Design quizzing and exercises to reach back to concepts and learning covered earlier in the term, so that retrieval practice continues and the learning is cumulative, helping","date":"2022-09-05","objectID":"/b10_make_it_stick/:49:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["self-learning"],"content":"Tips for Trainers Here are some ways trainers are using the same principles as those who teach in schools, in a variety of less structured and nonclassroom settings. In-Service Training Licensed professionals in many fields must earn continuing education credits to keep their skills current and maintain their licenses. As the pediatric neurologist Doug Larsen describes in Chapter 3 , this kind of training for doctors is typically compressed into a weekend symposium, out of respect for participants’ busy schedules, set at a hotel or resort, and structured around meals and PowerPoint lectures. In other words, the strategies of retrieval practice, spacing, and interleaving are nowhere to be seen. Participants are lucky to retain much of what they learn. If you see yourself in this scenario, there are a few things you might consider doing. One, get a copy of the presentation materials and use them to quiz yourself on the key ideas, much as Nathaniel Fuller quizzes himself on the arc of a play, his lines, the many layers of character. Two, schedule follow-up emails to appear in your inbox every month or so with questions that require you to retrieve the critical learning you gained from the seminar. Three, contact your professional association and ask them to consider revamping their approach to training along the lines outlined in this book. The testing effect forms the basis of a new commercial training platform called Qstream that helps trainers send learners periodic quizzes via their mobile devices to strengthen learning through spaced retrieval practice. Similarly, an emerging platform called Osmosis uses mobile and Web based software to provide learners access to thousands of crowdsourced practice questions and explanations. Osmosis combines the testing effect, spacing, and social networking to facilitate what its developers call “student-driven social learning.” Qstream (qstream.com ) and Osmosis (osmose-it.com ) suggest interesting possibilities for redesigning in-service training for professionals. Many other companies are developing similar programs. Kathy Maixner, Business Coach The Maixner Group is a consulting shop based in Portland, Oregon, that helps companies identify growth strategies and improve their sales tactics. Kathy Maixner fries big fish and little. One of the big fish added $21 million to its annual revenue as a result of hooking up with Maixner. One of the small ones, Inner Gate Acupuncture (profiled at the close of this chapter), learned how to establish a solid business management footing under a clinical practice whose growth was outpacing its control systems. We’re interested in Maixner because the coaching techniques she has developed over her career line up so well with the learning principles described in this book. In short, Maixner sees her role as helping the client dig past the symptoms of a problem to discover its root causes, and then to generate possible solutions and play out the implications of different strategies before committing to them. Maixner told us: “If you hand people the solution, they don’t need to explore how you got to that solution. If they generate the solution, then they’re the ones who are traveling down that road. Should they go left or right? We discuss the options.”11 Maixner’s years of experience working with clients in many different fields helps her see around corners, where the hazards lie. She often uses role-playing to simulate problems, getting her clients to generate solutions, try them out, get feedback, and practice what works. In other words, she introduces the difficulties that make the learning stronger and more accurately reflect what the client will encounter out in the marketplace. Farmers Insurance Corporate sales training can be complicated. Typically, it’s about corporate culture, beliefs and behavior, and learning to promote and protect the brand. It’s also technical, learning the features and advantages of the products. And it’s partly strategic, ","date":"2022-09-05","objectID":"/b10_make_it_stick/:50:0","tags":["self-learning"],"title":"Make it Stick","uri":"/b10_make_it_stick/"},{"categories":["capital"],"content":"Preface While Louis D. Brandeis’s series of articles on the money trust was running in Harper’s Weekly many inquiries came about publication in more accessible permanent form. Even without such urgence througbh the mail, however, it would have been clear that these articles inevitably constituted a book, since they embodied an analysis and a narrative by that mind which, on the great industrial movements of our era, is the most expert in the United States. The inquiries meant that the attentive public recognized that here was a contribution to history. Here was the clearest and most profound treatment ever published on that part of our business development which, as President Wilson and other wise men have said, has come to constitute the greatest of our problems. The story of our time is the story of industry. No scholar of the future will be able to describe our era with authority unless he comprehends that expansion and concentration which followed the harnessing of steam and electricity, the great uses of the change, and the great excesses. No historian of the future, in my opinion, will find among our contemporary documents so masterful an analysis of why concentration went astray. I am but one among many who look upon Mr. Brandeis as having ,in the gield of economics, the most inventive and sound mind of our time. While his articles were running in Harper’s Weekly I had ample opportunity to know how widespread was the belief among intelligent men that this brilliant diagnosis of our money trust was the most important contribution to current thought in many years. “Great” is one of the words that I do not use loosely, and I look upon Mr. Brandeis as a great man. In the composition of his intellect, one of the most important elements is his comprehension of figures. As one of the leading financiers of the country said to me, “Mr. Brandeis’s greatness as a lawyer is part of his greatness as a mathematician.” My views on this subject are sufficiently indicated in the follwing editorial in Harper’s Weekly. Arithmetic About five years before the Metropolitan Traction Company of New York went into the hands of receiver, Mr. Brandeis came down from Boston, and in a speech at Cooper Union prophesied that that company must fail. Leading bankers in New York and Boston were heartily recommending the stock to their customers. Mr. Braneis made his prophecy merely by analyzing the published figures. How did he win in the Pinchot-Glavis-Ballinger controversy? In various ways, no doubt; but perhaps the most critical step was when he calculated just how long it would take a fast worker to go through the Glavis-Ballinger record and make a juegment of it; whereupon he decided that Mr. Wickersham could not have made his report at the tiem it was stated to have been made, and therefore it must have been predated. Most of Mr. Brandiers’s other contributions to current history have involved arithmetic. when he succeeded in preventing a raise in freight rates, it was throught an exact analysis of cost. When he got Savings Bank Insurance started in Massachusees, it was by being abel to figure what insurance ought to cost. When he made the best contract between a city and a public utility that exists in this country, a definite grasp of the gas business was nescessary–combined, of course, with the wisdom and originality that make a statesman. He could not have invented the preferential shop if that new idea had not been founded on a precise knowledge of the conditions in the garment trades. When he established before the United States Supreme Court the constitutionality of legislation affedting women only, he relied much less upon reason than upon the amount of knowledge displayed of what actually happens to women when they are overworked–which, while not arithmetric, is built on the same intellectual quality. Nearly two years before Mr. Mellen resigned from the New Haven Railroad, Mr. Brandeis Wrote to the present Editor of this paper a private l","date":"2022-09-05","objectID":"/b45_other_peoples_money/:1:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter I: Our Financial Oligarchy PRESIDENT WILSON, when Governor, declared in 1911: “The great monopoly in this country is the money monopoly. So long as that exists, our old variety and freedom and individual energy of development are out of the question. A great industrial nation is controlled by its system of credit. Our system of credit is concentrated. The growth of the nation, therefore, and all our activities are in the hands of a few men, who, even if their actions be honest and intended for the public interest, are necessarily concentrated upon the great undertakings in which their own money is involved and who, necessarily, by every reason of their own limitations, chill and check and destroy genuine economic freedom. This is the greatest question of all; and to this, statesmen must address themselves with an earnest determination to serve the long future and the true liberties of men.” The Pujo Committee–appointed in 1912—found: “Far more dangerous than all that has happened to us in the past in the way of elimination of competition in industry is the control of credit through the domination of these groups over our banks and industries.\"… “Whether under a different currency system the resources in our banks would be greater or less is comparatively immaterial if they continue to be controlled by a small group.\"… “It is impossible that there should be competition with all the facilities for raising money or selling large issues of bonds in the hands of these few bankers and their partners and allies, who together dominate the financial policies of most of the existing systems. . . . The acts of this inner group, as here described, have nevertheless been more destructive of competition than anything accomplished by the trusts, for they strike at the very vitals of potential competition in every industry that is under their protection, a condition which if permitted to continue, will render impossible all attempts to restore normal competitive conditions in the industrial world. . . . “If the arteries of credit now clogged well-nigh to choking by the obstructions created through the control of these groups are opened so that they may be permitted freely to play their important part in the financial system, competition in large enterprises will become possible and business can be conducted on its merits instead of being subject to the tribute and the good will of this handful of self-constituted trustees of the national prosperity.” The promise of New Freedom was joyously proclaimed in 1913. The facts which the Pujo Investigating Committee and its able Counsel, Mr. Samuel Untermyer, have laid before the country, show clearly the means by which a few men control the business of America. The report proposes measures which promise some relief. Additional remedies will be proposed. Congress will soon be called upon to act. How shall the emancipation be wrought? On what lines shall we proceed? The facts, when fully understood, will teach us. The Dominant Element The dominant element in our financial oligarchy is the investment banker. Associated banks, trust companies and life insurance companies are his tools. Controlled railroads, public service and industrial corporations are his subjects. Though properly but middlemen, these bankers bestride as masters America’s business world, so that practically no large enterprise can be undertaken successfully without their participation or approval. These bankers are, of course, able men possessed of large fortunes; but the most potent factor in their control of business is not the possession of extraordinary ability or huge wealth. The key to their power is Combination–concentration intensive and comprehensive–advancing on three distinct lines: First: There is the obvious consolidation of banks and trust companies; the less obvious affiliations–through stockholdings, voting trusts and interlocking directorates–of banking institutions which are not legally connected; and the join","date":"2022-09-05","objectID":"/b45_other_peoples_money/:2:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter II: How the Combiners Combine Among the allies, two New York banks—the National City and the First National—stand preeminent. They constitute, with the Morgan firm, the inner group of the Money Trust. Each of the two banks, like J. P. Morgan \u0026 Co., has huge resources. Each of the two banks, like the firm of J. P. Morgan \u0026 Co., has been dominated by a genius in combination. In the Na­tional City it is James Stillman; in the First National, George F. Baker. Each of these gentlemen was for­merly President, and is now Chairman of the Board of Directors. The resources of the National City Bank (including its Siamese-twin security company) are about $300,000,000; those of the First National Bank (including its Siamese-twin security company) are about $200,000,000. The resources of the Morgan firm have not been disclosed. But it appears that they have available for their operations, also, huge deposits from their subjects; deposits reported as $162,500,000. The private fortunes of the chief actors in the combination have not been ascertained. But sporadic evidence indicates how great are the possibilities of accumulation when one has the use of “other people’s money.” Mr. Morgan’s wealth became proverbial. Of Mr. Stillman’s many investments, only one was specifically referred to, as he was in Europe during the investigation, and did not testify. But that one is significant. His 47,498 shares in the National City Bank are worth about 18,000,000. Mr. Jacob H. Schiff aptly described this as “a very nice investment.” Of Mr. Baker’s investments we know more, as he testified on many subjects. His 20,000 shares in the First National Bank are worth at least 20,000,000. His stocks in six other New York banks and trust companies are together worth about 3,000,000. The scale of his investment in railroads may be inferred from his former holdings in the Central Railroad of New Jersey. He was its largest stockholder—so large that with a few friends he held a majority of the 27,436,800 par value of outstanding stock, which the Reading bought at 160 a share. He is a director in 28 other railroad companies; and presumably a stock­holder in, at least, as many. The full extent of his fortune was not inquired into, for that was not an issue in the investigation. But it is not surprising that Mr. Baker saw little need of new laws. When asked: “You think everything is all right as it is in this world, do you not?” He answered, “Pretty nearly.” Ramifications of Power But wealth expressed in figures gives a wholly inadequate picture of the allies' power. Their wealth is dynamic. It is wielded by geniuses in combination. It finds its proper expression in means of control. To comprehend the power of the allies we must try to visualize the ramifications through which the forces operate. Mr. Baker is a director in 22 corporations having, with their many subsidiaries, aggregate resources or capitalization of 7,272,000,000. But the direct and visible power of the First National Bank, which Mr. Baker dominates, extends further. The Pujo report shows that its directors (including Mr. Baker’s son) are directors in at least 27 other corporations with resources of 4,270,000,000. That is, the First National is represented in 49 corporations, with aggregate resources or capitalization of 11,542,000,000. It may help to an appreciation of the allies’ power to name a few of the more prominent corporations in which, for instance, Mr. Baker’s influence is exerted—visibly and directly—as voting trustee, executive com­mittee man or simple director. Banks, Trust, and Life Insurance Companies:First National Bank of New York; National Bank of Commerce; Farmers' Loan and Trust Company; Mutual Life Insurance Company. Railroad Companies:New York Central Lines; New Haven, Reading, Erie, Lackawanna, Lehigh Valley, Southern, Northern Pacific, Chicago, Burlington \u0026 Quincy. Public Service Corporations:American Telegraph \u0026 Telephone Company, Adams Express Company. Industrial Corp","date":"2022-09-05","objectID":"/b45_other_peoples_money/:3:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter III: Interlocking Directorates The practice of interlocking directorates is the root of many evils. It offends laws human and divine. Applied to rival corporations, it tends to the suppression of competition and to violation of the Sherman law. Applied to corporations which deal with each other, it tends to disloyalty and to violation of the fundamental law that no man can serve two masters. In either event it tends to inefficiency; for it removes incentive and destroys soundness of judgment. It is undemocratic, for it rejects the platform: “A fair field and no favors,\"—substituting the pull of privilege for the push of manhood. It is the most potent instrument of the Money Trust. Break the control so exercised by the investment bankers over railroads, public-service and industrial corporations, over banks, life insurance and trust companies, and a long step will have been taken toward attainment of the New Freedom. The term “Interlocking directorates” is here used in a broad sense as including all intertwined conflicting interests, whatever the form, and by whatever device effected. The objection extends alike to contracts of a corporation whether with one of its directors individually, or with a firm of which he is a member, or with another corporation in which he is interested as an officer or director or stockholder. The objection extends likewise to men holding the inconsistent position of director in two potentially competing corporations, even if those corporations do not actually deal with each other. The Endless Chain A single example will illustrate the vicious circle of control—the endless chain—through which our financial oligarchy now operates: J. P. Morgan (or a partner), a director of the New York, New Haven \u0026 Hartford Railroad, causes that company to sell to J. P. Morgan \u0026 Co. an issue of bonds. J. P. Morgan \u0026 Co. borrow the money with which to pay for the bonds from the Guaranty Trust Company, of which Mr. Morgan (or a partner) is a director. J. P. Morgan \u0026 Co. sell the bonds to the Penn Mutual Life Insurance Company, of which Mr. Morgan (or a partner) is a director. The New Haven spends the proceeds of the bonds in purchasing steel rails from the United States Steel Corporation, of which Mr. Morgan (or a partner) is a director. The United States Steel Corporation spends the proceeds of the rails in purchasing electrical supplies from the General Electric Company, of which Mr. Morgan (or a partner) is a director. The General Electric sells supplies to the Western Union Telegraph Company, a subsidiary of the American Telephone and Telegraph Company; and in both Mr. Morgan (or a partner) is a director. The Telegraph Company has an exclusive wire contract with the Reading, of which Mr. Morgan (or a partner) is a director. The Reading buys its passenger cars from the Pullman Company, of which Mr. Morgan (or a partner) is a director. The Pullman Company buys (for local use) locomotives from the Baldwin Locomotive Company, of which Mr. Morgan (or a partner) is a director. The Reading, the General Electric, the Steel Corporation and the New Haven, like the Pullman, buy locomotives from the Baldwin Company. The Steel Corporation, the Telephone Company, the New Haven, the Reading, the Pullman and the Baldwin Companies, like the Western Union, buy electrical supplies from the General Electric. The Baldwin, the Pullman, the Reading, the Telephone, the Telegraph and the General Electric companies, like the New Haven, buy steel products from the Steel Corporation. Each and every one of the companies last named markets its securities through J. P. Morgan \u0026 Co.; each deposits its funds with J. P. Morgan \u0026 Co.; and with these funds of each, the firm enters upon further operations. This specific illustration is in part supposititious; but it represents truthfully the operation of interlocking directorates. Only it must be multiplied many times and with many permutations to represent fully the extent to which the intere","date":"2022-09-05","objectID":"/b45_other_peoples_money/:4:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter IV: Serve One Master Only The Pujo Committee has presented the facts concerning the Money Trust so clearly that the conclusions appear inevitable. Their diagnosis discloses intense financial concentration and the means by which it is effected. Combination,—the intertwining of interests,—is shown to be the all-pervading vice of the present system. With a view to freeing industry, the Committee recommends the enactment of twenty-one specific remedial provisions. Most of these measures are wisely framed to meet some abuse disclosed by the evidence; and if all of these were adopted the Pujo legislation would undoubtedly alleviate present suffering and aid in arresting the disease. But many of the remedies proposed are “local” ones; and a cure is not possible, without treatment which is fundamental. Indeed, a major operation is necessary. This the Committee has hesitated to advise; although the fundamental treatment required is simple: “Serve one Master only.” The evils incident to interlocking directorates are, of course, fully recognized; but the prohibitions proposed in that respect are restricted to a very narrow sphere. First: The Committee recognizes that potentially competing corporations should not have a common director;—but it restricts this prohibition to directors of national banks, saying: “No officer or director of a national bank shall be an officer or director of any other bank or of any trust company or other financial or other corporation or institution, whether organized under state or federal law, that is authorized to receive money on deposit or that is engaged in the business of loaning money on collateral or in buying and selling securities except as in this section provided; and no person shall be an officer or director of any national bank who is a private banker or a member of a firm or partnership of bankers that is engaged in the business of receiving deposits: Provided, That such bank, trust company, financial institution, banker, or firm of bankers is located at or engaged in business at or in the same city, town, or village as that in which such national bank is located or engaged in business: Provided further, That a director of a national bank or a partner of such director may be an officer or director of not more than one trust company organized by the laws of the state in which such national bank is engaged in business and doing business at the same place.” Second: The Committee recognizes that a corporation should not make a contract in which one of the management has a private interest; but it restricts this prohibition (1) to national banks, and (2) to the officers, saying: “No national bank shall lend or advance money or credit or purchase or discount any promissory note, draft, bill of exchange or other evidence of debt bearing the signature or indorsement of any of its officers or of any partnership of which such officer is a member, directly or indirectly, or of any corporation in which such officer owns or has a beneficial interest of upward of ten per centum of the capital stock, or lend or advance money or credit to, for or on behalf of any such officer or of any such partnership or corporation, or purchase any security from any such officer or of or from any partnership or corporation of which such officer is a member or in which he is financially interested, as herein specified, or of any corporation of which any of its officers is an officer at the time of such transaction.” Prohibitions of intertwining relations so restricted, however supplemented by other provisions, will not end financial concentration. The Money Trust snake will, at most, be scotched, not killed. The prohibition of a common director in potentially competing corporations should apply to state banks and trust companies, as well as to national banks; and it should apply to railroad and industrial corporations as fully as to banking institutions. The prohibition of corporate contracts in which one of the manag","date":"2022-09-05","objectID":"/b45_other_peoples_money/:5:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter V: What Publicity Can Do Publicity is justly commended as a remedy for social and industrial diseases. Sunlight is said to be the best of disinfectants; electric light the most efficient policeman. And publicity has already played an important part in the struggle against the Money Trust. The Pujo Committee has, in the disclosure of the facts concerning financial concentration, made a most important contribution toward attainment of the New Freedom. The battlefield has been surveyed and charted. The hostile forces have been located, counted and appraised. That was a necessary first step—and a long one—towards relief. The provisions in the Committee’s bill concerning the incorporation of stock exchanges and the statement to be made in connection with the listing of securities would doubtless have a beneficent effect. But there should be a further call upon publicity for service. That potent force must, in the impending struggle, be utilized in many ways as a continuous remedial measure. Wealth Combination and control of other people’s money and of other people’s businesses. These are the main factors in the development of the Money Trust. But the wealth of the investment banker is also a factor. And with the extraordinary growth of his wealth in recent years, the relative importance of wealth as a factor in financial concentration has grown steadily. It was wealth which enabled Mr. Morgan, in 1910, to pay $3,000,000 for $51,000 par value of the stock of the Equitable Life Insurance Society. His direct income from this investment was limited by law to less than one-eighth of one per cent. a year; but it gave legal control of 504,000,000 of assets. It was wealth which enabled the Morgan associates to buy, from the Equitable and the Mutual Life Insurance Company the stocks in the several banking institutions, which, merged in the Bankers' Trust Company and the Guaranty Trust Company, gave them control of 357,000,000 deposits. It was wealth which enabled Mr. Morgan to acquire his shares in the First National and National City banks, worth 21,000,000, through which he cemented the triple alliance with those institutions. Now, how has this great wealth been accumulated? Some of it was natural accretion. Some of it is due to special opportunities for investment wisely availed of. Some of it is due to the vast extent of the bankers' operations. Then power breeds wealth as wealth breeds power. But a main cause of these large fortunes is the huge tolls taken by those who control the avenues to capital and to investors. There has been exacted as toll literally “all that the traffic will bear.” Excessive Bankers’ Commissions The Pujo Committee was unfortunately prevented by lack of time from presenting to the country the evidence covering the amounts taken by the investment bankers as promoters' fees, underwriting commissions and profits. Nothing could have demonstrated so clearly the power exercised by the bankers, as a schedule showing the aggregate of these taxes levied within recent years. It would be well worth while now to reopen the Money Trust investigation merely to collect these data. But earlier investigations have disclosed some illuminating, though sporadic facts. The syndicate which promoted the Steel Trust, took, as compensation for a few weeks' work, securities yielding 62,500,000 in cash; and of this, J. P. Morgan \u0026 Co. received for their services, as Syndicate Managers, 12,500,000, besides their share, as syndicate subscribers, in the remaining 50,000,000. The Morgan syndicate took for promoting the Tube Trust 20,000,000 common stock out of a total issue of 80,000,000 stock (preferred and common). Nor were monster commissions limited to trust promotions. More recently, bankers' syndicates have, in many instances, received for floating preferred stocks of recapitalized industrial concerns, one-third of all common stock issued, besides a considerable sum in cash. And for the sale of preferred stock of well establish","date":"2022-09-05","objectID":"/b45_other_peoples_money/:6:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter VI: Where the Banker is Superfluous The abolition of interlocking directorates will greatly curtail the bankers' power by putting an end to many improper combinations. Publicity concerning bankers' commissions, profits and associates, will lend effective aid, particularly by curbing undue exactions. Many of the specific measures recommended by the Pujo Committee (some of them dealing with technical details) will go far toward correcting corporate and banking abuses; and thus tend to arrest financial concentration. But the investment banker has, within his legitimate province, acquired control so extensive as to menace the public welfare even where his business is properly conducted. If the New Freedom is to be attained, every proper means of lessening that power must be availed of. A simple and effective remedy, which can be widely applied, even without new legislation, lies near at hand: Eliminate the banker-middleman where he is superfluous. Today practically all governments, states and municipalities pay toll to the banker on all bonds sold. Why should they? It is not because the banker is always needed. It is because the banker controls the only avenue through which the investor in bonds and stocks can ordinarily be reached. The banker has become the universal tax gatherer. True, thepro rataof taxes levied by him upon our state and city governments is less than that levied by him upon the corporations. But few states or cities escape payment of some such tax to the banker on every loan it makes. Even where the new issues of bonds are sold at public auction, or to the highest bidder on sealed proposals, the bankers' syndicates usually secure large blocks of the bonds which are sold to the people at a considerable profit. The middleman, even though unnecessary, collects his tribute. There is a legitimate field for dealers in state and municipal bonds, as for other merchants. Investors already owning such bonds must have a medium through which they can sell their holdings. And those states or municipalities which lack an established reputation among investors, or which must seek more distant markets, need the banker to distribute new issues. But there are many states and cities which have an established reputation and have a home market at hand. These should sell their bonds direct to investors without the intervention of a middleman. And as like conditions prevail with some corporations, their bonds and stocks should also be sold direct to the investor. Both financial efficiency and industrial liberty demand that the bankers' toll be abolished, where that is possible. Banker and Broker The business of the investment banker must not be confused with that of the bond and stock broker. The two are often combined; but the functions are essentially different. The broker performs a very limited service. He has properly nothing to do with the original issue of securities, nor with their introduction into the market. He merely negotiates a purchase or sale as agent for another under specific orders. He exercises no discretion, except in the method of bringing buyer and seller together, or of executing orders. For his humble service he receives a moderate compensation, a commission, usually one-eighth of one per cent. (12 ½ cents for each $100) on the par value of the security sold. The investment banker also is a mere middleman. But he is a principal, not an agent. He is also a merchant in bonds and stocks. The compensation received for his part in the transaction is in many cases more accurately described as profit than as commission. So far as concerns new issues of government, state and municipal bonds, especially, he acts as merchant, buying and selling securities on his own behalf; buying commonly at wholesale from the maker and selling at retail to the investors; taking the merchant’s risk and the merchant’s profits. On purchases of corporate securities the profits are often very large; but even a large profit may be e","date":"2022-09-05","objectID":"/b45_other_peoples_money/:7:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter VII: Big Men and Little Business J. P. Morgan \u0026 Co. declare, in their letter to the Pujo Committee, that “practically all the railroad and industrial development of this country has taken place initially through the medium of the great banking houses.” That statement is entirely unfounded in fact. On the contrary nearly every such contribution to our comfort and prosperity was “initiated\"withouttheir aid. The “great banking houses” came into relation with these enterprises, either after success had been attained, or upon “reorganization” after the possibility of success had been demonstrated, but the funds of the hardy pioneers, who had risked their all, were exhausted. This is true of our early railroads, of our early street railways, and of the automobile; of the telegraph, the telephone and the wireless; of gas and oil; of harvesting machinery, and of our steel industry; of textile, paper and shoe industries; and of nearly every other important branch of manufacture. Theinitiationof each of these enterprises may properly be characterized as “great transactions”; and the men who contributed the financial aid and business management necessary for their introduction are entitled to share, equally with inventors, in our gratitude for what has been accomplished. But the instances are extremely rare where the original financing of such enterprises was undertaken by investment bankers, great or small. It was usually done by some common business man, accustomed to taking risks; or by some well-to-do friend of the inventor or pioneer, who was influenced largely by considerations other than money-getting. Here and there you will find that banker-aid was given; but usually in those cases it was a small local banking concern, not a “great banking house”' which helped to “initiate” the undertaking. Railroads We have come to associate the great bankers with railroads. But their part was not conspicuous in the early history of the Eastern railroads; and in the Middle West the experience was, to some extent, similar. The Boston \u0026 Maine Railroad owns and leases 2,215 miles of line; but it is a composite of about 166 separate railroad companies. The New Haven Railroad owns and leases 1,996 miles of line; but it is a composite of 112 separate railroad companies. The necessary capital to build these little roads was gathered together, partly through state, county or municipal aid; partly from business men or landholders who sought to advance their special interests; partly from investors; and partly from well-to-do public-spirited men, who wished to promote the welfare of their particular communities. About seventy-five years after the first of these railroads was built, J. P. Morgan \u0026 Co. became fiscal agent for all of them by creating the New Haven-Boston \u0026 Maine monopoly. Steamships The history of our steamship lines is similar. In 1807, Robert Fulton, with the financial aid of Robert R. Livingston, a judge and statesman—not a banker— demonstrated with the Claremont, that it was practicable to propel boats by steam. In 1833 the three Cunard brothers of Halifax and 232 other persons—stockholders of the Quebec and Halifax Steam Navigation Company—joined in supplying about $80,000 to build the Royal William, the first steamer to cross the Atlantic. In 1902, many years after individual enterprises had developed practically all the great ocean lines, J. P. Morgan \u0026 Co. floated the International Mercantile Marine with its $52,744,000 of 4 1/2 bonds, now selling at about 60, and $100,000,000 of stock (preferred and common) on which no dividend has ever been paid. It was just sixty-two years after the first regular line of transatlantic steamers—the Cunard—was founded that Mr. Morgan organized the Shipping Trust. Telegraph The story of the telegraph is similar. The money for developing Morse’s invention was supplied by his partner and co-worker, Alfred Vail. The initial line (from Washington to Baltimore) was built with an appropriation of ","date":"2022-09-05","objectID":"/b45_other_peoples_money/:8:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter VIII: A Curse of Bigness Bigness has been an important factor in the rise of the Money Trust: Big railroad systems, Big industrial trusts, big public service companies; and as instruments of these Big banks and Big trust companies. J. P. Morgan \u0026 Co. (in their letter of defence to the Pujo Committee) urge the needs of Big Business as the justification for financial concentration. They declare that what they euphemistically call “cooperation” is “simply a further result of the necessity for handling great transactions”; that “the country obviously requires not only the larger individual banks, but demands also that those banks shall cooperate to perform efficiently the country’s business”; and that “a step backward along this line would mean a halt in industrial progress that would affect every wage-earner from the Atlantic to the Pacific.” The phrase “great transactions” is used by the bankers apparently as meaning large corporate security issues. Leading bankers have undoubtedly cooperated during the last 15 years in floating some very large security issues, as well as many small ones. But relatively few large issues were made necessary by great improvements undertaken or by industrial development. Improvements and development ordinarily proceed slowly. For them, even where the enterprise involves large expenditures, a series of smaller issues is usually more appropriate than single large ones. This is particularly true in the East where the building of new railroads has practically ceased. The “great” security issues in which bankers have cooperated were, with relatively few exceptions, made either for the purpose of effecting combinations or, as a consequence of such combinations. Furthermore, the combinations which made necessary these large security issues or underwritings were, in most cases, either contrary to existing statute law, or contrary to laws recommended by the Interstate Commerce Commission, or contrary to the laws of business efficiency. So both the financial concentration and the combinations which they have served were, in the main, against the public interest. Size, we are told, is not a crime. But size may, at least, become noxious by reason of the means through which it was attained or the uses to which it is put. And it is size attained by combination, instead of natural growth, which has contributed so largely to our financial concentration. Let us examine a few cases: The Harriman Pacifics J. P. Morgan \u0026 Co., in urging the “need of large banks and the cooperation of bankers,” said: “The Attorney-General’s recent approval of the Union Pacific settlement calls for a single commitment on the part of bankers of $126,000,000.” This $126,000,000 “commitment” was not made to enable the Union Pacific to secure capital. On the contrary it was a guaranty that it would succeed in disposing of its Southern Pacific stock to that amount. And when it had disposed of that stock, it was confronted with the serious problem—what to do with the proceeds? This huge underwriting became necessary solely because the Union Pacific had violated the Sherman Law. It had acquired that amount of Southern Pacific stock illegally; and the Supreme Court of the United States finally decreed that the illegality cease. This same illegal purchase had been the occasion, twelve years earlier, of another “great transaction,\"—the issue, of a 100,000,000 of Union Pacific bonds, which were sold to provide funds for acquiring this Southern Pacific and other stocks in violation of law. Bankers “cooperated” also to accomplish that. Union Pacific Improvements The Union Pacific and its auxiliary lines (the Oregon Short Line, the Oregon Railway and Navigation and the Oregon-Washington Railroad) made, in the fourteen years, ending June 30, 1912, issues of securities aggregating $375,158,183 (of which $46,500,000 were refunded or redeemed) ; but the large security issues served mainly to supply funds for engaging in illegal combinations or sto","date":"2022-09-05","objectID":"/b45_other_peoples_money/:9:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter IX: The Failure of Banker-Management Thereis not one moral, but many, to be drawn from the Decline of the New Haven and the Fall of Mellen. That history offers texts for many sermons. It illustrates the Evils of Monopoly, the Curse of Bigness, the Futility of Lying, and the Pitfalls of Law-Breaking. But perhaps the most impressive lesson that it should teach to investors is the failure of banker-management. Banker Control For years J. P. Morgan \u0026 Co. were the fiscal agents of the New Haven. For years Mr. Morgan wasthedirector of the Company. He gave to that property probably closer personal attention than to any other of his many interests. Stockholders’ meetings are rarely interesting or important; and few indeed must have been the occasions when Mr. Morgan attended any stockholders' meeting of other companies in which he was a director. But it was his habit, when in America, to be present at meetings of the New Haven. In 1907, when the policy of monopolistic expansion was first challenged, and again at the meeting in 1909 (after Massachusetts had unwisely accorded its sanction to the Boston \u0026 Maine merger), Mr. Morgan himself moved the large increases of stock which were unanimously voted. Of course, he attended the important directors' meeting. His will was law. President Mellen indicated this in his statement before Interstate Commerce Commissioner Prouty, while discussing the New York, Westchester \u0026 Boston—the railroad without a terminal in New York, which cost the New Haven $1,500,000 a mile to acquire, and was then costing it, in operating deficits and interest charges, $100,000 a month to run: “I am in a very embarrassing position, Mr. Commissioner, regarding the New York, Westchester \u0026 Boston. I have never been enthusiastic or at all optimistic of its being a good investment for our company in the present, or in the immediate future; but people in whom I had greater confidence than I have in myself thought it was wise and desirable; I yielded my judgment; indeed, I don’t know that it would have made much difference whether I yielded or not.” The Banker’s Responsibility Bankers are credited with being a conservative force in the community. The tradition lingers that they are preeminently “safe and sane.” And yet, the most grievous fault of this banker-managed railroad has been its financial recklessness—a fault that has already brought heavy losses to many thousands of small investors throughout New England for whom bankers are supposed to be natural guardians. In a community where its railroad stocks have for generations been deemed absolutely safe investments, the passing of the New Haven and of the Boston \u0026 Maine dividends after an unbroken dividend record of generations comes as a disaster. This disaster is due mainly to enterprises outside the legitimate operation of these railroads; for no railroad company has equaled the New Haven in the quantity and extravagance of its outside enterprises. But it must be remembered, that neither the president of the New Haven nor any other railroad manager could engage in such transactions without the sanction of the Board of Directors. It is the directors, not Mr. Mellen, who should bear the responsibility. Close scrutiny of the transactions discloses no justification. On the contrary, scrutiny serves only to make more clear the gravity of the errors committed. Not merely were recklessly extravagant acquisitions made in mad pursuit of monopoly; but the financial judgment, the financiering itself, was conspicuously bad. To pay for property several times what it is worth, to engage in grossly unwise enterprises, are errors of which no conservative directors should be found guilty; for perhaps the most important function of directors is to test the conclusions and curb by calm counsel the excessive zeal of too ambitious managers. But while we have no right to expect from bankers exceptionally good judgment in ordinary business matters; we do have a right to expect from th","date":"2022-09-05","objectID":"/b45_other_peoples_money/:10:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":["capital"],"content":"Chapter X: The Inefficiency of the Oligarchs We must break the Money Trust or the Money Trust will break us. The Interstate Commerce Commission said in its report on the most disastrous of the recent wrecks on the New Haven Railroad: “On this directorate were and are men whom the confiding public recognize as magicians in the art of finance, and wizards in the construction, operation, and consolidation of great systems of railroads. The public therefore rested secure that with the knowledge of the railroad art possessed by such men investments and travel should both be safe. Experience has shown that this reliance of the public was not justified as to either finance or safety.” This failure of banker-management is not surprising. The surprise is that men should have supposed it would succeed. For banker-management contravenes the fundamental laws of human limitations: First, that no man can serve two masters; second, that a man cannot at the same time do many things well. Seeming Successes There are numerous seeming exceptions to these rules; and a relatively few real ones. Of course, many banker-managed properties have been prosperous; some for a long time, at the expense of the public; some for a shorter time, because of the impetus attained before they were banker-managed. It is not difficult to have a large net income, where one has the field to oneself, has all the advantage privilege can give, and may “charge all the traffic will bear.” And even in competitive business the success of a long-established, well-organized business with a widely extended good-will, must continue for a considerable time; especially if buttressed by intertwined relations constantly giving it the preference over competitors. The real test of efficiency comes when success has to be struggled for; when natural or legal conditions limit the charges which may be made for the goods sold or service rendered. Our banker-managed railroads have recently been subjected to such a test, and they have failed to pass it. “It is only,\"' says Goethe, “when working within limitations, that the master is disclosed.” Why Oligarchy Fails Banker-management fails, partly because the private interest destroys soundness of judgment and undermines loyalty. It fails partly, also, because banker directors are led by their occupation (and often even by the mere fact of their location remote from the operated properties) to apply a false test in making their decisions. Prominent in the banker-director mind is always this thought: “What will be the probable effect of our action upon the market value of the company’s stock and bonds, or, indeed, generally upon stock exchange values?” The stock market is so much a part of the investment-banker’s life, that he cannot help being affected by this consideration, however disinterested he may be. The stock market is sensitive. Facts are often misinterpreted “by the street'” or by investors. And with the best of intentions, directors susceptible to such influences are led to unwise decisions in the effort to prevent misinterpretations. Thus, expenditures necessary for maintenance, or for the ultimate good of a property are often deferred by banker-directors, because of the belief that the making of themnow, would (by showing smaller net earnings), create a bad, and even false, impression on the market. Dividends are paid which should not be, because of the effect which it is believed reduction or suspension would have upon the market value of the company’s securities. To exercise a sound judgment in the difficult affairs of business is, at best, a delicate operation. And no man can successfully perform that function whose mind is diverted, however innocently, from the study of, “what is best in the long run for the company of which I am director?” The banker-director is peculiarly liable to such distortion of judgment by reason of his occupation and his environment. But there is a further reason why, ordinarily, banker-management mus","date":"2022-09-05","objectID":"/b45_other_peoples_money/:11:0","tags":["captital","banker"],"title":"Other People's Money","uri":"/b45_other_peoples_money/"},{"categories":null,"content":"Create A Library under Your Own Control No one can interfere with your library. With Github’s popular and easy-to-use platform, you can catalog and organize all your books in one cyberspace with completely zero maintenance fee. Now, You can translate any book to your language, listen to your fond books anytime, and search content a breeze free. If you got any questions, please: ","date":"2022-09-04","objectID":"/about/:0:1","tags":null,"title":"Contact Libmind","uri":"/about/"},{"categories":null,"content":"Contact us Support website: libmind.com Mixin messenger: 29273 Mixin robot: 7000104144 Twitter: ChrisHowardaka Email: chrishowardaka@gmail.com ","date":"2022-09-04","objectID":"/about/:0:2","tags":null,"title":"Contact Libmind","uri":"/about/"},{"categories":["think"],"content":"Prologue After a bumpy flight, fifteen men dropped from the Montana sky. They weren’t skydivers. They were smokejumpers: elite wildland firefighters parachuting in to extinguish a forest fire started by lightning the day before. In a matter of minutes, they would be racing for their lives. The smokejumpers landed near the top of Mann Gulch late on a scorching August afternoon in 1949. With the fire visible across the gulch, they made their way down the slope toward the Missouri River. Their plan was to dig a line in the soil around the fire to contain it and direct it toward an area where there wasn’t much to burn. After hiking about a quarter mile, the foreman, Wagner Dodge, saw that the fire had leapt across the gulch and was heading straight at them. The flames stretched as high as 30 feet in the air. Soon the fire would be blazing fast enough to cross the length of two football fields in less than a minute. By 5:45 p.m. it was clear that even containing the fire was off the table. Realizing it was time to shift gears from fight to flight, Dodge immediately turned the crew around to run back up the slope. The smokejumpers had to bolt up an extremely steep incline, through knee-high grass on rocky terrain. Over the next eight minutes they traveled nearly 500 yards, leaving the top of the ridge less than 200 yards away. With safety in sight but the fire swiftly advancing, Dodge did something that baffled his crew. Instead of trying to outrun the fire, he stopped and bent over. He took out a matchbook, started lighting matches, and threw them into the grass. “We thought he must have gone nuts,” one later recalled. “With the fire almost on our back, what the hell is the boss doing lighting another fire in front of us?” He thought to himself: That bastard Dodge is trying to burn me to death. It’s no surprise that the crew didn’t follow Dodge when he waved his arms toward his fire and yelled, “Up! Up this way!” What the smokejumpers didn’t realize was that Dodge had devised a survival strategy: he was building an escape fire. By burning the grass ahead of him, he cleared the area of fuel for the wildfire to feed on. He then poured water from his canteen onto his handkerchief, covered his mouth with it, and lay facedown in the charred area for the next fifteen minutes. As the wildfire raged directly above him, he survived in the oxygen close to the ground. Tragically, twelve of the smokejumpers perished. A pocket watch belonging to one of the victims was later found with the hands melted at 5:56 p.m. Why did only three of the smokejumpers survive? Physical fitness might have been a factor; the other two survivors managed to outrun the fire and reach the crest of the ridge. But Dodge prevailed because of his mental fitness. when people reflect on what it takes to be mentally fit, the first idea that comes to mind is usually intelligence. The smarter you are, the more complex the problems you can solve—and the faster you can solve them. Intelligence is traditionally viewed as the ability to think and learn. Yet in a turbulent world, there’s another set of cognitive skills that might matter more: the ability to rethink and unlearn. Imagine that you’ve just finished taking a multiple-choice test, and you start to second-guess one of your answers. You have some extra time—should you stick with your first instinct or change it? About three quarters of students are convinced that revising their answer will hurt their score. Kaplan, the big test-prep company, once warned students to “exercise great caution if you decide to change an answer. Experience indicates that many students who change answers change to the wrong answer.” With all due respect to the lessons of experience, I prefer the rigor of evidence. When a trio of psychologists conducted a comprehensive review of thirty-three studies, they found that in every one, the majority of answer revisions were from wrong to right. This phenomenon is known as the first-instinct fallacy. In","date":"2022-09-03","objectID":"/b90_think_again/:0:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 1 ","date":"2022-09-03","objectID":"/b90_think_again/:1:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"A Preacher, a Prosecutor, a Politician, and a Scientist Walk into Your Mind Progress is impossible without change; and those who cannot change their minds cannot change anything. —george bernard shaw You probably don’t recognize his name, but Mike Lazaridis has had a defining impact on your life. From an early age, it was clear that Mike was something of an electronics wizard. By the time he turned four, he was building his own record player out of Legos and rubber bands. In high school, when his teachers had broken TVs, they called Mike to fix them. In his spare time, he built a computer and designed a better buzzer for high school quiz-bowl teams, which ended up paying for his first year of college. Just months before finishing his electrical engineering degree, Mike did what so many great entrepreneurs of his era would do: he dropped out of college. It was time for this son of immigrants to make his mark on the world. Mike’s first success came when he patented a device for reading the bar codes on movie film, which was so useful in Hollywood that it won an Emmy and an Oscar for technical achievement. That was small potatoes compared to his next big invention, which made his firm the fastest-growing company on the planet. Mike’s flagship device quickly attracted a cult following, with loyal customers ranging from Bill Gates to Christina Aguilera. “It’s literally changed my life,” Oprah Winfrey gushed. “I cannot live without this.” When he arrived at the White House, President Obama refused to relinquish his to the Secret Service. Mike Lazaridis dreamed up the idea for the BlackBerry as a wireless communication device for sending and receiving emails. As of the summer of 2009, it accounted for nearly half of the U.S. smartphone market. By 2014, its market share had plummeted to less than 1 percent. When a company takes a nosedive like that, we can never pinpoint a single cause of its downfall, so we tend to anthropomorphize it: BlackBerry failed to adapt. Yet adapting to a changing environment isn’t something a company does—it’s something people do in the multitude of decisions they make every day. As the cofounder, president, and co-CEO, Mike was in charge of all the technical and product decisions on the BlackBerry. Although his thinking may have been the spark that ignited the smartphone revolution, his struggles with rethinking ended up sucking the oxygen out of his company and virtually extinguishing his invention. Where did he go wrong? Most of us take pride in our knowledge and expertise, and in staying true to our beliefs and opinions. That makes sense in a stable world, where we get rewarded for having conviction in our ideas. The problem is that we live in a rapidly changing world, where we need to spend as much time rethinking as we do thinking. Rethinking is a skill set, but it’s also a mindset. We already have many of the mental tools we need. We just have to remember to get them out of the shed and remove the rust. ","date":"2022-09-03","objectID":"/b90_think_again/:2:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"SECOND THOUGHTS With advances in access to information and technology, knowledge isn’t just increasing. It’s increasing at an increasing rate. In 2011, you consumed about five times as much information per day as you would have just a quarter century earlier. As of 1950, it took about fifty years for knowledge in medicine to double. By 1980, medical knowledge was doubling every seven years, and by 2010, it was doubling in half that time. The accelerating pace of change means that we need to question our beliefs more readily than ever before. This is not an easy task. As we sit with our beliefs, they tend to become more extreme and more entrenched. I’m still struggling to accept that Pluto may not be a planet. In education, after revelations in history and revolutions in science, it often takes years for a curriculum to be updated and textbooks to be revised. Researchers have recently discovered that we need to rethink widely accepted assumptions about such subjects as Cleopatra’s roots (her father was Greek, not Egyptian, and her mother’s identity is unknown); the appearance of dinosaurs (paleontologists now think some tyrannosaurs had colorful feathers on their backs); and what’s required for sight (blind people have actually trained themselves to “see”—sound waves can activate the visual cortex and create representations in the mind’s eye, much like how echolocation helps bats navigate in the dark).* Vintage records, classic cars, and antique clocks might be valuable collectibles, but outdated facts are mental fossils that are best abandoned. We’re swift to recognize when other people need to think again. We question the judgment of experts whenever we seek out a second opinion on a medical diagnosis. Unfortunately, when it comes to our own knowledge and opinions, we often favor feeling right over being right. In everyday life, we make many diagnoses of our own, ranging from whom we hire to whom we marry. We need to develop the habit of forming our own second opinions. Imagine you have a family friend who’s a financial adviser, and he recommends investing in a retirement fund that isn’t in your employer’s plan. You have another friend who’s fairly knowledgeable about investing, and he tells you that this fund is risky. What would you do? When a man named Stephen Greenspan found himself in that situation, he decided to weigh his skeptical friend’s warning against the data available. His sister had been investing in the fund for several years, and she was pleased with the results. A number of her friends had been, too; although the returns weren’t extraordinary, they were consistently in the double digits. The financial adviser was enough of a believer that he had invested his own money in the fund. Armed with that information, Greenspan decided to go forward. He made a bold move, investing nearly a third of his retirement savings in the fund. Before long, he learned that his portfolio had grown by 25 percent. Then he lost it all overnight when the fund collapsed. It was the Ponzi scheme managed by Bernie Madoff. Two decades ago my colleague Phil Tetlock discovered something peculiar. As we think and talk, we often slip into the mindsets of three different professions: preachers, prosecutors, and politicians. In each of these modes, we take on a particular identity and use a distinct set of tools. We go into preacher mode when our sacred beliefs are in jeopardy: we deliver sermons to protect and promote our ideals. We enter prosecutor mode when we recognize flaws in other people’s reasoning: we marshal arguments to prove them wrong and win our case. We shift into politician mode when we’re seeking to win over an audience: we campaign and lobby for the approval of our constituents. The risk is that we become so wrapped up in preaching that we’re right, prosecuting others who are wrong, and politicking for support that we don’t bother to rethink our own views. When Stephen Greenspan and his sister made the choice to invest with ","date":"2022-09-03","objectID":"/b90_think_again/:2:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"A DIFFERENT PAIR OF GOGGLES If you’re a scientist by trade, rethinking is fundamental to your profession. You’re paid to be constantly aware of the limits of your understanding. You’re expected to doubt what you know, be curious about what you don’t know, and update your views based on new data. In the past century alone, the application of scientific principles has led to dramatic progress. Biological scientists discovered penicillin. Rocket scientists sent us to the moon. Computer scientists built the internet. But being a scientist is not just a profession. It’s a frame of mind—a mode of thinking that differs from preaching, prosecuting, and politicking. We move into scientist mode when we’re searching for the truth: we run experiments to test hypotheses and discover knowledge. Scientific tools aren’t reserved for people with white coats and beakers, and using them doesn’t require toiling away for years with a microscope and a petri dish. Hypotheses have as much of a place in our lives as they do in the lab. Experiments can inform our daily decisions. That makes me wonder: is it possible to train people in other fields to think more like scientists, and if so, do they end up making smarter choices? Recently, a quartet of European researchers decided to find out. They ran a bold experiment with more than a hundred founders of Italian startups in technology, retail, furniture, food, health care, leisure, and machinery. Most of the founders’ businesses had yet to bring in any revenue, making it an ideal setting to investigate how teaching scientific thinking would influence the bottom line. The entrepreneurs arrived in Milan for a training program in entrepreneurship. Over the course of four months, they learned to create a business strategy, interview customers, build a minimum viable product, and then refine a prototype. What they didn’t know was that they’d been randomly assigned to either a “scientific thinking” group or a control group. The training for both groups was identical, except that one was encouraged to view startups through a scientist’s goggles. From that perspective, their strategy is a theory, customer interviews help to develop hypotheses, and their minimum viable product and prototype are experiments to test those hypotheses. Their task is to rigorously measure the results and make decisions based on whether their hypotheses are supported or refuted. Over the following year, the startups in the control group averaged under $300 in revenue. The startups in the scientific thinking group averaged over $12,000 in revenue. They brought in revenue more than twice as fast—and attracted customers sooner, too. Why? The entrepreneurs in the control group tended to stay wedded to their original strategies and products. It was too easy to preach the virtues of their past decisions, prosecute the vices of alternative options, and politick by catering to advisers who favored the existing direction. The entrepreneurs who had been taught to think like scientists, in contrast, pivoted more than twice as often. When their hypotheses weren’t supported, they knew it was time to rethink their business models. What’s surprising about these results is that we typically celebrate great entrepreneurs and leaders for being strong-minded and clear-sighted. They’re supposed to be paragons of conviction: decisive and certain. Yet evidence reveals that when business executives compete in tournaments to price products, the best strategists are actually slow and unsure. Like careful scientists, they take their time so they have the flexibility to change their minds. I’m beginning to think decisiveness is overrated . . . but I reserve the right to change my mind. Just as you don’t have to be a professional scientist to reason like one, being a professional scientist doesn’t guarantee that someone will use the tools of their training. Scientists morph into preachers when they present their pet theories as gospel and treat thoughtful critiq","date":"2022-09-03","objectID":"/b90_think_again/:2:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE SMARTER THEY ARE, THE HARDER THEY FAIL Mental horsepower doesn’t guarantee mental dexterity. No matter how much brainpower you have, if you lack the motivation to change your mind, you’ll miss many occasions to think again. Research reveals that the higher you score on an IQ test, the more likely you are to fall for stereotypes, because you’re faster at recognizing patterns. And recent experiments suggest that the smarter you are, the more you might struggle to update your beliefs. One study investigated whether being a math whiz makes you better at analyzing data. The answer is yes—if you’re told the data are about something bland, like a treatment for skin rashes. But what if the exact same data are labeled as focusing on an ideological issue that activates strong emotions—like gun laws in the United States? Being a quant jock makes you more accurate in interpreting the results—as long as they support your beliefs. Yet if the empirical pattern clashes with your ideology, math prowess is no longer an asset; it actually becomes a liability. The better you are at crunching numbers, the more spectacularly you fail at analyzing patterns that contradict your views. If they were liberals, math geniuses did worse than their peers at evaluating evidence that gun bans failed. If they were conservatives, they did worse at assessing evidence that gun bans worked. In psychology there are at least two biases that drive this pattern. One is confirmation bias: seeing what we expect to see. The other is desirability bias: seeing what we want to see. These biases don’t just prevent us from applying our intelligence. They can actually contort our intelligence into a weapon against the truth. We find reasons to preach our faith more deeply, prosecute our case more passionately, and ride the tidal wave of our political party. The tragedy is that we’re usually unaware of the resulting flaws in our thinking. My favorite bias is the “I’m not biased” bias, in which people believe they’re more objective than others. It turns out that smart people are more likely to fall into this trap. The brighter you are, the harder it can be to see your own limitations. Being good at thinking can make you worse at rethinking. When we’re in scientist mode, we refuse to let our ideas become ideologies. We don’t start with answers or solutions; we lead with questions and puzzles. We don’t preach from intuition; we teach from evidence. We don’t just have healthy skepticism about other people’s arguments; we dare to disagree with our own arguments. Thinking like a scientist involves more than just reacting with an open mind. It means being actively open-minded. It requires searching for reasons why we might be wrong—not for reasons why we must be right—and revising our views based on what we learn. That rarely happens in the other mental modes. In preacher mode, changing our minds is a mark of moral weakness; in scientist mode, it’s a sign of intellectual integrity. In prosecutor mode, allowing ourselves to be persuaded is admitting defeat; in scientist mode, it’s a step toward the truth. In politician mode, we flip-flop in response to carrots and sticks; in scientist mode, we shift in the face of sharper logic and stronger data. I’ve done my best to write this book in scientist mode.* I’m a teacher, not a preacher. I can’t stand politics, and I hope a decade as a tenured professor has cured me of whatever temptation I once felt to appease my audience. Although I’ve spent more than my share of time in prosecutor mode, I’ve decided that in a courtroom I’d rather be the judge. I don’t expect you to agree with everything I think. My hope is that you’ll be intrigued by how I think—and that the studies, stories, and ideas covered here will lead you to do some rethinking of your own. After all, the purpose of learning isn’t to affirm our beliefs; it’s to evolve our beliefs. One of my beliefs is that we shouldn’t be open-minded in every circumstance. There are situations ","date":"2022-09-03","objectID":"/b90_think_again/:2:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"DON’T STOP UNBELIEVING As I’ve studied the process of rethinking, I’ve found that it often unfolds in a cycle. It starts with intellectual humility—knowing what we don’t know. We should all be able to make a long list of areas where we’re ignorant. Mine include art, financial markets, fashion, chemistry, food, why British accents turn American in songs, and why it’s impossible to tickle yourself. Recognizing our shortcomings opens the door to doubt. As we question our current understanding, we become curious about what information we’re missing. That search leads us to new discoveries, which in turn maintain our humility by reinforcing how much we still have to learn. If knowledge is power, knowing what we don’t know is wisdom. Scientific thinking favors humility over pride, doubt over certainty, curiosity over closure. When we shift out of scientist mode, the rethinking cycle breaks down, giving way to an overconfidence cycle. If we’re preaching, we can’t see gaps in our knowledge: we believe we’ve already found the truth. Pride breeds conviction rather than doubt, which makes us prosecutors: we might be laser-focused on changing other people’s minds, but ours is set in stone. That launches us into confirmation bias and desirability bias. We become politicians, ignoring or dismissing whatever doesn’t win the favor of our constituents—our parents, our bosses, or the high school classmates we’re still trying to impress. We become so busy putting on a show that the truth gets relegated to a backstage seat, and the resulting validation can make us arrogant. We fall victim to the fat-cat syndrome, resting on our laurels instead of pressure-testing our beliefs. In the case of the BlackBerry, Mike Lazaridis was trapped in an overconfidence cycle. Taking pride in his successful invention gave him too much conviction. Nowhere was that clearer than in his preference for the keyboard over a touchscreen. It was a BlackBerry virtue he loved to preach—and an Apple vice he was quick to prosecute. As his company’s stock fell, Mike got caught up in confirmation bias and desirability bias, and fell victim to validation from fans. “It’s an iconic product,” he said of the BlackBerry in 2011. “It’s used by business, it’s used by leaders, it’s used by celebrities.” By 2012, the iPhone had captured a quarter of the global smartphone market, but Mike was still resisting the idea of typing on glass. “I don’t get this,” he said at a board meeting, pointing at a phone with a touchscreen. “The keyboard is one of the reasons they buy BlackBerrys.” Like a politician who campaigns only to his base, he focused on the keyboard taste of millions of existing users, neglecting the appeal of a touchscreen to billions of potential users. For the record, I still miss the keyboard, and I’m excited that it’s been licensed for an attempted comeback. When Mike finally started reimagining the screen and software, some of his engineers didn’t want to abandon their past work. The failure to rethink was widespread. In 2011, an anonymous high-level employee inside the firm wrote an open letter to Mike and his co-CEO. “We laughed and said they are trying to put a computer on a phone, that it won’t work,” the letter read. “We are now 3–4 years too late.” Our convictions can lock us in prisons of our own making. The solution is not to decelerate our thinking—it’s to accelerate our rethinking. That’s what resurrected Apple from the brink of bankruptcy to become the world’s most valuable company. The legend of Apple’s renaissance revolves around the lone genius of Steve Jobs. It was his conviction and clarity of vision, the story goes, that gave birth to the iPhone. The reality is that he was dead-set against the mobile phone category. His employees had the vision for it, and it was their ability to change his mind that really revived Apple. Although Jobs knew how to “think different,” it was his team that did much of the rethinking. In 2004, a small group of engineers, designe","date":"2022-09-03","objectID":"/b90_think_again/:2:4","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 2 ","date":"2022-09-03","objectID":"/b90_think_again/:3:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"The Armchair Quarterback and the Impostor ","date":"2022-09-03","objectID":"/b90_think_again/:4:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Finding the Sweet Spot of Confidence Ignorance more frequently begets confidence than does knowledge. —charles darwin When Ursula Mercz was admitted to the clinic, she complained of headaches, back pain, and dizziness severe enough that she could no longer work. Over the following month her condition deteriorated. She struggled to locate the glass of water she put next to her bed. She couldn’t find the door to her room. She walked directly into her bed frame. Ursula was a seamstress in her midfifties, and she hadn’t lost her dexterity: she was able to cut different shapes out of paper with scissors. She could easily point to her nose, mouth, arms, and legs, and had no difficulty describing her home and her pets. For an Austrian doctor named Gabriel Anton, she presented a curious case. When Anton put a red ribbon and scissors on the table in front of her, she couldn’t name them, even though “she confirmed, calmly and faithfully, that she could see the presented objects.” She was clearly having problems with language production, which she acknowledged, and with spatial orientation. Yet something else was wrong: Ursula could no longer tell the difference between light and dark. When Anton held up an object and asked her to describe it, she didn’t even try to look at it but instead reached out to touch it. Tests showed that her eyesight was severely impaired. Oddly, when Anton asked her about the deficit, she insisted she could see. Eventually, when she lost her vision altogether, she remained completely unaware of it. “It was now extremely astonishing,” Anton wrote, “that the patient did not notice her massive and later complete loss of her ability to see . . . she was mentally blind to her blindness.” It was the late 1800s, and Ursula wasn’t alone. A decade earlier a neuropathologist in Zurich had reported a case of a man who suffered an accident that left him blind but was unaware of it despite being “intellectually unimpaired.” Although he didn’t blink when a fist was placed in front of his face and couldn’t see the food on his plate, “he thought he was in a dark humid hole or cellar.” Half a century later, a pair of doctors reported six cases of people who had gone blind but claimed otherwise. “One of the most striking features in the behavior of our patients was their inability to learn from their experiences,” the doctors wrote: As they were not aware of their blindness when they walked about, they bumped into the furniture and walls but did not change their behavior. When confronted with their blindness in a rather pointed fashion, they would either deny any visual difficulty or remark: “It is so dark in the room; why don’t they turn the light on?”; “I forgot my glasses,” or “My vision is not too good, but I can see all right.” The patients would not accept any demonstration or assurance which would prove their blindness. This phenomenon was first described by the Roman philosopher Seneca, who wrote of a woman who was blind but complained that she was simply in a dark room. It’s now accepted in the medical literature as Anton’s syndrome—a deficit of self-awareness in which a person is oblivious to a physical disability but otherwise doing fairly well cognitively. It’s known to be caused by damage to the occipital lobe of the brain. Yet I’ve come to believe that even when our brains are functioning normally, we’re all vulnerable to a version of Anton’s syndrome. We all have blind spots in our knowledge and opinions. The bad news is that they can leave us blind to our blindness, which gives us false confidence in our judgment and prevents us from rethinking. The good news is that with the right kind of confidence, we can learn to see ourselves more clearly and update our views. In driver’s training we were taught to identify our visual blind spots and eliminate them with the help of mirrors and sensors. In life, since our minds don’t come equipped with those tools, we need to learn to recognize our cognitive blind spots and ","date":"2022-09-03","objectID":"/b90_think_again/:5:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"A TALE OF TWO SYNDROMES On the first day of December 2015, Halla Tómasdóttir got a call she never expected. The roof of Halla’s house had just given way to a thick layer of snow and ice. As she watched water pouring down one of the walls, the friend on the other end of the line asked if Halla had seen the Facebook posts about her. Someone had started a petition for Halla to run for the presidency of Iceland. Halla’s first thought was, Who am I to be president? She had helped start a university and then cofounded an investment firm in 2007. When the 2008 financial crisis rocked the world, Iceland was hit particularly hard; all three of its major private commercial banks defaulted and its currency collapsed. Relative to the size of its economy, the country faced the worst financial meltdown in human history, but Halla demonstrated her leadership skills by guiding her firm successfully through the crisis. Even with that accomplishment, she didn’t feel prepared for the presidency. She had no political background; she had never served in government or in any kind of public-sector role. It wasn’t the first time Halla had felt like an impostor. At the age of eight, her piano teacher had placed her on a fast track and frequently asked her to play in concerts, but she never felt she was worthy of the honor—and so, before every concert, she felt sick. Although the stakes were much higher now, the self-doubt felt familiar. “I had a massive pit in my stomach, like the piano recital but much bigger,” Halla told me. “It’s the worst case of adult impostor syndrome I’ve ever had.” For months, she struggled with the idea of becoming a candidate. As her friends and family encouraged her to recognize that she had some relevant skills, Halla was still convinced that she lacked the necessary experience and confidence. She tried to persuade other women to run—one of whom ended up ascending to a different office, as the prime minister of Iceland. Yet the petition didn’t go away, and Halla’s friends, family, and colleagues didn’t stop urging her on. Eventually, she found herself asking, Who am I not to serve? She ultimately decided to go for it, but the odds were heavily stacked against her. She was running as an unknown independent candidate in a field of more than twenty contenders. One of her competitors was particularly powerful—and particularly dangerous. When an economist was asked to name the three people most responsible for Iceland’s bankruptcy, she nominated Davíð Oddsson for all three spots. As Iceland’s prime minister from 1991 to 2004, Oddsson put the country’s banks in jeopardy by privatizing them. Then, as governor of Iceland’s central bank from 2005 to 2009, he allowed the banks’ balance sheets to balloon to more than ten times the national GDP. When the people protested his mismanagement, Oddsson refused to resign and had to be forced out by Parliament. Time magazine later identified him as one of the twenty-five people to blame for the financial crisis worldwide. Nevertheless, in 2016 Oddsson announced his candidacy for the presidency of Iceland: “My experience and knowledge, which is considerable, could go well with this office.” In theory, confidence and competence go hand in hand. In practice, they often diverge. You can see it when people rate their own leadership skills and are also evaluated by their colleagues, supervisors, or subordinates. In a meta-analysis of ninety-five studies involving over a hundred thousand people, women typically underestimated their leadership skills, while men overestimated their skills. You’ve probably met some football fans who are convinced they know more than the coaches on the sidelines. That’s the armchair quarterback syndrome, where confidence exceeds competence. Even after calling financial plays that destroyed an economy, Davíð Oddsson still refused to acknowledge that he wasn’t qualified to coach—let alone quarterback. He was blind to his weaknesses. Jason Adam Katzenstein/The New Yorker ","date":"2022-09-03","objectID":"/b90_think_again/:5:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE IGNORANCE OF ARROGANCE One of my favorite accolades is a satirical award for research that’s as entertaining as it is enlightening. It’s called the Ig™ Nobel Prize, and it’s handed out by actual Nobel laureates. One autumn in college, I raced to the campus theater to watch the ceremony along with over a thousand fellow nerds. The winners included a pair of physicists who created a magnetic field to levitate a live frog, a trio of chemists who discovered that the biochemistry of romantic love has something in common with obsessive-compulsive disorder, and a computer scientist who invented PawSense—software that detects cat paws on a keyboard and makes an annoying noise to deter them. Unclear whether it also worked with dogs. Several of the awards made me laugh, but the honorees who made me think the most were two psychologists, David Dunning and Justin Kruger. They had just published a “modest report” on skill and confidence that would soon become famous. They found that in many situations, those who can’t . . . don’t know they can’t. According to what’s now known as the Dunning-Kruger effect, it’s when we lack competence that we’re most likely to be brimming with overconfidence. In the original Dunning-Kruger studies, people who scored the lowest on tests of logical reasoning, grammar, and sense of humor had the most inflated opinions of their skills. On average, they believed they did better than 62 percent of their peers, but in reality outperformed only 12 percent of them. The less intelligent we are in a particular domain, the more we seem to overestimate our actual intelligence in that domain. In a group of football fans, the one who knows the least is the most likely to be the armchair quarterback, prosecuting the coach for calling the wrong play and preaching about a better playbook. This tendency matters because it compromises self-awareness, and it trips us up across all kinds of settings. Look what happened when economists evaluated the operations and management practices of thousands of companies across a wide range of industries and countries, and compared their assessments with managers’ self-ratings: Sources: World Management Survey; Bloom and Van Reenen 2007; and Maloney 2017b. In this graph, if self-assessments of performance matched actual performance, every country would be on the dotted line. Overconfidence existed in every culture, and it was most rampant where management was the poorest.* Of course, management skills can be hard to judge objectively. Knowledge should be easier—you were tested on yours throughout school. Compared to most people, how much do you think you know about each of the following topics—more, less, or the same? Why English became the official language of the United States Why women were burned at the stake in Salem What job Walt Disney had before he drew Mickey Mouse On which spaceflight humans first laid eyes on the Great Wall of China Why eating candy affects how kids behave One of my biggest pet peeves is feigned knowledge, where people pretend to know things they don’t. It bothers me so much that at this very moment I’m writing an entire book about it. In a series of studies, people rated whether they knew more or less than most people about a range of topics like these, and then took a quiz to test their actual knowledge. The more superior participants thought their knowledge was, the more they overestimated themselves—and the less interested they were in learning and updating. If you think you know more about history or science than most people, chances are you know less than you think. As Dunning quips, “The first rule of the Dunning-Kruger club is you don’t know you’re a member of the Dunning-Kruger club.”* On the questions above, if you felt you knew anything at all, think again. America has no official language, suspected witches were hanged in Salem but not burned, Walt Disney didn’t draw Mickey Mouse (it was the work of an animator named Ub Iwerks), you can’t actually","date":"2022-09-03","objectID":"/b90_think_again/:5:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"STRANDED AT THE SUMMIT OF MOUNT STUPID The problem with armchair quarterback syndrome is that it stands in the way of rethinking. If we’re certain that we know something, we have no reason to look for gaps and flaws in our knowledge—let alone fill or correct them. In one study, the people who scored the lowest on an emotional intelligence test weren’t just the most likely to overestimate their skills. They were also the most likely to dismiss their scores as inaccurate or irrelevant—and the least likely to invest in coaching or self-improvement. Yes, some of this comes down to our fragile egos. We’re driven to deny our weaknesses when we want to see ourselves in a positive light or paint a glowing picture of ourselves to others. A classic case is the crooked politician who claims to crusade against corruption, but is actually motivated by willful blindness or social deception. Yet motivation is only part of the story.* There’s a less obvious force that clouds our vision of our abilities: a deficit in metacognitive skill, the ability to think about our thinking. Lacking competence can leave us blind to our own incompetence. If you’re a tech entrepreneur and you’re uninformed about education systems, you can feel certain that your master plan will fix them. If you’re socially awkward and you’re missing some insight on social graces, you can strut around believing you’re James Bond. In high school, a friend told me I didn’t have a sense of humor. What made her think that? “You don’t laugh at all my jokes.” I’m hilarious . . . said no funny person ever. I’ll leave it to you to decide who lacked the sense of humor. When we lack the knowledge and skills to achieve excellence, we sometimes lack the knowledge and skills to judge excellence. This insight should immediately put your favorite confident ignoramuses in their place. Before we poke fun at them, though, it’s worth remembering that we all have moments when we are them. We’re all novices at many things, but we’re not always blind to that fact. We tend to overestimate ourselves on desirable skills, like the ability to carry on a riveting conversation. We’re also prone to overconfidence in situations where it’s easy to confuse experience for expertise, like driving, typing, trivia, and managing emotions. Yet we underestimate ourselves when we can easily recognize that we lack experience—like painting, driving a race car, and rapidly reciting the alphabet backward. Absolute beginners rarely fall into the Dunning-Kruger trap. If you don’t know a thing about football, you probably don’t walk around believing you know more than the coach. It’s when we progress from novice to amateur that we become overconfident. A bit of knowledge can be a dangerous thing. In too many domains of our lives, we never gain enough expertise to question our opinions or discover what we don’t know. We have just enough information to feel self-assured about making pronouncements and passing judgment, failing to realize that we’ve climbed to the top of Mount Stupid without making it over to the other side. You can see this phenomenon in one of Dunning’s experiments that involved people playing the role of doctors in a simulated zombie apocalypse. When they’ve seen only a handful of injured victims, their perceived and actual skills match. Unfortunately, as they gain experience, their confidence climbs faster than their competence, and confidence remains higher than competence from that point on. This might be one of the reasons that patient mortality rates in hospitals seem to spike in July, when new residents take over. It’s not their lack of skill alone that proves hazardous; it’s their overestimation of that skill. Advancing from novice to amateur can break the rethinking cycle. As we gain experience, we lose some of our humility. We take pride in making rapid progress, which promotes a false sense of mastery. That jump-starts an overconfidence cycle, preventing us from doubting what we know and being cur","date":"2022-09-03","objectID":"/b90_think_again/:5:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"WHAT GOLDILOCKS GOT WRONG Many people picture confidence as a seesaw. Gain too much confidence, and we tip toward arrogance. Lose too much confidence, and we become meek. This is our fear with humility: that we’ll end up having a low opinion of ourselves. We want to keep the seesaw balanced, so we go into Goldilocks mode and look for the amount of confidence that’s just right. Recently, though, I learned that this is the wrong approach. Humility is often misunderstood. It’s not a matter of having low self-confidence. One of the Latin roots of humility means “from the earth.” It’s about being grounded—recognizing that we’re flawed and fallible. Confidence is a measure of how much you believe in yourself. Evidence shows that’s distinct from how much you believe in your methods. You can be confident in your ability to achieve a goal in the future while maintaining the humility to question whether you have the right tools in the present. That’s the sweet spot of confidence. We become blinded by arrogance when we’re utterly convinced of our strengths and our strategies. We get paralyzed by doubt when we lack conviction in both. We can be consumed by an inferiority complex when we know the right method but feel uncertain about our ability to execute it. What we want to attain is confident humility: having faith in our capability while appreciating that we may not have the right solution or even be addressing the right problem. That gives us enough doubt to reexamine our old knowledge and enough confidence to pursue new insights. When Spanx founder Sara Blakely had the idea for footless pantyhose, she believed in her ability to make the idea a reality, but she was full of doubt about her current tools. Her day job was selling fax machines door-to-door, and she was aware that she didn’t know anything about fashion, retail, or manufacturing. When she was designing the prototype, she spent a week driving around to hosiery mills to ask them for help. When she couldn’t afford a law firm to apply for a patent, she read a book on the topic and filled out the application herself. Her doubt wasn’t debilitating—she was confident she could overcome the challenges in front of her. Her confidence wasn’t in her existing knowledge—it was in her capacity to learn. Confident humility can be taught. In one experiment, when students read a short article about the benefits of admitting what we don’t know rather than being certain about it, their odds of seeking extra help in an area of weakness spiked from 65 to 85 percent. They were also more likely to explore opposing political views to try to learn from the other side. Confident humility doesn’t just open our minds to rethinking—it improves the quality of our rethinking. In college and graduate school, students who are willing to revise their beliefs get higher grades than their peers. In high school, students who admit when they don’t know something are rated by teachers as learning more effectively and by peers as contributing more to their teams. At the end of the academic year, they have significantly higher math grades than their more self-assured peers. Instead of just assuming they’ve mastered the material, they quiz themselves to test their understanding. When adults have the confidence to acknowledge what they don’t know, they pay more attention to how strong evidence is and spend more time reading material that contradicts their opinions. In rigorous studies of leadership effectiveness across the United States and China, the most productive and innovative teams aren’t run by leaders who are confident or humble. The most effective leaders score high in both confidence and humility. Although they have faith in their strengths, they’re also keenly aware of their weaknesses. They know they need to recognize and transcend their limits if they want to push the limits of greatness. If we care about accuracy, we can’t afford to have blind spots. To get an accurate picture of our knowledge and skill","date":"2022-09-03","objectID":"/b90_think_again/:5:4","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE BENEFITS OF DOUBT Just a month and a half before Iceland’s presidential election, Halla Tómasdóttir was polling at only 1 percent support. To focus on the most promising candidates, the network airing the first televised debate announced that they wouldn’t feature anyone with less than 2.5 percent of the vote. On the day of the debate, Halla ended up barely squeaking through. Over the following month her popularity skyrocketed. She wasn’t just a viable candidate; she was in the final four. A few years later, when I invited her to speak to my class, Halla mentioned that the psychological fuel that propelled her meteoric rise was none other than impostor syndrome. Feeling like an impostor is typically viewed as a bad thing, and for good reason—a chronic sense of being unworthy can breed misery, crush motivation, and hold us back from pursuing our ambitions. From time to time, though, a less crippling sense of doubt waltzes into many of our minds. Some surveys suggest that more than half the people you know have felt like impostors at some point in their careers. It’s thought to be especially common among women and marginalized groups. Strangely, it also seems to be particularly pronounced among high achievers. I’ve taught students who earned patents before they could drink and became chess masters before they could drive, but these same individuals still wrestle with insecurity and constantly question their abilities. The standard explanation for their accomplishments is that they succeed in spite of their doubts, but what if their success is actually driven in part by those doubts? To find out, Basima Tewfik—then a doctoral student at Wharton, now an MIT professor—recruited a group of medical students who were preparing to begin their clinical rotations. She had them interact for more than half an hour with actors who had been trained to play the role of patients presenting symptoms of various diseases. Basima observed how the medical students treated the patients—and also tracked whether they made the right diagnoses. A week earlier the students had answered a survey about how often they entertained impostor thoughts like I am not as qualified as others think I am and People important to me think I am more capable than I think I am. Those who self-identified as impostors didn’t do any worse in their diagnoses, and they did significantly better when it came to bedside manner—they were rated as more empathetic, respectful, and professional, as well as more effective in asking questions and sharing information. In another study, Basima found a similar pattern with investment professionals: the more often they felt like impostors, the higher their performance reviews from their supervisors four months later. This evidence is new, and we still have a lot to learn about when impostor syndrome is beneficial versus when it’s detrimental. Still, it leaves me wondering if we’ve been misjudging impostor syndrome by seeing it solely as a disorder. When our impostor fears crop up, the usual advice is to ignore them—give ourselves the benefit of the doubt. Instead, we might be better off embracing those fears, because they can give us three benefits of doubt. The first upside of feeling like an impostor is that it can motivate us to work harder. It’s probably not helpful when we’re deciding whether to start a race, but once we’ve stepped up to the starting line, it gives us the drive to keep running to the end so that we can earn our place among the finalists.* In some of my own research across call centers, military and government teams, and nonprofits, I’ve found that confidence can make us complacent. If we never worry about letting other people down, we’re more likely to actually do so. When we feel like impostors, we think we have something to prove. Impostors may be the last to jump in, but they may also be the last to bail out. Second, impostor thoughts can motivate us to work smarter. When we don’t believe we’re going to win, we","date":"2022-09-03","objectID":"/b90_think_again/:5:5","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE LEAGUE OF EXTRAORDINARY HUMILITY When I sat down with Halla, she told me that in the past her doubts had been debilitating. She took them as a sign that she lacked the ability to succeed. Now she had reached a point of confident humility, and she interpreted doubts differently: they were a cue that she needed to improve her tools. Plenty of evidence suggests that confidence is just as often the result of progress as the cause of it. We don’t have to wait for our confidence to rise to achieve challenging goals. We can build it through achieving challenging goals. “I have come to welcome impostor syndrome as a good thing: it’s fuel to do more, try more,” Halla says. “I’ve learned to use it to my advantage. I actually thrive on the growth that comes from the self-doubt.” While other candidates were content to rely on the usual media coverage, Halla’s uncertainty about her tools made her eager to rethink the way campaigns were run. She worked harder and smarter, staying up late to personally answer social media messages. She held Facebook Live sessions where voters could ask her anything, and learned to use Snapchat to reach young people. Deciding she had nothing to lose, she went where few presidential candidates had gone before: instead of prosecuting her opponents, she ran a positive campaign. How much worse can it get? she thought. It was part of why she resonated so strongly with voters: they were tired of watching candidates smear one another and delighted to see a candidate treat her competitors with respect. Uncertainty primes us to ask questions and absorb new ideas. It protects us against the Dunning-Kruger effect. “Impostor syndrome always keeps me on my toes and growing because I never think I know it all,” Halla reflects, sounding more like a scientist than a politician. “Maybe impostor syndrome is needed for change. Impostors rarely say, ‘This is how we do things around here.’ They don’t say, ‘This is the right way.’ I was so eager to learn and grow that I asked everyone for advice on how I could do things differently.” Although she doubted her tools, she had confidence in herself as a learner. She understood that knowledge is best sought from experts, but creativity and wisdom can come from anywhere. Iceland’s presidential election came down to Halla, Davíð Oddsson, and two other men. The three men all enjoyed more media coverage than Halla throughout the campaign, including front-page interviews, which she never received. They also had bigger campaign budgets. Yet on election day, Halla stunned her country—and herself—by winning more than a quarter of the vote. She didn’t land the presidency; she came in second. Her 28 percent fell shy of the victor’s 39 percent. But Halla trounced Davíð Oddsson, who finished fourth, with less than 14 percent. Based on her trajectory and momentum, it’s not crazy to imagine that with a few more weeks, she could have won. Great thinkers don’t harbor doubts because they’re impostors. They maintain doubts because they know we’re all partially blind and they’re committed to improving their sight. They don’t boast about how much they know; they marvel at how little they understand. They’re aware that each answer raises new questions, and the quest for knowledge is never finished. A mark of lifelong learners is recognizing that they can learn something from everyone they meet. Arrogance leaves us blind to our weaknesses. Humility is a reflective lens: it helps us see them clearly. Confident humility is a corrective lens: it enables us to overcome those weaknesses. ","date":"2022-09-03","objectID":"/b90_think_again/:5:6","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 3 ","date":"2022-09-03","objectID":"/b90_think_again/:6:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"The Joy of Being Wrong ","date":"2022-09-03","objectID":"/b90_think_again/:7:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"The Thrill of Not Believing Everything You Think I have a degree from Harvard. Whenever I’m wrong, the world makes a little less sense. —dr. frasier crane, played by kelsey grammer In the fall of 1959, a prominent psychologist welcomed new participants into a wildly unethical study. He had handpicked a group of Harvard sophomores to join a series of experiments that would run through the rest of their time in college. The students volunteered to spend a couple of hours a week contributing to knowledge about how personality develops and how psychological problems can be solved. They had no idea that they were actually signing up to have their beliefs attacked. The researcher, Henry Murray, had originally trained as a physician and biochemist. After becoming a distinguished psychologist, he was disillusioned that his field paid little attention to how people navigate difficult interactions, so he decided to create them in his own lab. He gave students a month to write out their personal philosophy of life, including their core values and guiding principles. When they showed up to submit their work, they were paired with another student who had done the same exercise. They would have a day or two to read each other’s philosophies, and then they would be filmed debating them. The experience would be much more intense than they anticipated. Murray modeled the study on psychological assessments he had developed for spies in World War II. As a lieutenant colonel, Murray had been recruited to vet potential agents for the Office of Strategic Services, the precursor to the CIA. To gauge how candidates would handle pressure, he sent them down to a basement to be interrogated with a bright light shining in their faces. The examiner would wait for an inconsistency in their accounts to pop up and then scream, “You’re a liar!” Some candidates quit on the spot; others were reduced to tears. Those who withstood the onslaught got the gig. Now Murray was ready for a more systematic study of reactions to stress. He had carefully screened students to create a sample that included a wide range of personalities and mental health profiles. He gave them code names based on their character traits, including Drill, Quartz, Locust, Hinge, and Lawful—more on him later. When students arrived for the debate, they discovered that their sparring partner was not a peer but a law student. What they didn’t know was that the law student was in cahoots with the research team: his task was to spend eighteen minutes launching an aggressive assault on their worldviews. Murray called it a “stressful interpersonal disputation,” having directed the law student to make the participants angry and anxious with a “mode of attack” that was “vehement, sweeping, and personally abusive.” The poor students sweated and shouted as they struggled to defend their ideals. The pain didn’t stop there. In the weeks that followed, the students were invited back to the lab to discuss the films of their own interactions. They watched themselves grimacing and stringing together incoherent sentences. All in all, they spent about eight hours reliving those humiliating eighteen minutes. A quarter century later, when the participants reflected on the experience, it was clear that many had found it agonizing. Drill described feeling “unabating rage.” Locust recalled his bewilderment, anger, chagrin, and discomfort. “They have deceived me, telling me there was going to be a discussion, when in fact there was an attack,” he wrote. “How could they have done this to me; what is the point of this?” Other participants had a strikingly different response: they actually seemed to get a kick out of being forced to rethink their beliefs. “Some may have found the experience mildly discomforting, in that their cherished (and in my case, at least, sophomoric) philosophies were challenged in an aggressive manner,” one participant remembers. “But it was hardly an experience that would blight one for a week, le","date":"2022-09-03","objectID":"/b90_think_again/:8:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE DICTATOR POLICING YOUR THOUGHTS When our son was five, he was excited to learn that his uncle was expecting a child. My wife and I both predicted a boy, and so did our son. A few weeks later, we found out the baby would be a girl. When we broke the news to our son, he burst into tears. “Why are you crying?” I asked. “Is it because you were hoping your new cousin would be a boy?” “No!” he shouted, pounding his fists on the floor. “Because we were wrong!” I explained that being wrong isn’t always a bad thing. It can be a sign that we’ve learned something new—and that discovery itself can be a delight. This realization didn’t come naturally to me. Growing up, I was determined to be right. In second grade I corrected my teacher for misspelling the word lightning as lightening. When trading baseball cards I would rattle off statistics from recent games as proof that the price guide was valuing players inaccurately. My friends found this annoying and started calling me Mr. Facts. It got so bad that one day my best friend announced that he wouldn’t talk to me until I admitted I was wrong. It was the beginning of my journey to become more accepting of my own fallibility. In a classic paper, sociologist Murray Davis argued that when ideas survive, it’s not because they’re true—it’s because they’re interesting. What makes an idea interesting is that it challenges our weakly held opinions. Did you know that the moon might originally have formed inside a vaporous Earth out of magma rain? That a narwhal’s tusk is actually a tooth? When an idea or assumption doesn’t matter deeply to us, we’re often excited to question it. The natural sequence of emotions is surprise (“Really?”) followed by curiosity (“Tell me more!”) and thrill (“Whoa!”). To paraphrase a line attributed to Isaac Asimov, great discoveries often begin not with “Eureka!” but with “That’s funny . . .” When a core belief is questioned, though, we tend to shut down rather than open up. It’s as if there’s a miniature dictator living inside our heads, controlling the flow of facts to our minds, much like Kim Jong-un controls the press in North Korea. The technical term for this in psychology is the totalitarian ego, and its job is to keep out threatening information. It’s easy to see how an inner dictator comes in handy when someone attacks our character or intelligence. Those kinds of personal affronts threaten to shatter aspects of our identities that are important to us and might be difficult to change. The totalitarian ego steps in like a bodyguard for our minds, protecting our self-image by feeding us comforting lies. They’re all just jealous. You’re really, really, ridiculously good-looking. You’re on the verge of inventing the next Pet Rock. As physicist Richard Feynman quipped, “You must not fool yourself—and you are the easiest person to fool.” Our inner dictator also likes to take charge when our deeply held opinions are threatened. In the Harvard study of attacking students’ worldviews, the participant who had the strongest negative reaction was code-named Lawful. He came from a blue-collar background and was unusually precocious, having started college at sixteen and joined the study at seventeen. One of his beliefs was that technology was harming civilization, and he became hostile when his views were questioned. Lawful went on to become an academic, and when he penned his magnum opus, it was clear that he hadn’t changed his mind. His concerns about technology had only intensified: The Industrial Revolution and its consequences have been a disaster for the human race. They have greatly increased the life-expectancy of those of us who live in “advanced” countries, but they have destabilized society, have made life unfulfilling, have subjected human beings to indignities . . . to physical suffering as well . . . and have inflicted severe damage on the natural world. That kind of conviction is a common response to threats. Neuroscientists find that when our core belie","date":"2022-09-03","objectID":"/b90_think_again/:8:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"ATTACHMENT ISSUES Not long ago I gave a speech at a conference about my research on givers, takers, and matchers. I was studying whether generous, selfish, or fair people were more productive in jobs like sales and engineering. One of the attendees was Daniel Kahneman, the Nobel Prize–winning psychologist who has spent much of his career demonstrating how flawed our intuitions are. He told me afterward that he was surprised by my finding that givers had higher rates of failure than takers and matchers—but higher rates of success, too. When you read a study that surprises you, how do you react? Many people would get defensive, searching for flaws in the study’s design or the statistical analysis. Danny did the opposite. His eyes lit up, and a huge grin appeared on his face. “That was wonderful,” he said. “I was wrong.” Later, I sat down with Danny for lunch and asked him about his reaction. It looked a lot to me like the joy of being wrong—his eyes twinkled as if he was having fun. He said that in his eighty-five years, no one had pointed that out before, but yes, he genuinely enjoys discovering that he was wrong, because it means he is now less wrong than before. I knew the feeling. In college, what first attracted me to social science was reading studies that clashed with my expectations; I couldn’t wait to tell my roommates about all the assumptions I’d been rethinking. In my first independent research project, I tested some predictions of my own, and more than a dozen of my hypotheses turned out to be false.* It was a major lesson in intellectual humility, but I wasn’t devastated. I felt an immediate rush of excitement. Discovering I was wrong felt joyful because it meant I’d learned something. As Danny told me, “Being wrong is the only way I feel sure I’ve learned anything.” Danny isn’t interested in preaching, prosecuting, or politicking. He’s a scientist devoted to the truth. When I asked him how he stays in that mode, he said he refuses to let his beliefs become part of his identity. “I change my mind at a speed that drives my collaborators crazy,” he explained. “My attachment to my ideas is provisional. There’s no unconditional love for them.” Attachment. That’s what keeps us from recognizing when our opinions are off the mark and rethinking them. To unlock the joy of being wrong, we need to detach. I’ve learned that two kinds of detachment are especially useful: detaching your present from your past and detaching your opinions from your identity. Let’s start with detaching your present from your past. In psychology, one way of measuring the similarity between the person you are right now and your former self is to ask: which pair of circles best describes how you see yourself? In the moment, separating your past self from your current self can be unsettling. Even positive changes can lead to negative emotions; evolving your identity can leave you feeling derailed and disconnected. Over time, though, rethinking who you are appears to become mentally healthy—as long as you can tell a coherent story about how you got from past to present you. In one study, when people felt detached from their past selves, they became less depressed over the course of the year. When you feel as if your life is changing direction, and you’re in the process of shifting who you are, it’s easier to walk away from foolish beliefs you once held. My past self was Mr. Facts—I was too fixated on knowing. Now I’m more interested in finding out what I don’t know. As Bridgewater founder Ray Dalio told me, “If you don’t look back at yourself and think, ‘Wow, how stupid I was a year ago,’ then you must not have learned much in the last year.” The second kind of detachment is separating your opinions from your identity. I’m guessing you wouldn’t want to see a doctor whose identity is Professional Lobotomist, send your kids to a teacher whose identity is Corporal Punisher, or live in a town where the police chief’s identity is Stop-and-Frisker. Once upon","date":"2022-09-03","objectID":"/b90_think_again/:8:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE YODA EFFECT: “YOU MUST UNLEARN WHAT YOU HAVE LEARNED” On my quest to find people who enjoy discovering they were wrong, a trusted colleague told me I had to meet Jean-Pierre Beugoms. He’s in his late forties, and he’s the sort of person who’s honest to a fault; he tells the truth even if it hurts. When his son was a toddler, they were watching a space documentary together, and Jean-Pierre casually mentioned that the sun would one day turn into a red giant and engulf the Earth. His son was not amused. Between tears, he cried, “But I love this planet!” Jean-Pierre felt so terrible that he decided to bite his tongue instead of mentioning threats that could prevent the Earth from even lasting that long. Back in the 1990s, Jean-Pierre had a hobby of collecting the predictions that pundits made on the news and scoring his own forecasts against them. Eventually he started competing in forecasting tournaments—international contests hosted by Good Judgment, where people try to predict the future. It’s a daunting task; there’s an old saying that historians can’t even predict the past. A typical tournament draws thousands of entrants from around the world to anticipate big political, economic, and technological events. The questions are time-bound, with measurable, specific results. Will the current president of Iran still be in office in six months? Which soccer team will win the next World Cup? In the following year, will an individual or a company face criminal charges for an accident involving a self-driving vehicle? Participants don’t just answer yes or no; they have to give their odds. It’s a systematic way of testing whether they know what they don’t know. They get scored months later on accuracy and calibration—earning points not just for giving the right answer, but also for having the right level of conviction. The best forecasters have confidence in their predictions that come true and doubt in their predictions that prove false. On November 18, 2015, Jean-Pierre registered a prediction that stunned his opponents. A day earlier, a new question had popped up in an open forecasting tournament: in July 2016, who would win the U.S. Republican presidential primary? The options were Jeb Bush, Ben Carson, Ted Cruz, Carly Fiorina, Marco Rubio, Donald Trump, and none of the above. With eight months to go before the Republican National Convention, Trump was largely seen as a joke. His odds of becoming the Republican nominee were only 6 percent according to Nate Silver, the celebrated statistician behind the website FiveThirtyEight. When Jean-Pierre peered into his crystal ball, though, he decided Trump had a 68 percent chance of winning. Jean-Pierre didn’t just excel in predicting the results of American events. His Brexit forecasts hovered in the 50 percent range when most of his competitors thought the referendum had little chance of passing. He successfully predicted that the incumbent would lose a presidential election in Senegal, even though the base rates of reelection were extremely high and other forecasters were expecting a decisive win. And he had, in fact, pegged Trump as the favorite long before pundits and pollsters even considered him a viable contender. “It’s striking,” Jean-Pierre wrote early on, back in 2015, that so many forecasters are “still in denial about his chances.” Based on his performance, Jean-Pierre might be the world’s best election forecaster. His advantage: he thinks like a scientist. He’s passionately dispassionate. At various points in his life, Jean-Pierre has changed his political ideologies and religious beliefs.* He doesn’t come from a polling or statistics background; he’s a military historian, which means he has no stake in the way things have always been done in forecasting. The statisticians were attached to their views about how to aggregate polls. Jean-Pierre paid more attention to factors that were hard to measure and overlooked. For Trump, those included “Mastery at manipulating the medi","date":"2022-09-03","objectID":"/b90_think_again/:8:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"MISTAKES WERE MADE . . . MOST LIKELY BY ME As prescient as Jean-Pierre’s bet on Trump was, he still had trouble sticking to it in the face of his feelings. In the spring of 2016, he identified the media coverage of Hillary Clinton’s emails as a red flag, and kept predicting a Trump victory for two months more. By the summer, though, as he contemplated the impending possibility of a Trump presidency, he found himself struggling to sleep at night. He changed his forecast to Clinton. Looking back, Jean-Pierre isn’t defensive about his decision. He freely admits that despite being an experienced forecaster, he made the rookie mistake of falling victim to desirability bias, allowing his preference to cloud his judgment. He focused on the forces that would enable him to predict a Clinton win because he desperately wanted a Trump loss. “That was just a way of me trying to deal with this unpleasant forecast I had issued,” he says. Then he does something unexpected: he laughs at himself. If we’re insecure, we make fun of others. If we’re comfortable being wrong, we’re not afraid to poke fun at ourselves. Laughing at ourselves reminds us that although we might take our decisions seriously, we don’t have to take ourselves too seriously. Research suggests that the more frequently we make fun of ourselves, the happier we tend to be.* Instead of beating ourselves up about our mistakes, we can turn some of our past misconceptions into sources of present amusement. Being wrong won’t always be joyful. The path to embracing mistakes is full of painful moments, and we handle those moments better when we remember they’re essential for progress. But if we can’t learn to find occasional glee in discovering we were wrong, it will be awfully hard to get anything right. I’ve noticed a paradox in great scientists and superforecasters: the reason they’re so comfortable being wrong is that they’re terrified of being wrong. What sets them apart is the time horizon. They’re determined to reach the correct answer in the long run, and they know that means they have to be open to stumbling, backtracking, and rerouting in the short run. They shun rose-colored glasses in favor of a sturdy mirror. The fear of missing the mark next year is a powerful motivator to get a crystal-clear view of last year’s mistakes. “People who are right a lot listen a lot, and they change their mind a lot,” Jeff Bezos says. “If you don’t change your mind frequently, you’re going to be wrong a lot.” Jean-Pierre Beugoms has a favorite trick for catching himself when he’s wrong. When he makes a forecast, he also makes a list of the conditions in which it should hold true—as well as the conditions under which he would change his mind. He explains that this keeps him honest, preventing him from getting attached to a bad prediction. What forecasters do in tournaments is good practice in life. When you form an opinion, ask yourself what would have to happen to prove it false. Then keep track of your views so you can see when you were right, when you were wrong, and how your thinking has evolved. “I started out just wanting to prove myself,” Jean-Pierre says. “Now I want to improve myself—to see how good I can get.” It’s one thing to admit to ourselves that we’ve been wrong. It’s another thing to confess that to other people. Even if we manage to overthrow our inner dictator, we run the risk of facing outer ridicule. In some cases we fear that if others find out we were wrong, it could destroy our reputations. How do people who accept being wrong cope with that? In the early 1990s, the British physicist Andrew Lyne published a major discovery in the world’s most prestigious science journal. He presented the first evidence that a planet could orbit a neutron star—a star that had exploded into a supernova. Several months later, while preparing to give a presentation at an astronomy conference, he noticed that he hadn’t adjusted for the fact that the Earth moves in an elliptical orbit, not a c","date":"2022-09-03","objectID":"/b90_think_again/:8:4","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 4 ","date":"2022-09-03","objectID":"/b90_think_again/:9:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"The Good Fight Club ","date":"2022-09-03","objectID":"/b90_think_again/:10:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"The Psychology of Constructive Conflict Arguments are extremely vulgar, for everybody in good society holds exactly the same opinions. —oscar wilde As the two youngest boys in a big family, the bishop’s sons did everything together. They launched a newspaper and built their own printing press together. They opened a bicycle shop and then started manufacturing their own bikes together. And after years of toiling away at a seemingly impossible problem, they invented the first successful airplane together. Wilbur and Orville Wright first caught the flying bug when their father brought home a toy helicopter. After it broke, they built one of their own. As they advanced from playing together to working together to rethinking human flight together, there was no trace of sibling rivalry between them. Wilbur even said they “thought together.” Even though it was Wilbur who launched the project, the brothers shared equal credit for their achievement. When it came time to decide who would pilot their historic flight at Kitty Hawk, they just flipped a coin. New ways of thinking often spring from old bonds. The comedic chemistry of Tina Fey and Amy Poehler can be traced back to their early twenties, when they immediately hit it off in an improv class. The musical harmony of the Beatles started even earlier, when they were in high school. Just minutes after a mutual friend introduced them, Paul McCartney was teaching John Lennon how to tune a guitar. Ben \u0026 Jerry’s Ice Cream grew out of a friendship between the two founders that began in seventh-grade gym class. It seems that to make progress together, we need to be in sync. But the truth, like all truths, is more complicated. One of the world’s leading experts on conflict is an organizational psychologist in Australia named Karen “Etty” Jehn. When you think about conflict, you’re probably picturing what Etty calls relationship conflict—personal, emotional clashes that are filled not just with friction but also with animosity. I hate your stinking guts. I’ll use small words so that you’ll be sure to understand, you warthog-faced buffoon. You bob for apples in the toilet . . . and you like it. But Etty has identified another flavor called task conflict—clashes about ideas and opinions. We have task conflict when we’re debating whom to hire, which restaurant to pick for dinner, or whether to name our child Gertrude or Quasar. The question is whether the two types of conflict have different consequences. A few years ago I surveyed hundreds of new teams in Silicon Valley on conflict several times during their first six months working together. Even if they argued constantly and agreed on nothing else, they agreed on what kind of conflict they were having. When their projects were finished, I asked their managers to evaluate each team’s effectiveness. The teams that performed poorly started with more relationship conflict than task conflict. They entered into personal feuds early on and were so busy disliking one another that they didn’t feel comfortable challenging one another. It took months for many of the teams to make real headway on their relationship issues, and by the time they did manage to debate key decisions, it was often too late to rethink their directions. What happened in the high-performing groups? As you might expect, they started with low relationship conflict and kept it low throughout their work together. That didn’t stop them from having task conflict at the outset: they didn’t hesitate to surface competing perspectives. As they resolved some of their differences of opinion, they were able to align on a direction and carry out their work until they ran into new issues to debate. All in all, more than a hundred studies have examined conflict types in over eight thousand teams. A meta-analysis of those studies showed that relationship conflict is generally bad for performance, but some task conflict can be beneficial: it’s been linked to higher creativity and smarter choices. ","date":"2022-09-03","objectID":"/b90_think_again/:11:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE PLIGHT OF THE PEOPLE PLEASER As long as I can remember, I’ve been determined to keep the peace. Maybe it’s because my group of friends dropped me in middle school. Maybe it’s genetic. Maybe it’s because my parents got divorced. Whatever the cause, in psychology there’s a name for my affliction. It’s called agreeableness, and it’s one of the major personality traits around the world. Agreeable people tend to be nice. Friendly. Polite. Canadian.* My first impulse is to avoid even the most trivial of conflicts. When I’m riding in an Uber and the air-conditioning is blasting, I struggle to bring myself to ask the driver to turn it down—I just sit there shivering in silence until my teeth start to chatter. When someone steps on my shoe, I’ve actually apologized for inconveniently leaving my foot in his path. When students fill out course evaluations, one of their most common complaints is that I’m “too supportive of stupid comments.” Disagreeable people tend to be more critical, skeptical, and challenging—and they’re more likely than their peers to become engineers and lawyers. They’re not just comfortable with conflict; it energizes them. If you’re highly disagreeable, you might be happier in an argument than in a friendly conversation. That quality often comes with a bad rap: disagreeable people get stereotyped as curmudgeons who complain about every idea, or Dementors who suck the joy out of every meeting. When I studied Pixar, though, I came away with a dramatically different view. In 2000, Pixar was on fire. Their teams had used computers to rethink animation in their first blockbuster, Toy Story, and they were fresh off two more smash hits. Yet the company’s founders weren’t content to rest on their laurels. They recruited an outside director named Brad Bird to shake things up. Brad had just released his debut film, which was well reviewed but flopped at the box office, so he was itching to do something big and bold. When he pitched his vision, the technical leadership at Pixar said it was impossible: they would need a decade and $500 million to make it. Brad wasn’t ready to give up. He sought out the biggest misfits at Pixar for his project—people who were disagreeable, disgruntled, and dissatisfied. Some called them black sheep. Others called them pirates. When Brad rounded them up, he warned them that no one believed they could pull off the project. Just four years later, his team didn’t only succeed in releasing Pixar’s most complex film ever; they actually managed to lower the cost of production per minute. The Incredibles went on to gross upwards of $631 million worldwide and won the Oscar for Best Animated Feature. Notice what Brad didn’t do. He didn’t stock his team with agreeable people. Agreeable people make for a great support network: they’re excited to encourage us and cheerlead for us. Rethinking depends on a different kind of network: a challenge network, a group of people we trust to point out our blind spots and help us overcome our weaknesses. Their role is to activate rethinking cycles by pushing us to be humble about our expertise, doubt our knowledge, and be curious about new perspectives. The ideal members of a challenge network are disagreeable, because they’re fearless about questioning the way things have always been done and holding us accountable for thinking again. There’s evidence that disagreeable people speak up more frequently—especially when leaders aren’t receptive—and foster more task conflict. They’re like the doctor in the show House or the boss in the film The Devil Wears Prada. They give the critical feedback we might not want to hear, but need to hear. Harnessing disagreeable people isn’t always easy. It helps if certain conditions are in place. Studies in oil drilling and tech companies suggest that dissatisfaction promotes creativity only when people feel committed and supported—and that cultural misfits are most likely to add value when they have strong bonds with their colleague","date":"2022-09-03","objectID":"/b90_think_again/:11:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"DON’T AGREE TO DISAGREE Hashing out competing views has potential downsides—risks that need to be managed. On the first Incredibles film, a rising star named Nicole Grindle had managed the simulation of the hair, watching John and Brad’s interactions from a distance. When Nicole came in to produce the sequel with John, one of her concerns was that the volume of the arguments between the two highly accomplished leaders might drown out the voices of people who were less comfortable speaking up: newcomers, introverts, women, and minorities. It’s common for people who lack power or status to shift into politician mode, suppressing their dissenting views in favor of conforming to the HIPPO—the HIghest Paid Person’s Opinion. Sometimes they have no other choice if they want to survive. To make sure their desire for approval didn’t prevent them from introducing task conflict, Nicole encouraged new people to bring their divergent ideas to the table. Some voiced them directly to the group; others went to her for feedback and support. Although Nicole wasn’t a pirate, as she found herself advocating for different perspectives she became more comfortable challenging Brad on characters and dialogue. “Brad is still the ornery guy who first came to Pixar, so you have to be ready for a spirited debate when you put forward a contrary point of view.” The notion of a spirited debate captures something important about how and why good fights happen. If you watch Brad argue with his colleagues—or the pirates fight with one another—you can quickly see that the tension is intellectual, not emotional. The tone is vigorous and feisty rather than combative or aggressive. They don’t disagree just for the sake of it; they disagree because they care. “Whether you disagree loudly, or quietly yet persistently put forward a different perspective,” Nicole explains, “we come together to support the common goal of excellence—of making great films.” After seeing their interactions up close, I finally understood what had long felt like a contradiction in my own personality: how I could be highly agreeable and still cherish a good argument. Agreeableness is about seeking social harmony, not cognitive consensus. It’s possible to disagree without being disagreeable. Although I’m terrified of hurting other people’s feelings, when it comes to challenging their thoughts, I have no fear. In fact, when I argue with someone, it’s not a display of disrespect—it’s a sign of respect. It means I value their views enough to contest them. If their opinions didn’t matter to me, I wouldn’t bother. I know I have chemistry with someone when we find it delightful to prove each other wrong. Agreeable people don’t always steer clear of conflict. They’re highly attuned to the people around them and often adapt to the norms in the room. My favorite demonstration is an experiment by my colleagues Jennifer Chatman and Sigal Barsade. Agreeable people were significantly more accommodating than disagreeable ones—as long as they were in a cooperative team. When they were assigned to a competitive team, they acted just as disagreeably as their disagreeable teammates. That’s how working with Brad Bird influenced John Walker. John’s natural tendency is to avoid conflict: at restaurants, if the waiter brings him the wrong dish, he just goes ahead and eats it anyway. “But when I’m involved in something bigger than myself,” he observes, “I feel like I have an opportunity, a responsibility really, to speak up, speak out, debate. Fight like hell when the morning whistle blows, but go out for a beer after the one at five o’clock.” That adaptability was also visible in the Wright brothers’ relationship. In Wilbur, Orville had a built-in challenge network. Wilbur was known to be highly disagreeable: he was unfazed by other people’s opinions and had a habit of pouncing on anyone else’s idea the moment it was raised. Orville was known as gentle, cheerful, and sensitive to criticism. Yet those qualities see","date":"2022-09-03","objectID":"/b90_think_again/:11:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"GETTING HOT WITHOUT GETTING MAD A major problem with task conflict is that it often spills over into relationship conflict. One minute you’re disagreeing about how much seasoning to put on the Thanksgiving turkey, and the next minute you find yourself yelling “You smell!” Although the Wright brothers had a lifetime of experience discovering each other’s hot buttons, that didn’t mean they always kept their cool. Their last grand challenge before liftoff was their single hardest problem: designing a propeller. They knew their airplane couldn’t take flight without one, but the right kind didn’t exist. As they struggled with various approaches, they argued back and forth for hours at a time, often raising their voices. The feuding lasted for months as each took turns preaching the merits of his own solutions and prosecuting the other’s points. Eventually their younger sister, Katharine, threatened to leave the house if they didn’t stop fighting. They kept at it anyway, until one night it culminated in what might have been the loudest shouting match of their lives. Strangely, the next morning, they came into the shop and acted as if nothing had happened. They picked up the argument about the propeller right where they had left off—only now without the yelling. Soon they were both rethinking their assumptions and stumbling onto what would become one of their biggest breakthroughs. The Wright brothers were masters at having intense task conflict without relationship conflict. When they raised their voices, it reflected intensity rather than hostility. As their mechanic marveled, “I don’t think they really got mad, but they sure got awfully hot.” Experiments show that simply framing a dispute as a debate rather than as a disagreement signals that you’re receptive to considering dissenting opinions and changing your mind, which in turn motivates the other person to share more information with you. A disagreement feels personal and potentially hostile; we expect a debate to be about ideas, not emotions. Starting a disagreement by asking, “Can we debate?” sends a message that you want to think like a scientist, not a preacher or a prosecutor—and encourages the other person to think that way, too. The Wright brothers had the benefit of growing up in a family where disagreements were seen as productive and enjoyable. When arguing with others, though, they often had to go out of their way to reframe their behavior. “Honest argument is merely a process of mutually picking the beams and motes out of each other’s eyes so both can see clearly,” Wilbur once wrote to a colleague whose ego was bruised after a fiery exchange about aeronautics. Wilbur stressed that it wasn’t personal: he saw arguments as opportunities to test and refine their thinking. “I see that you are back at your old trick of giving up before you are half beaten in an argument. I feel pretty certain of my own ground but was anticipating the pleasure of a good scrap before the matter was settled. Discussion brings out new ways of looking at things.” When they argued about the propeller, the Wright brothers were making a common mistake. Each was preaching about why he was right and why the other was wrong. When we argue about why, we run the risk of becoming emotionally attached to our positions and dismissive of the other side’s. We’re more likely to have a good fight if we argue about how. When social scientists asked people why they favor particular policies on taxes, health care, or nuclear sanctions, they often doubled down on their convictions. Asking people to explain how those policies would work in practice—or how they’d explain them to an expert—activated a rethinking cycle. They noticed gaps in their knowledge, doubted their conclusions, and became less extreme; they were now more curious about alternative options. Psychologists find that many of us are vulnerable to an illusion of explanatory depth. Take everyday objects like a bicycle, a piano, or earbuds: how well d","date":"2022-09-03","objectID":"/b90_think_again/:11:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 5 ","date":"2022-09-03","objectID":"/b90_think_again/:12:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Dances with Foes ","date":"2022-09-03","objectID":"/b90_think_again/:13:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"How to Win Debates and Influence People Exhausting someone in argument is not the same as convincing him. —tim kreider At thirty-one, Harish Natarajan has won three dozen international debate tournaments. He’s been told it’s a world record. But his opponent today presents a unique challenge. Debra Jo Prectet is a prodigy hailing from Haifa, Israel. She’s just eight years old, and although she made her first foray into public debating only last summer, she’s been preparing for this moment for years. Debra has absorbed countless articles to accumulate knowledge, closely studied speechwriting to hone her clarity, and even practiced her delivery to incorporate humor. Now she’s ready to challenge the champion himself. Her parents are hoping she’ll make history. Harish was a wunderkind too. By the time he was eight, he was outmaneuvering his own parents in dinner-table debates about the Indian caste system. He went on to become the European debate champion and a grand finalist in the world debate championship, and coached the Filipino national school debate team at the world championship. I was introduced to Harish by an unusually bright former student who used to compete against him, and remembers having lost “many (likely all)” of their debates. Harish and Debra are facing off in San Francisco in February 2019 in front of a large crowd. They’ve been kept in the dark about the debate topic. When they walk onstage, the moderator announces the subject: should preschools be subsidized by the government? After just fifteen minutes of preparation, Debra will present her strongest arguments in favor of subsidies, and Harish will marshal his best case against them. Their goal is to win the audience over to their side on preschool subsidies, but their impact on me will be much broader: they’ll end up changing my view of what it takes to win a debate. Debra kicks off with a joke, drawing laughter from the crowd by telling Harish that although he may hold the world record in debate wins, he’s never debated someone like her. Then she goes on to summarize an impressive number of studies—citing her sources—about the academic, social, and professional benefits of preschool programs. For good measure, she quotes a former prime minister’s argument about preschool being a smart investment. Harish acknowledges the facts that Debra presented, but then makes his case that subsidizing preschools is not the appropriate remedy for the damage caused by poverty. He suggests that the issue should be evaluated on two grounds: whether preschool is currently underprovided and underconsumed, and whether it helps those who are the least fortunate. He argues that in a world full of trade-offs, subsidizing preschool is not the best use of taxpayer money. Going into the debate, 92 percent of the audience has already made up their minds. I’m one of them: it didn’t take me long to figure out where I stood on preschool subsidies. In the United States, public education is free from kindergarten through high school. I’m familiar with evidence that early access to education in the first few years of children’s lives may be even more critical to helping them escape poverty than anything they learn later. I believe education is a fundamental human right, like access to water, food, shelter, and health care. That puts me on Team Debra. As I watch the debate, her early arguments strike a chord. Here are some highlights: Debra: Research clearly shows that a good preschool can help kids overcome the disadvantages often associated with poverty. Data for the win! Be still, my beating heart. Debra: You will possibly hear my opponent talk today about different priorities . . . he might say that subsidies are needed, but not for preschools. I would like to ask you, Mr. Natarajan . . . why don’t we examine the evidence and the data and decide accordingly? If Harish has an Achilles’ heel, my former student has told me, it’s that his brilliant arguments aren’t always grounded in facts","date":"2022-09-03","objectID":"/b90_think_again/:14:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE SCIENCE OF THE DEAL A few years ago a former student named Jamie called me for advice on where to go to business school. Since she was already well on her way to building a successful career, I told her it was a waste of time and money. I walked her through the lack of evidence that a graduate degree would make a tangible difference in her future, and the risk that she’d end up overqualified and underexperienced. When she insisted that her employer expected an MBA for promotions, I told her that I knew of exceptions and pointed out that she probably wouldn’t spend her whole career at that firm anyway. Finally, she hit back: “You’re a logic bully!” A what? “A logic bully,” Jamie repeated. “You just overwhelmed me with rational arguments, and I don’t agree with them, but I can’t fight back.” At first I was delighted by the label. It felt like a solid description of one of my roles as a social scientist: to win debates with the best data. Then Jamie explained that my approach wasn’t actually helpful. The more forcefully I argued, the more she dug in her heels. Suddenly I realized I had instigated that same kind of resistance many times before. David Sipress/The New Yorker Collection/The Cartoon Bank; © Condé Nast Growing up, I was taught by my karate sensei never to start a fight unless I was prepared to be the only one standing at the end. That’s how I approached debates at work and with friends: I thought the key to victory was to go into battle armed with airtight logic and rigorous data. The harder I attacked, though, the harder my opponents fought back. I was laser-focused on convincing them to accept my views and rethink theirs, but I was coming across like a preacher and a prosecutor. Although those mindsets sometimes motivated me to persist in making my points, I often ended up alienating my audience. I was not winning. For centuries, debating has been prized as an art form, but there’s now a growing science of how to do it well. In a formal debate your goal is to change the mind of your audience. In an informal debate, you’re trying to change the mind of your conversation partner. That’s a kind of negotiation, where you’re trying to reach an agreement about the truth. To build my knowledge and skills about how to win debates, I studied the psychology of negotiations and eventually used what I’d learned to teach bargaining skills to leaders across business and government. I came away convinced that my instincts—and what I’d learned in karate—were dead wrong. A good debate is not a war. It’s not even a tug-of-war, where you can drag your opponent to your side if you pull hard enough on the rope. It’s more like a dance that hasn’t been choreographed, negotiated with a partner who has a different set of steps in mind. If you try too hard to lead, your partner will resist. If you can adapt your moves to hers, and get her to do the same, you’re more likely to end up in rhythm. In a classic study, a team of researchers led by Neil Rackham examined what expert negotiators do differently. They recruited one group of average negotiators and another group of highly skilled ones, who had significant track records of success and had been rated as effective by their counterparts. To compare the participants’ techniques, they recorded both groups doing labor and contract negotiations. In a war, our goal is to gain ground rather than lose it, so we’re often afraid to surrender a few battles. In a negotiation, agreeing with someone else’s argument is disarming. The experts recognized that in their dance they couldn’t stand still and expect the other person to make all the moves. To get in harmony, they needed to step back from time to time. One difference was visible before anyone even arrived at the bargaining table. Prior to the negotiations, the researchers interviewed both groups about their plans. The average negotiators went in armed for battle, hardly taking note of any anticipated areas of agreement. The experts, in contrast,","date":"2022-09-03","objectID":"/b90_think_again/:14:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"DANCING TO THE SAME BEAT Since the audience started out favoring preschool subsidies, there was more room for change in Harish’s direction—but he also had the more difficult task of advocating for the unpopular position. He opened the audience’s mind by taking a page out of the playbook of expert negotiators. Harish started by emphasizing common ground. When he took the stage for his rebuttal, he immediately drew attention to his and Debra’s areas of agreement. “So,” he began, “I think we disagree on far less than it may seem.” He called out their alignment on the problem of poverty—and on the validity of some of the studies—before objecting to subsidies as a solution. We won’t have much luck changing other people’s minds if we refuse to change ours. We can demonstrate openness by acknowledging where we agree with our critics and even what we’ve learned from them. Then, when we ask what views they might be willing to revise, we’re not hypocrites. Convincing other people to think again isn’t just about making a good argument—it’s about establishing that we have the right motives in doing so. When we concede that someone else has made a good point, we signal that we’re not preachers, prosecutors, or politicians trying to advance an agenda. We’re scientists trying to get to the truth. “Arguments are often far more combative and adversarial than they need to be,” Harish told me. “You should be willing to listen to what someone else is saying and give them a lot of credit for it. It makes you sound like a reasonable person who is taking everything into account.” Being reasonable literally means that we can be reasoned with, that we’re open to evolving our views in light of logic and data. So in the debate with Harish, why did Debra neglect to do that—why did she overlook common ground? It’s not because Debra is eight years old. It’s because she isn’t human. Debra Jo Prectet is an anagram I invented. Her official name is Project Debater, and she’s a machine. More specifically, an artificial intelligence developed by IBM to do for debate what Watson did for chess. They first dreamed the idea up in 2011 and started working intensively on it in 2014. Just a few years later, Project Debater had developed the remarkable ability to conduct an intelligent debate in public, complete with facts, coherent sentences, and even counterarguments. Her knowledge corpus consists of 400 million articles, largely from credible newspapers and magazines, and her claim detection engine is designed to locate key arguments, identify their boundaries, and weigh the evidence. For any debate topic, she can instantaneously search her knowledge graph for relevant data points, mold them into a logical case, and deliver it clearly—even entertainingly—in a female voice within the time constraints. Her first words in the preschool subsidy debate were, “Greetings, Harish. I’ve heard you hold the world record in debate competition wins against humans, but I suspect you’ve never debated a machine. Welcome to the future.” Of course, it’s possible that Harish won because the audience was biased against the computer and rooting for the human. It’s worth noting, though, that Harish’s approach in that debate is the same one that he’s used to defeat countless humans on international stages. What amazes me is that the computer was able to master multiple complex capabilities while completely missing this crucial one. After studying 10 billion sentences, a computer was able to say something funny—a skill that’s normally thought to be confined to sentient beings with high levels of social and emotional intelligence. The computer had learned to make a logical argument and even anticipate the other side’s counterargument. Yet it hadn’t learned to agree with elements of the other side’s argument, apparently because that behavior was all too rarely deployed across 400 million articles by humans. They were usually too busy preaching their arguments, prosecuting their enemies, or po","date":"2022-09-03","objectID":"/b90_think_again/:14:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"DON’T STEP ON THEIR TOES Harish’s next advantage stemmed from one of his disadvantages. He would never have access to as many facts as the computer. When the audience was polled afterward about who taught them more, the overwhelming majority said they learned more from the computer than from Harish. But it was Harish who succeeded in swaying their opinions. Why? The computer piled on study after study to support a long list of reasons in favor of preschool subsidies. Like a skilled negotiator, Harish focused on just two reasons against them. He knew that making too many points could come at the cost of developing, elaborating, and reinforcing his best ones. “If you have too many arguments, you’ll dilute the power of each and every one,” he told me. “They are going to be less well explained, and I don’t know if any of them will land enough—I don’t think the audience will believe them to be important enough. Most top debaters aren’t citing a lot of information.” Is this always the best way to approach a debate? The answer is—like pretty much everything else in social science—it depends. The ideal number of reasons varies from one circumstance to another. There are times when preaching and prosecuting can make us more persuasive. Research suggests that the effectiveness of these approaches hinges on three key factors: how much people care about the issue, how open they are to our particular argument, and how strong-willed they are in general. If they’re not invested in the issue or they’re receptive to our perspective, more reasons can help: people tend to see quantity as a sign of quality. The more the topic matters to them, the more the quality of reasons matters. It’s when audiences are skeptical of our view, have a stake in the issue, and tend to be stubborn that piling on justifications is most likely to backfire. If they’re resistant to rethinking, more reasons simply give them more ammunition to shoot our views down. It’s not just about the number of reasons, though. It’s also how they fit together. A university once approached me to see if I could bring in donations from alumni who had never given a dime. My colleagues and I ran an experiment testing two different messages meant to convince thousands of resistant alumni to give. One message emphasized the opportunity to do good: donating would benefit students, faculty, and staff. The other emphasized the opportunity to feel good: donors would enjoy the warm glow of giving. The two messages were equally effective: in both cases, 6.5 percent of the stingy alumni ended up donating. Then we combined them, because two reasons are better than one. Except they weren’t. When we put the two reasons together, the giving rate dropped below 3 percent. Each reason alone was more than twice as effective as the two combined. The audience was already skeptical. When we gave them different kinds of reasons to donate, we triggered their awareness that someone was trying to persuade them—and they shielded themselves against it. A single line of argument feels like a conversation; multiple lines of argument can become an onslaught. The audience tuned out the preacher and summoned their best defense attorney to refute the prosecutor. As important as the quantity and quality of reasons might be, the source matters, too. And the most convincing source is often the one closest to your audience. A student in one of my classes, Rachel Breuhaus, noticed that although top college basketball teams have rabid fans, there are usually empty seats in their arenas. To study strategies for motivating more fans to show up, we launched an experiment in the week before an upcoming game targeting hundreds of season ticket holders. When left to their own devices, 77 percent of these supposedly die-hard fans actually made it to the game. We decided that the most persuasive message would come from the team itself, so we sent fans an email with quotes from players and coaches about how part of the home-court adva","date":"2022-09-03","objectID":"/b90_think_again/:14:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"DR. JEKYLL AND MR. HOSTILE Some years ago, a Wall Street firm brought me in to consult on a project to attract and retain junior analysts and associates. After two months of research I submitted a report with twenty-six data-driven recommendations. In the middle of my presentation to the leadership team, one of the members interrupted and asked, “Why don’t we just pay them more?” I told him money alone probably wouldn’t make a difference. Many studies across a range of industries have shown that once people are earning enough to meet their basic needs, paying them more doesn’t stop them from leaving bad jobs and bad bosses. The executive started arguing with me: “That’s not what I’ve found in my experience.” I fired back in prosecutor mode: “Yes, that’s why I brought you randomized, controlled experiments with longitudinal data: to learn rigorously from many people’s experiences, not idiosyncratically from yours.” The executive pushed back, insisting that his company was different, so I rattled off some basic statistics from his own employees. In surveys and interviews, a grand total of zero had even mentioned compensation. They were already well paid (read: overpaid), and if that could have solved the problem, it already would have.* But the executive still refused to budge. Finally I became so exasperated that I did something out of character. I shot back, “I’ve never seen a group of smart people act so dumb.” In the hierarchy of disagreement created by computer scientist Paul Graham, the highest form of argument is refuting the central point, and the lowest is name-calling. In a matter of seconds I’d devolved from logic bully to playground bully. If I could do that session over, I’d start with common ground and fewer data points. Instead of attacking their beliefs with my research, I’d ask them what would open their minds to my data. A few years later, I had a chance to test that approach. During a keynote speech on creativity, I cited evidence that Beethoven and Mozart didn’t have higher hit rates than some of their peers; they generated a larger volume of work, which gave them more shots at greatness. A member of the audience interrupted. “Bullsh*t!” he shouted. “You’re disrespecting the great masters of music. You’re totally ignorant—you don’t know what you’re talking about!” Instead of reacting right then, I waited a few minutes until a scheduled break and then made my way to my heckler. Me: You’re welcome to disagree with the data, but I don’t think that’s a respectful way to express your opinion. It’s not how I was trained to have an intellectual debate. Were you? Music man: Well, no . . . I just think you’re wrong. Me: It’s not my opinion—it’s the independent finding of two different social scientists. What evidence would change your mind? Music man: I don’t believe you can quantify a musician’s greatness, but I’d like to see the research. When I sent him the study, he responded with an apology. I don’t know if I succeeded in changing his mind, but I had done a better job of opening it. When someone becomes hostile, if you respond by viewing the argument as a war, you can either attack or retreat. If instead you treat it as a dance, you have another option—you can sidestep. Having a conversation about the conversation shifts attention away from the substance of the disagreement and toward the process for having a dialogue. The more anger and hostility the other person expresses, the more curiosity and interest you show. When someone is losing control, your tranquility is a sign of strength. It takes the wind out of their emotional sails. It’s pretty rare for someone to respond by screaming “SCREAMING IS MY PREFERRED MODE OF COMMUNICATION!” This is a fifth move that expert negotiators made more often than average negotiators. They were more likely to comment on their feelings about the process and test their understanding of the other side’s feelings: I’m disappointed in the way this discussion has unfolded—are you fr","date":"2022-09-03","objectID":"/b90_think_again/:14:4","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE STRENGTH OF WEAK OPINIONS When we hit a brick wall in a debate, we don’t have to stop talking altogether. “Let’s agree to disagree” shouldn’t end a discussion. It should start a new conversation, with a focus on understanding and learning rather than arguing and persuading. That’s what we’d do in scientist mode: take the long view and ask how we could have handled the debate more effectively. Doing so might land us in a better position to make the same case to a different person—or to make a different case to the same person on a different day. When I asked one of the Wall Street executives for advice on how to approach debates differently in the future, he suggested expressing less conviction. I could easily have countered that I was uncertain about which of my twenty-six recommendations might be relevant. I could also have conceded that although money didn’t usually solve the problem, I’d never seen anyone test the effect of million-dollar retention bonuses. That would be a fun experiment to run, don’t you think? A few years ago, I argued in my book Originals that if we want to fight groupthink, it helps to have “strong opinions, weakly held.” Since then I’ve changed my mind—I now believe that’s a mistake. If we hold an opinion weakly, expressing it strongly can backfire. Communicating it with some uncertainty signals confident humility, invites curiosity, and leads to a more nuanced discussion. Research shows that in courtrooms, expert witnesses and deliberating jurors are more credible and more persuasive when they express moderate confidence, rather than high or low confidence.* And these principles aren’t limited to debates—they apply in a wide range of situations where we’re advocating for our beliefs or even for ourselves. In 2014, a young woman named Michele Hansen came across a job opening for a product manager at an investment company. She was excited about the position but she wasn’t qualified for it: she had no background in finance and lacked the required number of years of experience. If you were in her shoes and you decided to go for it, what would you say in your cover letter? The natural starting point would be to emphasize your strengths and downplay your weaknesses. As Michael Scott deadpanned on The Office, “I work too hard, I care too much, and sometimes I can be too invested in my job.” But Michele Hansen did the opposite, taking a page out of the George Costanza playbook on Seinfeld: “My name is George. I’m unemployed and I live with my parents.” Rather than trying to hide her shortcomings, Michele opened with them. “I’m probably not the candidate you’ve been envisioning,” her cover letter began. “I don’t have a decade of experience as a Product Manager nor am I a Certified Financial Planner.” After establishing the drawbacks of her case, she emphasized a few reasons to hire her anyway: But what I do have are skills that can’t be taught. I take ownership of projects far beyond my pay grade and what is in my defined scope of responsibilities. I don’t wait for people to tell me what to do and go seek for myself what needs to be done. I invest myself deeply in my projects and it shows in everything I do, from my projects at work to my projects that I undertake on my own time at night. I’m entrepreneurial, I get things done, and I know I would make an excellent right hand for the co-founder leading this project. I love breaking new ground and starting with a blank slate. (And any of my previous bosses would be able to attest to these traits.) A week later a recruiter contacted her for a phone screen, and then she had another phone screen with the team. On the calls, she asked about experiments they’d run recently that had surprised them. The question itself surprised the team—they ended up talking about times when they were sure they were right but were later proven wrong. Michele got the job, thrived, and was promoted to lead product development. This success isn’t unique to her: there’s evidence that ","date":"2022-09-03","objectID":"/b90_think_again/:14:5","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 6 ","date":"2022-09-03","objectID":"/b90_think_again/:15:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Bad Blood on the Diamond ","date":"2022-09-03","objectID":"/b90_think_again/:16:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Diminishing Prejudice by Destabilizing Stereotypes I hated the Yankees with all my heart, even to the point of having to confess in my first holy confession that I wished harm to others—namely that I wished various New York Yankees would break arms, legs and ankles. . . . —doris kearns goodwin One afternoon in Maryland in 1983, Daryl Davis arrived at a lounge to play the piano at a country music gig. It wasn’t his first time being the only Black man in the room. Before the night was out, it would be his first time having a conversation with a white supremacist. After the show, an older white man in the audience walked up to Daryl and told him that he was astonished to see a Black musician play like Jerry Lee Lewis. Daryl replied that he and Lewis were, in fact, friends, and that Lewis himself had acknowledged that his style was influenced by Black musicians. Although the man was skeptical, he invited Daryl to sit down for a drink. Soon the man was admitting that he’d never had a drink with a Black person before. Eventually he explained to Daryl why. He was a member of the Ku Klux Klan, the white supremacist hate group that had been murdering African Americans for over a century and had lynched a man just two years earlier. If you found yourself sitting down with someone who hated you and all people who shared your skin color, your instinctive options might be fight, flight, or freeze—and rightfully so. Daryl had a different reaction: he burst out laughing. When the man pulled out his KKK membership card to show he wasn’t joking, Daryl returned to a question that had been on his mind since he was ten years old. In the late 1960s, he was marching in a Cub Scout parade when white spectators started throwing cans, rocks, and bottles at him. It was the first time he remembers facing overt racism, and although he could justifiably have gotten angry, he was bewildered: “How can you hate me when you don’t even know me?” At the end of the conversation, the Klansman handed Daryl his phone number and asked if he would call him whenever he was playing locally. Daryl followed up, and the next month the man showed up with a bunch of his friends to see Daryl perform. Over time a friendship grew, and the man ended up leaving the KKK. That was a turning point in Daryl’s life, too. It wasn’t long before Daryl was sitting down with Imperial Wizards and Grand Dragons—the Klan’s highest officers—to ask his question. Since then, Daryl has convinced many white supremacists to leave the KKK and abandon their hatred. I wanted to understand how that kind of change happens—how to break overconfidence cycles that are steeped in stereotypes and prejudice about entire groups of people. Strangely enough, my journey started at a baseball game. ","date":"2022-09-03","objectID":"/b90_think_again/:17:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"HATE ME OUT AT THE BALLGAME “Yankees suck! Yankees suck!” It was a summer night at Fenway Park, my first and only time at a Boston Red Sox baseball game. In the seventh inning, without warning, 37,000 people erupted into a chant. The entire stadium was dissing the New York Yankees in perfect harmony. I knew the two teams had a century-long rivalry, widely viewed as the most heated in all of American professional sports. I took it for granted that the Boston fans would root against the Yankees. I just didn’t expect it to happen that day, because the Yankees weren’t even there. The Red Sox were playing against the Oakland A’s. The Boston fans were booing a team that was hundreds of miles away. It was as if Burger King fans were going head-to-head against Wendy’s in a taste test and started chanting “McDonald’s sucks!” I started to wonder if Red Sox fans hate the Yankees more than they love their own team. Boston parents have been known to teach their kids to flip the bird at the Yankees and detest anything in pinstripes, and yankees suck is apparently among the most popular T-shirts in Boston history. When asked how much money it would take to get them to taunt their own team, Red Sox fans requested an average of $503. To root for the Yankees, they wanted even more: $560. The feelings run so deep that neuroscientists can watch them light up people’s minds: when Red Sox fans see the Yankees fail, they show immediate activation in brain regions linked to reward and pleasure. Those feelings extend well beyond Boston: in a 2019 analysis of tweets, the Yankees were the most hated baseball team in twenty-eight of the fifty U.S. states, which may explain the popularity of this T-shirt: I recently called a friend who’s a die-hard Red Sox fan with a simple question: what would it take to get him to root for the Yankees? Without pausing, he said, “If they were playing Al Qaeda . . . maybe.” It’s one thing to love your team. It’s another to hate your rivals so much that you’d consider rooting for terrorists to crush them. If you despise a particular sports team—and its fans—you’re harboring some strong opinions about a group of people. Those beliefs are stereotypes, and they often spill over into prejudice. The stronger your attitudes become, the less likely you are to rethink them. Rivalries aren’t unique to sports. A rivalry exists whenever we reserve special animosity for a group we see as competing with us for resources or threatening our identities. In business, the rivalry between footwear companies Puma and Adidas was so intense that for generations, families self-segregated based on their allegiance to the brands—they went to different bakeries, pubs, and shops, and even refused to date people who worked for the rival firm. In politics, you probably know some Democrats who view Republicans as being greedy, ignorant, heartless cretins, and some Republicans who regard Democrats as lazy, dishonest, hypersensitive snowflakes. As stereotypes stick and prejudice deepens, we don’t just identify with our own group; we disidentify with our adversaries, coming to define who we are by what we’re not. We don’t just preach the virtues of our side; we find self-worth in prosecuting the vices of our rivals. When people hold prejudice toward a rival group, they’re often willing to do whatever it takes to elevate their own group and undermine their rivals—even if it means doing harm or doing wrong. We see people cross those lines regularly in sports rivalries.* Aggression extends well beyond the playing field: from Barcelona to Brazil, fistfights frequently break out between soccer fans. Cheating scandals are rampant, too, and they aren’t limited to athletes or coaches. When students at The Ohio State University were paid to participate in an experiment, they learned that if they were willing to lie to a student from a different school, their own pay would double and the other student’s compensation would be cut in half. Their odds of lying quadrup","date":"2022-09-03","objectID":"/b90_think_again/:17:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"FITTING IN AND STANDING OUT For decades psychologists have found that people can feel animosity toward other groups even when the boundaries between them are trivial. Take a seemingly innocuous question: is a hot dog a sandwich? When students answered this question, most felt strongly enough that they were willing to sacrifice a dollar to those who agreed with them to make sure those who disagreed got less. In every human society, people are motivated to seek belonging and status. Identifying with a group checks both boxes at the same time: we become part of a tribe, and we take pride when our tribe wins. In classic studies on college campuses, psychologists found that after their team won a football game, students were more likely to walk around wearing school swag. From Arizona State to Notre Dame to USC, students basked in the reflected glory of Saturday victories, donning team shirts and hats and jackets on Sunday. If their team lost, they shunned school apparel, and distanced themselves by saying “they lost” instead of “we lost.” Some economists and finance experts have even found that the stock market rises if a country’s soccer team wins World Cup matches and falls if they lose.* Rivalries are most likely to develop between teams that are geographically close, compete regularly, and are evenly matched. The Yankees and Red Sox fit this pattern: they’re both on the East Coast, they play each other eighteen or nineteen times a season, they both have histories of success, and as of 2019, they had competed over 2,200 times—with each team winning over 1,000 times. The two teams also have more fans than any other franchises in baseball. I decided to test what it would take to get fans to rethink their beliefs about their bitter rivals. Working with a doctoral student, Tim Kundro, I ran a series of experiments with passionate Yankees and Red Sox supporters. To get a sense of their stereotypes, we asked over a thousand Red Sox and Yankees fans to list three negative things about their rivals. They mostly used the same words to describe one another, complaining about their respective accents, their beards, and their tendency to “smell like old corn chips.” WHY RED SOX FANS HATE YANKEES FANS WHY YANKEES FANS HATE RED SOX FANS Once we’ve formed those kinds of stereotypes, for both mental and social reasons it’s hard to undo them. Psychologist George Kelly observed that our beliefs are like pairs of reality goggles. We use them to make sense of the world and navigate our surroundings. A threat to our opinions cracks our goggles, leaving our vision blurred. It’s only natural to put up our guard in response—and Kelly noticed that we become especially hostile when trying to defend opinions that we know, deep down, are false. Rather than trying on a different pair of goggles, we become mental contortionists, twisting and turning until we find an angle of vision that keeps our current views intact. Socially, there’s another reason stereotypes are so sticky. We tend to interact with people who share them, which makes them even more extreme. This phenomenon is called group polarization, and it’s been demonstrated in hundreds of experiments. Juries with authoritarian beliefs recommend harsher punishments after deliberating together. Corporate boards are more likely to support paying outlandish premiums for companies after group discussions. Citizens who start out with a clear belief on affirmative action and gay marriage develop more extreme views on these issues after talking with a few others who share their stance. Their preaching and prosecuting move in the direction of their politics. Polarization is reinforced by conformity: peripheral members fit in and gain status by following the lead of the most prototypical member of the group, who often holds the most intense views. Grow up in a family of Red Sox fans and you’re bound to hear some unpleasant things about Yankees fans. Start making regular trips to a ballpark packed with people w","date":"2022-09-03","objectID":"/b90_think_again/:17:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"HYPOTHESIS 1: NOT IN A LEAGUE OF THEIR OWN If you ever leave the planet Earth, you’ll probably end up rethinking some of your feelings about other human beings. A team of psychologists has studied the effects of outer space on inner space, assessing the changes in more than a hundred astronauts and cosmonauts through interviews, surveys, and analyses of autobiographies. Upon returning from space, astronauts are less focused on individual achievements and personal happiness, and more concerned about the collective good. “You develop an instant global consciousness . . . an intense dissatisfaction with the state of the world, and a compulsion to do something about it,” Apollo 14 astronaut Edgar Mitchell reflected. “From out there on the moon, international politics looks so petty. You want to grab a politician by the scruff of the neck and drag him a quarter of a million miles out and say, ‘Look at that, you son of a b*tch.’” This reaction is known as the overview effect. The astronaut who described it most vividly to me is space shuttle commander Jeff Ashby. He recalled that the first time he looked back at the Earth from outer space, it changed him forever: On Earth, astronauts look to the stars—most of us are star fanatics—but in space, the stars look the same as they do on Earth. What is so different is the planet—the perspective that it gives you. My first glimpse of the Earth from space was about fifteen minutes into my first flight, when I looked up from my checklist and suddenly we were over the lit part of the Earth with our windows facing down. Below me was the continent of Africa, and it was moving by much as a city would move by from an airline seat. Circling the entire planet in ninety minutes, you see that thin blue arc of the atmosphere. Seeing how fragile the little layer is in which all of humankind exists, you can easily from space see the connection between someone on one side of the planet to someone on the other—and there are no borders evident. So it appears as just this one common layer that we all exist in. When you get to see an overview of the Earth from outer space, you realize you share a common identity with all human beings. I wanted to create a version of the overview effect for baseball fans. There’s some evidence that common identity can build bridges between rivals. In one experiment, psychologists randomly assigned Manchester United soccer fans a short writing task. They then staged an emergency in which a passing runner slipped and fell, screaming in pain as he held his ankle. He was wearing the T-shirt of their biggest rival, and the question was whether they would stop to help him. If the soccer fans had just written about why they loved their team, only 30 percent helped. If they had written about what they had in common with other soccer fans, 70 percent helped. When Tim and I tried to get Red Sox and Yankees fans to reflect on their common identity as baseball fans, it didn’t work. They didn’t end up with more positive views of one another or a greater willingness to help one another outside emergency situations. Shared identity doesn’t stick in every circumstance. If a rival fan has just had an accident, thinking about a common identity might motivate us to help. If he’s not in danger or dire need, though, it’s too easy to dismiss him as just another jerk—or not our responsibility. “We both love baseball,” one Red Sox supporter commented. “The Yankees fans just like the wrong team.” Another stated that their shared love of baseball had no effect on his opinions: “The Yankees suck, and their fans are annoying.” ","date":"2022-09-03","objectID":"/b90_think_again/:17:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"HYPOTHESIS 2: FEELING FOR OUR FOES I next turned to the psychology of peace. Years ago the pioneering psychologist and Holocaust survivor Herb Kelman set out to challenge some of the stereotypes behind the Israel-Palestine conflict by teaching the two sides to understand and empathize with one another. He designed interactive problem-solving workshops in which influential Israeli and Palestinian leaders talked off the record about paths to peace. For years, they came together to share their own experiences and perspectives, address one another’s needs and fears, and explore novel solutions to the conflict. Over time, the workshops didn’t just shatter stereotypes—some of the participants ended up forming lifelong friendships. Humanizing the other side should be much easier in sports, because the stakes are lower and the playing field is more level. I started with another of the biggest rivalries in sports: UNC-Duke. I asked Shane Battier, who led Duke to an NCAA basketball championship in 2001, what it would take for him to root for UNC. His immediate reply: “If they were playing the Taliban.” I had no idea so many people fantasized about crushing terrorists in their favorite sport. I wondered whether humanizing a Duke student would change UNC students’ stereotypes of the group. In an experiment with my colleagues Alison Fragale and Karren Knowlton, we asked UNC students to help improve the job application of a peer. If we mentioned that he went to Duke rather than UNC, as long as he was facing significant financial need, participants spent extra time helping him. Once they felt empathy for his plight, they saw him as a unique individual deserving of assistance and liked him more. Yet when we measured their views of Duke students in general, the UNC students were just as likely to see them as their rivals, to say that it felt like a personal compliment if they heard someone criticize Duke, and to take it as a personal insult if they heard Duke praised. We had succeeded in changing their attitudes toward the student, but failed in changing their stereotypes of the group. Something similar happened when Tim and I tried to humanize a Yankees fan. We had Red Sox fans read a story written by a baseball buff who had learned the game as a child with his grandfather and had fond memories of playing catch with his mom. At the very end of the piece he mentioned that he was a die-hard supporter of the Yankees. “I think this person is very authentic and is a rare Yankee fan,” one Red Sox supporter commented. “This person gets it and is not your typical Yankee fan,” a second observed. “Ugh, I really liked this text until I got to the part about them being a Yankees fan,” a third fan lamented, but “I think this particular person I would have more in common with than the typical, stereotypical Yankees fan. This person is okay.” Herb Kelman ran into the same problem with Israelis and Palestinians. In the problem-solving workshops, they came to trust the individuals across the table, but they still held on to their stereotypes of the group. In an ideal world, learning about individual group members will humanize the group, but often getting to know a person better just establishes her as different from the rest of her group. When we meet group members who defy a stereotype, our first instinct isn’t to see them as exemplars and rethink the stereotype. It’s to see them as exceptions and cling to our existing beliefs. So that attempt also failed. Back to the drawing board again. ","date":"2022-09-03","objectID":"/b90_think_again/:17:4","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"HYPOTHESIS 3: BEASTS OF HABIT My all-time favorite commercial starts with a close-up of a man and a woman kissing. As the camera zooms out, you see that he’s wearing an Ohio State Buckeyes sweatshirt and she’s wearing a Michigan Wolverines T-shirt. The caption: “Without sports, this wouldn’t be disgusting.” As a lifelong Wolverine fan, I was raised to boo at Buckeye fans. My uncle filled his basement with Michigan paraphernalia, got up at 3:00 a.m. on Saturdays to start setting up for tailgates, and drove a van with the Michigan logo emblazoned on the side. When I went back home to Michigan for grad school and one of my college roommates started medical school at Ohio State, it was only natural for me to preach my school’s superiority by phone and prosecute his intelligence by text. A few years ago, I got to know an unusually kind woman in her seventies who works with Holocaust survivors. Last summer, when she mentioned that she had gone to Ohio State, my first response was “yuck.” My next reaction was to be disgusted with myself. Who cares where she went to school half a century ago? How did I get programmed this way? Suddenly it seemed odd that anyone would hate a team at all. In ancient Greece, Plutarch wrote of a wooden ship that Theseus sailed from Crete to Athens. To preserve the ship, as its old planks decayed, Athenians would replace them with new wood. Eventually all the planks had been replaced. It looked like the same ship, but none of its parts was the same. Was it still the same ship? Later, philosophers added a wrinkle: if you collected all the original planks and fashioned them into a ship, would that be the same ship? The ship of Theseus has a lot in common with a sports franchise. If you hail from Boston, you might hate the 1920 Yankees for taking Babe Ruth or the 1978 Yankees for dashing your World Series hopes. Although the current team carries the same name, the pieces are different. The players are long gone. So are the managers and coaches. The stadium has been replaced. “You’re actually rooting for the clothes,” Jerry Seinfeld quipped. “Fans will be so in love with a player, but if he goes to a different team, they boo him. This is the same human being in a different shirt; they hate him now. Boo! Different shirt! Boo!” I think it’s a ritual. A fun but arbitrary ritual—a ceremony that we perform out of habit. We imprinted on it when we were young and impressionable, or were new to a city and looking for esprit de corps. Sure, there are moments where team loyalty does matter in our lives: it allows us to high-five acquaintances at bars and hug strangers at victory parades. It gives us a sense of solidarity. If you reflect on it, though, hating an opposing team is an accident of birth. If you had been born in New York instead of Boston, would you really hate the Yankees? For our third approach, Tim and I recruited fans of the Red Sox and Yankees. To prove their allegiance, they had to correctly name one of their team’s players from a photo—and the last year his team had won the World Series. Then we took some steps to open their minds. First, to help them recognize the complexity of their own beliefs, we asked them to list three positives and three negatives about fans of the opposing team. You saw the most common negatives earlier, but they were able to come up with some positives, too: WHAT RED SOX FANS LIKE ABOUT YANKEES FANS WHAT YANKEES FANS LIKE ABOUT RED SOX FANS Then we randomly assigned half of them to go the extra step of reflecting on the arbitrariness of their animosity: Think and write about how Yankee fans and Red Sox fans dislike each other for reasons that are fairly arbitrary. For example, if you were born into a family of fans of the rival team, you would likely also be a fan of them today. To gauge their animosity toward their opponents, we gave them a chance to decide how spicy the hot sauce sold in the rival team’s stadium should be. The backstory was that consumer product researchers ","date":"2022-09-03","objectID":"/b90_think_again/:17:5","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"ENTERING A PARALLEL UNIVERSE Outside the lab, dismantling stereotypes and decreasing prejudice rarely happen overnight. Even if people aren’t on guard from the start, they’re quick to put their defenses up when their attitudes are challenged. Getting through to them requires more than just telling them that their views are arbitrary. A key step is getting them to do some counterfactual thinking: helping them consider what they’d believe if they were living in an alternative reality. In psychology, counterfactual thinking involves imagining how the circumstances of our lives could have unfolded differently. When we realize how easily we could have held different stereotypes, we might be more willing to update our views.* To activate counterfactual thinking, you might ask people questions like: How would your stereotypes be different if you’d been born Black, Hispanic, Asian, or Native American? What opinions would you hold if you’d been raised on a farm versus in a city, or in a culture on the other side of the world? What beliefs would you cling to if you lived in the 1700s? You’ve already learned from debate champions and expert negotiators that asking people questions can motivate them to rethink their conclusions. What’s different about these kinds of counterfactual questions is that they invite people to explore the origins of their own beliefs—and reconsider their stances toward other groups. People gain humility when they reflect on how different circumstances could have led them to different beliefs. They might conclude that some of their past convictions had been too simplistic and begin to question some of their negative views. That doubt could leave them more curious about groups they’ve stereotyped, and they might end up discovering some unexpected commonalities. Recently, I stumbled onto an opportunity to encourage some counterfactual thinking. A startup founder asked me to join an all-hands meeting to share insights on how to better understand other people’s personalities and our own. During our virtual fireside chat, she mentioned that she was an astrology fan and the company was full of them. I wondered if I could get some of them to see that they held inaccurate stereotypes about people based on the month in which they happened to be born. Here’s an excerpt of what happened: Me: You know we have no evidence whatsoever that horoscopes influence personality, right? Founder: That’s such a Capricorn thing to say. Me: I think I’m a Leo. I’d love to find out what evidence would change your mind. Founder: So my partner has been trying for as long as we’ve been dating. He’s given up. There’s nothing that can convince me otherwise. Me: Then you’re not thinking like a scientist. This is a religion for you. Founder: Yeah, well, maybe a little. Me: What if you’d been born in China instead of the U.S.? Some evidence just came out that if you’re a Virgo in China, you get discriminated against in hiring and also in dating. These poor Virgos are stereotyped as being difficult and ornery.* Founder: So in the West, Adam, that same discrimination happens to Scorpios. Although the founder started out resistant to my argument, after considering how she might hold different stereotypes if she lived in China, she recognized a familiar pattern. She’d seen an entire group of people mistreated as a result of the positions of the sun and the moon on the day they happened to enter the world. Realizing how unfair discrimination based on zodiac signs was, the founder ended up jumping in to help me build my case. As we wrapped up the conversation, I offered to do a follow-up discussion on the science of personality. More than a quarter of the company signed up to participate. Afterward, one of the participants wrote that “the biggest takeaway from this chat is the importance of ‘unlearning’ things to avoid being ignorant.” Having grasped how arbitrary their stereotypes were, people were now more open to rethinking their views. Psychologists ","date":"2022-09-03","objectID":"/b90_think_again/:17:6","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"HOW A BLACK MUSICIAN CONFRONTS WHITE SUPREMACISTS One day, Daryl was driving his car with the chief officer of a KKK chapter, whose official title was Exalted Cyclops. Before long, the Cyclops was sharing his stereotypes of Black people. They were an inferior species, he said—they had smaller brains, which made them unintelligent, and a genetic predisposition toward violence. When Daryl pointed out that he was Black but had never shot anyone or stolen a car, the Cyclops told him his criminal gene must be latent. It hadn’t come out yet. Daryl decided to beat the Cyclops at his own game. He challenged him to name three Black serial killers. When the Cyclops couldn’t name any, Daryl rattled off a long list of well-known white serial killers and told the Cyclops that he must be one. When the Cyclops protested that he’d never killed anybody, Daryl turned his own argument against him and said that his serial-killer gene must be latent. “Well, that’s stupid,” the flustered Cyclops replied. “Well, duh!” Daryl agreed. “You’re right. What I said about you was stupid, but no more stupid than what you said about me.” The Cyclops got very quiet and changed the subject. Several months later, he told Daryl that he was still thinking about that conversation. Daryl had planted a seed of doubt and made him curious about his own beliefs. The Cyclops ended up quitting the KKK and giving his hood and his robe to Daryl. Daryl is obviously extraordinary—not only in his ability to wage a one-man war on prejudice, but also in his inclination to do so. As a general rule, it’s those with greater power who need to do more of the rethinking, both because they’re more likely to privilege their own perspectives and because their perspectives are more likely to go unquestioned. In most cases, the oppressed and marginalized have already done a great deal of contortion to fit in. Having been the target of racism since childhood, Daryl had a lifetime of legitimate reasons to harbor animosity toward white people. He was still willing to approach white supremacists with an open mind and give them the opportunity to rethink their views. But it shouldn’t have been Daryl’s responsibility to challenge white supremacists and put himself at risk. In an ideal world, the Cyclops would have taken it upon himself to educate his peers. Some other former KKK members have stepped up, working independently and with Daryl to advocate for the oppressed and reform the structures that produce oppression in the first place. As we work toward systemic change, Daryl urges us not to overlook the power of conversation. When we choose not to engage with people because of their stereotypes or prejudice, we give up on opening their minds. “We are living in space-age times, yet there are still so many of us thinking with stone-age minds,” he reflects. “Our ideology needs to catch up to our technology.” He estimates that he has helped upwards of two hundred white supremacists rethink their beliefs and leave the KKK and other neo-Nazi groups. Many of them have gone on to educate their families and friends. Daryl is quick to point out that he hasn’t directly persuaded these men to change their minds. “I didn’t convert anybody,” he says. “I gave them reason to think about their direction in life, and they thought about it, and thought, ‘I need a better path, and this is the way to go.’” Daryl doesn’t do this by preaching or prosecuting. When he begins a dialogue with white supremacists, many are initially surprised by his thoughtfulness. As they start to see him as an individual and spend more time with him, they often tap into a common identity around shared interests in topics like music. Over time, he helps them see that they joined these hate groups for reasons that weren’t their own—it was a family tradition dating back multiple generations, or someone had told them their jobs were being taken by Black men. As they realize how little they truly know about other groups, and how shallow ste","date":"2022-09-03","objectID":"/b90_think_again/:17:7","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 7 ","date":"2022-09-03","objectID":"/b90_think_again/:18:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Vaccine Whisperers and Mild-Mannered Interrogators ","date":"2022-09-03","objectID":"/b90_think_again/:19:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"How the Right Kind of Listening Motivates People to Change It’s a rare person who wants to hear what he doesn’t want to hear. —attributed to dick cavett When Marie-Hélène Étienne-Rousseau went into labor, she broke down in tears. It was September 2018, and her baby wasn’t due until December. Just before midnight, Tobie arrived, weighing just two pounds. His body was so tiny that his head could fit in the palm of her hand, and Marie-Hélène was terrified that he wouldn’t survive. Tobie spent only a few seconds in her arms before he was rushed to the neonatal intensive care unit. He needed a mask to breathe and was soon taken to surgery for internal bleeding. It would be months before he was allowed to go home. While Tobie was still in the hospital, Marie-Hélène was shopping for diapers when she saw a headline about measles spreading in her province of Quebec. She hadn’t had Tobie vaccinated. It wasn’t even a question—he seemed too fragile. She hadn’t vaccinated her three other children, either; it wasn’t the norm in her community. Her friends and neighbors took it for granted that vaccines were dangerous and passed around horror stories about their side effects. Still, the fact remained: Quebec had already had two serious measles outbreaks that decade. Today in the developed world, measles is on the rise for the first time in at least half a century, and its mortality rate is around one in a thousand. In the developing world, it’s closer to one in a hundred. Estimates suggest that from 2016 to 2018, measles deaths spiked worldwide by 58 percent, with over a hundred thousand casualties. These deaths could have been prevented by the vaccine, which has saved roughly 20 million lives in the past two decades. Although epidemiologists recommend two doses of the measles vaccine and a minimum immunization rate of 95 percent, around the globe only 85 percent of people get the first dose and just 67 percent continue to the second. Many of those who skip the shot simply do not believe in the science. Government officials have tried to prosecute the problem, some warning that the unvaccinated could be fined up to a thousand dollars and sentenced to jail for up to six months. Many schools shut their doors to unvaccinated children, and one county even banned them from enclosed public places. When such measures failed to solve the problem, public officials turned to preaching. Since people held unfounded fears about vaccines, it was time to educate them with a dose of the truth. The results were often disappointing. In a pair of experiments in Germany, introducing people to the research on vaccine safety backfired: they ended up seeing vaccines as riskier. Similarly, when Americans read accounts of the dangers of measles, saw pictures of children suffering from it, or learned of an infant who nearly died from it, their interest in vaccination didn’t rise at all. And when they were informed that there was no evidence that the measles vaccine causes autism, those who already had concerns actually became less interested in vaccinating. It seemed that no logical argument or data-driven explanation could shake their conviction that vaccines were unsafe. This is a common problem in persuasion: what doesn’t sway us can make our beliefs stronger. Much like a vaccine inoculates our physical immune system against a virus, the act of resistance fortifies our psychological immune system. Refuting a point of view produces antibodies against future influence attempts. We become more certain of our opinions and less curious about alternative views. Counterarguments no longer surprise us or stump us—we have our rebuttals ready. Marie-Hélène Étienne-Rousseau had been through that journey. Visits to the doctor with her older kids followed a familiar script. The doctor extolled the benefits of vaccines, warned her about the risks of refusing them, and stuck to generic messaging instead of engaging with her particular questions. The whole experience reeked of con","date":"2022-09-03","objectID":"/b90_think_again/:20:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"MOTIVATING THROUGH INTERVIEWING In the early 1980s, a clinical psychologist named Bill Miller was troubled by his field’s attitude toward people with addictions. It was common for therapists and counselors to accuse their substance-abusing clients of being pathological liars who were living in denial. That didn’t track with what Miller was seeing up close in his own work treating people with alcohol problems, where preaching and prosecuting typically boomeranged. “People who drink too much are usually aware of it,” Miller told me. “If you try to persuade them that they do drink too much or need to make a change, you evoke resistance, and they are less likely to change.” Instead of attacking or demeaning his clients, Miller started asking them questions and listening to their answers. Soon afterward, he published a paper on his philosophy, which found its way into the hands of Stephen Rollnick, a young nurse trainee working in addiction treatment. A few years later, the two happened to meet in Australia and realized that what they were exploring was much bigger than just a new approach to treatment. It was an entirely different way of helping people change. Together, they developed the core principles of a practice called motivational interviewing. The central premise is that we can rarely motivate someone else to change. We’re better off helping them find their own motivation to change. Let’s say you’re a student at Hogwarts, and you’re worried your uncle is a fan of Voldemort. A motivational interview might go like this: You: I’d love to better understand your feelings about He Who Must Not Be Named. Uncle: Well, he’s the most powerful wizard alive. Also, his followers promised me a fancy title. You: Interesting. Is there anything you dislike about him? Uncle: Hmm. I’m not crazy about all the murdering. You: Well, nobody’s perfect. Uncle: Yeah, but the killing is really bad. You: Sounds like you have some reservations about Voldemort. What’s stopped you from abandoning him? Uncle: I’m afraid he might direct the murdering toward me. You: That’s a reasonable fear. I’ve felt it too. I’m curious: are there any principles that matter so deeply to you that you’d be willing to take that risk? Motivational interviewing starts with an attitude of humility and curiosity. We don’t know what might motivate someone else to change, but we’re genuinely eager to find out. The goal isn’t to tell people what to do; it’s to help them break out of overconfidence cycles and see new possibilities. Our role is to hold up a mirror so they can see themselves more clearly, and then empower them to examine their beliefs and behaviors*.* That can activate a rethinking cycle, in which people approach their own views more scientifically. They develop more humility about their knowledge, doubt in their convictions, and curiosity about alternative points of view. The process of motivational interviewing involves three key techniques: Asking open-ended questions Engaging in reflective listening Affirming the person’s desire and ability to change As Marie-Hélène was getting ready to take Tobie home, the vaccine whisperer the nurses called was a neonatologist and researcher named Arnaud Gagneur. His specialty was applying the techniques of motivational interviewing to vaccination discussions. When Arnaud sat down with Marie-Hélène, he didn’t judge her for not vaccinating her children, nor did he order her to change. He was like a scientist or “a less abrasive Socrates,” as journalist Eric Boodman described him in reporting on their meeting. Arnaud told Marie-Hélène he was afraid of what might happen if Tobie got the measles, but he accepted her decision and wanted to understand it better. For over an hour, he asked her open-ended questions about how she had reached the decision not to vaccinate. He listened carefully to her answers, acknowledging that the world is full of confusing information about vaccine safety. At the end of the discussion, Arnaud reminded","date":"2022-09-03","objectID":"/b90_think_again/:20:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"BEYOND THE CLINIC Years ago I got a call asking for help from a biotechnology startup. The CEO, Jeff, was a scientist by training; he liked to have all the necessary data before making a decision. After more than a year and a half at the helm, he still hadn’t rolled out a vision for the company, and it was in danger of failing. A trio of consultants tried to convince him to offer some direction, and he fired them all. Before the head of HR threw in the towel, she threw a Hail Mary pass and contacted an academic. It was the perfect time for a motivational interview: Jeff seemed reluctant to change, and I had no idea why. When we met, I decided to see if I could help him find his motivation to change. Here are the pivotal moments from our conversation: Me: I really enjoy being the guy who gets hired after three consultants get fired. I’d love to hear how they screwed up. Jeff: The first consultant gave me answers instead of asking questions. That was arrogant: how could he solve a problem before he’d even taken the time to understand it? The next two did a better job learning from me, but they still ended up trying to tell me how to do my job. Me: So why did you bother to bring in another outsider? Jeff: I’m looking for some fresh ideas on leadership. Me: It’s not my place to tell you how to lead. What does leadership mean to you? Jeff: Making systemic decisions, having a well-thought-out strategy. Me: Are there any leaders you admire for those qualities? Jeff: Abraham Lincoln, Martin Luther King Jr., Steve Jobs. That was a turning point. In motivational interviewing, there’s a distinction between sustain talk and change talk. Sustain talk is commentary about maintaining the status quo. Change talk is referencing a desire, ability, need, or commitment to make adjustments. When contemplating a change, many people are ambivalent—they have some reasons to consider it but also some reasons to stay the course. Miller and Rollnick suggest asking about and listening for change talk, and then posing some questions about why and how they might change. Say you have a friend who mentions a desire to stop smoking. You might respond by asking why she’s considering quitting. If she says a doctor recommended it, you might follow up by inquiring about her own motivations: what does she think of the idea? If she offers a reason why she’s determined to stop, you might ask what her first step toward quitting could be. “Change talk is a golden thread,” clinical psychologist Theresa Moyers says. “What you need to do is you need to pick that thread up and pull it.” So that’s what I did with Jeff. Me: What do you appreciate most about the leaders you named? Jeff: They all had vivid visions. They inspired people to achieve extraordinary things. Me: Interesting. If Steve Jobs were in your shoes right now, what do you think he’d do? Jeff: He’d probably get his leadership team fired up about a bold idea and create a reality distortion field to make it seem possible. Maybe I should do that, too. A few weeks later, Jeff stood up at an executive off-site to deliver his first-ever vision speech. When I heard about it, I was beaming with pride: I had conquered my inner logic bully and led him to find his own motivation. Unfortunately, the board ended up shutting down the company anyway. Jeff’s speech had fallen flat. He stumbled through notes on a napkin and didn’t stir up enthusiasm about the company’s direction. I had overlooked a key step—helping him think about how to execute the change effectively. There’s a fourth technique of motivational interviewing, which is often recommended for the end of a conversation and for transition points: summarizing. The idea is to explain your understanding of other people’s reasons for change, to check on whether you’ve missed or misrepresented anything, and to inquire about their plans and possible next steps. The objective is not to be a leader or a follower, but a guide. Miller and Rollnick liken it to hiring a tour g","date":"2022-09-03","objectID":"/b90_think_again/:20:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE ART OF INFLUENTIAL LISTENING Betty Bigombe had already hiked eight miles through the jungle, and there was still no sign of life. She was no stranger to a long walk: growing up in northern Uganda, she walked four miles each way to school. She subsisted on one meal a day in a communal homestead where her uncle had eight wives. Now she had made it all the way to the Ugandan Parliament, and she was undertaking a challenge that none of her colleagues would brave: trying to make peace with a warlord. Joseph Kony was the leader of the Lord’s Resistance Army. He and his rebel group would eventually be held responsible for murdering over a hundred thousand people, abducting over thirty thousand children, and displacing over two million Ugandans. In the early 1990s, Betty convinced the Ugandan president to send her in to see if she could stop the violence. When Betty finally made contact with the rebels after months of effort, they were insulted at the prospect of negotiating with a woman. Yet Betty negotiated her way into getting permission to meet Kony himself. Soon he was referring to her as Mummy, and he even agreed to leave the jungle to start peace talks. Although the peace effort didn’t succeed, opening Kony’s mind to conversation was a remarkable accomplishment in itself.* For her efforts to end the violence, Betty was named Uganda’s Woman of the Year. When I spoke to her recently, I asked how she had succeeded in getting through to Kony and his people. The key, she explained, was not persuading or even coaxing, but listening. Listening well is more than a matter of talking less. It’s a set of skills in asking and responding. It starts with showing more interest in other people’s interests rather than trying to judge their status or prove our own. We can all get better at asking “truly curious questions that don’t have the hidden agenda of fixing, saving, advising, convincing or correcting,” journalist Kate Murphy writes, and helping to “facilitate the clear expression of another person’s thoughts.”* When we’re trying to get people to change, that can be a difficult task. Even if we have the best intentions, we can easily slip into the mode of a preacher perched on a pulpit, a prosecutor making a closing argument, or a politician giving a stump speech. We’re all vulnerable to the “righting reflex,” as Miller and Rollnick describe it—the desire to fix problems and offer answers. A skilled motivational interviewer resists the righting reflex—although people want a doctor to fix their broken bones, when it comes to the problems in their heads, they often want sympathy rather than solutions. That’s what Betty Bigombe set out to provide in Uganda. She started traveling through rural areas to visit camps for internally displaced people. She figured some might have relatives in Joseph Kony’s army and might know something of his whereabouts. Although she hadn’t been trained in motivational interviewing, she intuitively understood the philosophy. At each camp, she announced to people that she wasn’t there to lecture them, but to listen to them. Her curiosity and confident humility caught the Ugandans by surprise. Other peacemakers had come in ordering them to stop fighting. They had preached about their own plans for conflict resolution and prosecuted the past efforts that failed. Now Betty, a politician by profession, wasn’t telling them what to do. She just sat patiently for hours in front of a bonfire, taking notes and chiming in from time to time to ask questions. “If you want to call me names, feel free to do so,” she said. “If you want me to leave, I will.” To demonstrate her commitment to peace, Betty stayed in the camps even though they lacked sufficient food and proper sanitation. She invited people to air their grievances and suggest remedial measures to be taken. They told her that it was rare and refreshing for an outsider to give them the opportunity to share their views. She empowered them to generate their own solutio","date":"2022-09-03","objectID":"/b90_think_again/:20:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 8 ","date":"2022-09-03","objectID":"/b90_think_again/:21:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Charged Conversations ","date":"2022-09-03","objectID":"/b90_think_again/:22:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Depolarizing Our Divided Discussions When conflict is cliché, complexity is breaking news. —amanda ripley Eager to have a jaw-clenching, emotionally fraught argument about abortion? How about immigration, the death penalty, or climate change? If you think you can handle it, head for the second floor of a brick building on the Columbia University campus in New York. It’s the home of the Difficult Conversations Lab. If you’re brave enough to visit, you’ll be matched up with a stranger who strongly disagrees with your views on a controversial topic. You’ll be given just twenty minutes to discuss the issue, and then you’ll both have to decide whether you’ve aligned enough to write and sign a joint statement on your shared views around abortion laws. If you’re able to do so—no small feat—your statement will be posted on a public forum. For two decades, the psychologist who runs the lab, Peter T. Coleman, has been bringing people together to talk about polarizing issues. His mission is to reverse-engineer the successful conversations and then experiment with recipes to make more of them. To put you in the right mindset before you begin your conversation about abortion, Peter gives you and the stranger a news article about another divisive issue: gun control. What you don’t know is that there are different versions of the gun control article, and which one you read is going to have a major impact on whether you land on the same page about abortion. If the gun control article covers both sides of the issue, making a balanced case for both gun rights and gun legislation, you and your adversary have a decent chance at reaching consensus on abortion. In one of Peter’s experiments, after reading a “both-sides” article, 46 percent of pairs were able to find enough common ground to draft and sign a statement together. That’s a remarkable result. But Peter went on to do something far more impressive. He randomly assigned some pairs to read another version of the same article, which led 100 percent of them to generate and sign a joint statement about abortion laws. That version of the article featured the same information but presented it differently. Instead of describing the issue as a black-and-white disagreement between two sides, the article framed the debate as a complex problem with many shades of gray, representing a number of different viewpoints. At the turn of the last century, the great hope for the internet was that it would expose us to different views. But as the web welcomed a few billion fresh voices and vantage points into the conversation, it also became a weapon of misinformation and disinformation. By the 2016 elections, as the problem of political polarization became more extreme and more visible, the solution seemed obvious to me. We needed to burst filter bubbles in our news feeds and shatter echo chambers in our networks. If we could just show people the other side of an issue, they would open their minds and become more informed. Peter’s research challenges that assumption. We now know that where complicated issues are concerned, seeing the opinions of the other side is not enough. Social media platforms have exposed us to them, but they haven’t changed our minds. Knowing another side exists isn’t sufficient to leave preachers doubting whether they’re on the right side of morality, prosecutors questioning whether they’re on the right side of the case, or politicians wondering whether they’re on the right side of history. Hearing an opposing opinion doesn’t necessarily motivate you to rethink your own stance; it makes it easier for you to stick to your guns (or your gun bans). Presenting two extremes isn’t the solution; it’s part of the polarization problem. Psychologists have a name for this: binary bias. It’s a basic human tendency to seek clarity and closure by simplifying a complex continuum into two categories. To paraphrase the humorist Robert Benchley, there are two kinds of people: those who divide the world i","date":"2022-09-03","objectID":"/b90_think_again/:23:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"SOME INCONVENIENT TRUTHS In 2006, Al Gore starred in a blockbuster film on climate change, An Inconvenient Truth. It won the Academy Award for Best Documentary and spawned a wave of activism, motivating businesses to go green and governments to pass legislation and sign landmark agreements to protect the planet. History teaches us that it sometimes takes a combination of preaching, prosecuting, and politicking to fuel that kind of dramatic swing. Yet by 2018, only 59 percent of Americans saw climate change as a major threat—and 16 percent believed it wasn’t a threat at all. Across many countries in Western Europe and Southeast Asia, higher percentages of the populations had opened their minds to the evidence that climate change is a dire problem. In the past decade in the United States, beliefs about climate change have hardly budged. This thorny issue is a natural place to explore how we can bring more complexity into our conversations. Fundamentally, that involves drawing attention to the nuances that often get overlooked. It starts with seeking and spotlighting shades of gray. A fundamental lesson of desirability bias is that our beliefs are shaped by our motivations. What we believe depends on what we want to believe. Emotionally, it can be unsettling for anyone to admit that all life as we know it might be in danger, but Americans have some additional reasons to be dubious about climate change. Politically, climate change has been branded in the United States as a liberal issue; in some conservative circles, merely acknowledging the fact that it might exist puts people on a fast track to exile. There’s evidence that higher levels of education predict heightened concern about climate change among Democrats but dampened concern among Republicans. Economically, we remain confident that America will be more resilient in response to a changing climate than most of the world, and we’re reluctant to sacrifice our current ways of achieving prosperity. These deep-seated beliefs are hard to change. As a psychologist, I want to zoom in on another factor. It’s one we can all control: the way we communicate about climate change. Many people believe that preaching with passion and conviction is necessary for persuasion. A clear example is Al Gore. When he narrowly lost the U.S. presidential election in 2000, one of the knocks against him was his energy—or lack thereof. People called him dry. Boring. Robotic. Fast-forward a few years: his film was riveting and his own platform skills had evolved dramatically. In 2016, when I watched Gore speak in the red circle at TED, his language was vivid, his voice pulsated with emotion, and his passion literally dripped off him in the form of sweat. If a robot was ever controlling his brain, it short-circuited and left the human in charge. “Some still doubt that we have the will to act,” he boomed, “but I say the will to act is itself a renewable resource.” The audience erupted in a standing ovation, and afterward he was called the Elvis of TED. If it’s not his communication style that’s failing to reach people, what is? At TED, Gore was preaching to the choir: his audience was heavily progressive. For audiences with more varied beliefs, his language hasn’t always resonated. In An Inconvenient Truth, Gore contrasted the “truth” with claims made by “so-called skeptics.” In a 2010 op-ed, he contrasted scientists with “climate deniers.” This is binary bias in action. It presumes that the world is divided into two sides: believers and nonbelievers. Only one side can be right, because there is only one truth. I don’t blame Al Gore for taking that position; he was presenting rigorous data and representing the consensus of the scientific community. Because he was a recovering politician, seeing two sides to an issue must have been second nature. But when the only available options are black and white, it’s natural to slip into a mentality of us versus them and to focus on the sides over the science. For t","date":"2022-09-03","objectID":"/b90_think_again/:23:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"SOME CAVEATS AND CONTINGENCIES If you want to get better at conveying complexity, it’s worth taking a close look at how scientists communicate. One key step is to include caveats. It’s rare that a single study or even a series of studies is conclusive. Researchers typically feature multiple paragraphs about the limitations of each study in their articles. We see them less as holes in our work and more as portholes to future discoveries. When we share the findings with nonscientists, though, we sometimes gloss over these caveats. That’s a mistake, according to recent research. In a series of experiments, psychologists demonstrated that when news reports about science included caveats, they succeeded in capturing readers’ interest and keeping their minds open. Take a study suggesting that a poor diet accelerates aging. Readers were just as engaged in the story—but more flexible in their beliefs—when it mentioned that scientists remained hesitant to draw strong causal conclusions given the number of factors that can affect aging. It even helped just to note that scientists believed more work needed to be done in this area. We can also convey complexity by highlighting contingencies. Every empirical finding raises unanswered questions about when and where results will be replicated, nullified, or reversed. Contingencies are all the places and populations where an effect may change. Consider diversity: although headlines often say “Diversity is good,” the evidence is full of contingencies. Although diversity of background and thought has the potential to help groups think more broadly and process information more deeply, that potential is realized in some situations but not others. New research reveals that people are more likely to promote diversity and inclusion when the message is more nuanced (and more accurate): “Diversity is good, but it isn’t easy.”* Acknowledging complexity doesn’t make speakers and writers less convincing; it makes them more credible. It doesn’t lose viewers and readers; it maintains their engagement while stoking their curiosity. In social science, rather than cherry-picking information to fit our existing narratives, we’re trained to ask whether we should rethink and revise those narratives. When we find evidence that doesn’t fit neatly into our belief systems, we’re expected to share it anyway.* In some of my past writing for the public, though, I regret not having done enough to emphasize areas where evidence was incomplete or conflicting. I sometimes shied away from discussing mixed results because I didn’t want to leave readers confused. Research suggests that many writers fall into the same trap, caught up in trying to “maintain a consistent narrative rather than an accurate record.” A fascinating example is the divide around emotional intelligence. On one extreme is Daniel Goleman, who popularized the concept. He preaches that emotional intelligence matters more for performance than cognitive ability (IQ) and accounts for “nearly 90 percent” of success in leadership jobs. At the other extreme is Jordan Peterson, writing that “There is NO SUCH THING AS EQ” and prosecuting emotional intelligence as “a fraudulent concept, a fad, a convenient band-wagon, a corporate marketing scheme.” Both men hold doctorates in psychology, but neither seems particularly interested in creating an accurate record. If Peterson had bothered to read the comprehensive meta-analyses of studies spanning nearly two hundred jobs, he’d have discovered that—contrary to his claims—emotional intelligence is real and it does matter. Emotional intelligence tests predict performance even after controlling for IQ and personality. If Goleman hadn’t ignored those same data, he’d have learned that if you want to predict performance across jobs, IQ is more than twice as important as emotional intelligence (which accounts for only 3 to 8 percent of performance). I think they’re both missing the point. Instead of arguing about whether emotio","date":"2022-09-03","objectID":"/b90_think_again/:23:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"MIXED FEELINGS In polarized discussions, a common piece of advice is to take the other side’s perspective. In theory, putting ourselves in another person’s shoes enables us to walk in lockstep with them. In practice, though, it’s not that simple. In a pair of experiments, randomly assigning people to reflect on the intentions and interests of their political opposites made them less receptive to rethinking their own attitudes on health care and universal basic income. Across twenty-five experiments, imagining other people’s perspectives failed to elicit more accurate insights—and occasionally made participants more confident in their own inaccurate judgments. Perspective-taking consistently fails because we’re terrible mind readers. We’re just guessing. If we don’t understand someone, we can’t have a eureka moment by imagining his perspective. Polls show that Democrats underestimate the number of Republicans who recognize the prevalence of racism and sexism—and Republicans underestimate the number of Democrats who are proud to be Americans and oppose open borders. The greater the distance between us and an adversary, the more likely we are to oversimplify their actual motives and invent explanations that stray far from their reality. What works is not perspective-taking but perspective-seeking: actually talking to people to gain insight into the nuances of their views. That’s what good scientists do: instead of drawing conclusions about people based on minimal clues, they test their hypotheses by striking up conversations. For a long time, I believed that the best way to make those conversations less polarizing was to leave emotions out of them. If only we could keep our feelings off the table, we’d all be more open to rethinking. Then I read evidence that complicated my thinking. It turns out that even if we disagree strongly with someone on a social issue, when we discover that she cares deeply about the issue, we trust her more. We might still dislike her, but we see her passion for a principle as a sign of integrity. We reject the belief but grow to respect the person behind it. It can help to make that respect explicit at the start of a conversation. In one experiment, if an ideological opponent merely began by acknowledging that “I have a lot of respect for people like you who stand by their principles,” people were less likely to see her as an adversary—and showed her more generosity. When Peter Coleman brings people together in his Difficult Conversations Lab, he plays them the recording of their discussions afterward. What he wants to learn is how they were feeling, moment by moment, as they listen to themselves. After studying over five hundred of these conversations, he found that the unproductive ones feature a more limited set of both positive and negative emotions, as illustrated below in the image on the left. People get trapped in emotional simplicity, with one or two dominant feelings. As you can see with the duo on the right, the productive conversations cover a much more varied spectrum of emotions. They’re not less emotional—they’re more emotionally complex. At one point, people might be angry about the other person’s views, but by the next minute they’re curious to learn more. Soon they could be shifting into anxiety and then excitement about considering a new perspective. Sometimes they even stumble into the joy of being wrong. In a productive conversation, people treat their feelings as a rough draft. Like art, emotions are works in progress. It rarely serves us well to frame our first sketch. As we gain perspective, we revise what we feel. Sometimes we even start over from scratch. What stands in the way of rethinking isn’t the expression of emotion; it’s a restricted range of emotion. So how do we infuse our charged conversations with greater emotional variety—and thereby greater potential for mutual understanding and rethinking? It helps to remember that we can fall victim to binary bias with emotions","date":"2022-09-03","objectID":"/b90_think_again/:23:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 9 ","date":"2022-09-03","objectID":"/b90_think_again/:24:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Rewriting the Textbook ","date":"2022-09-03","objectID":"/b90_think_again/:25:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Teaching Students to Question Knowledge No schooling was allowed to interfere with my education. —grant allen Adecade ago, if you had told Erin McCarthy she would become a teacher, she would have laughed. When she graduated from college, the last thing she wanted to do was teach. She was fascinated by history but bored by her social studies classes. Searching for a way to breathe life into overlooked objects and forgotten events, Erin started her career working in museums. Before long, she found herself writing a resource manual for teachers, leading school tours, and engaging students in interactive programs. She realized that the enthusiasm she saw on field trips was missing in too many classrooms, and she decided to do something about it. For the past eight years, Erin has taught social studies in the Milwaukee area. Her mission is to cultivate curiosity about the past, but also to motivate students to update their knowledge in the present. In 2020, she was named Wisconsin’s Teacher of the Year. One day, an eighth grader complained that the reading assignment from a history textbook was inaccurate. If you’re a teacher, that kind of criticism could be a nightmare. Using an outdated textbook would be a sign that you don’t know your material, and it would be embarrassing if your students noticed the error before you did. But Erin had assigned that particular reading intentionally. She collects old history books because she enjoys seeing how the stories we tell change over time, and she decided to give her students part of a textbook from 1940. Some of them just accepted the information it presented at face value. Through years of education, they had come to take it for granted that textbooks told the truth. Others were shocked by errors and omissions. It was ingrained in their minds that their readings were filled with incontrovertible facts. The lesson led them to start thinking like scientists and questioning what they were learning: whose story was included, whose was excluded, and what were they missing if only one or two perspectives were shared? After opening her students’ eyes to the fact that knowledge can evolve, Erin’s next step was to show them that it’s always evolving. To set up a unit on expansion in the West, she created her own textbook section describing what it’s like to be a middle-school student today. All the protagonists were women and girls, and all the generic pronouns were female. In the first year she introduced the material, a student raised his hand to point out that the boys were missing. “But there’s one boy,” Erin replied. “Boys were around. They just weren’t doing anything important.” It was an aha moment for the student: he suddenly realized what it was like for an entire group to be marginalized for hundreds of years. My favorite assignment of Erin’s is her final one. As a passionate champion of inquiry-based learning, she sends her eighth graders off to do self-directed research in which they inspect, investigate, interrogate, and interpret. Their active learning culminates in a group project: they pick a chapter from their textbook, choosing a time period that interests them and a theme in history that they see as underrepresented. Then they go off to rewrite it. One group took on the civil rights chapter for failing to cover the original March on Washington, which was called off at the last minute in the early 1940s but inspired Martin Luther King Jr.’s historic march two decades later. Other groups revised the chapter on World War II to include the infantry regiments of Hispanic soldiers and second-generation Japanese soldiers who fought for the U.S. Army. “It’s a huge light-bulb moment,” Erin told me. Even if you’re not a teacher by profession, you probably have roles in which you spend time educating others—whether as a parent, a mentor, a friend, or a colleague. In fact, every time we try to help someone think again, we’re doing a kind of education. Whether we do our instruction in a cl","date":"2022-09-03","objectID":"/b90_think_again/:26:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"LEARNING, INTERRUPTED Looking back on my own early education, one of my biggest disappointments is that I never got to fully experience the biggest upheavals in science. Long before it ever occurred to me to be curious about the cosmos, my teachers started demystifying it in kindergarten. I often wonder how I would have felt if I was a teenager when I first learned that we don’t live on a static, flat disc, but on a spinning, moving sphere. I hope I would have been stunned, and that disbelief would have quickly given way to curiosity and eventually the awe of discovery and the joy of being wrong. I also suspect it would have been a life-changing lesson in confident humility. If I could be that mistaken about what was under my own two feet, how many other so-called truths were actually question marks? Sure, I knew that many earlier generations of humans had gotten it wrong, but there’s a huge difference between learning about other people’s false beliefs and actually learning to unbelieve things ourselves. I realize this thought experiment is wildly impractical. It’s hard enough to keep kids in the dark about Santa Claus or the Tooth Fairy. Even if we could pull off such a delay, there’s a risk that some students would seize and freeze on what they learned early on. They could become trapped in an overconfidence cycle where pride in false knowledge fuels conviction, and confirmation and desirability biases lead to validation. Before you know it, we might have a whole nation of flat-earthers. Evidence shows that if false scientific beliefs aren’t addressed in elementary school, they become harder to change later. “Learning counterintuitive scientific ideas [is] akin to becoming a fluent speaker of a second language,” psychologist Deborah Kelemen writes. It’s “a task that becomes increasingly difficult the longer it is delayed, and one that is almost never achieved with only piecemeal instruction and infrequent practice.” That’s what kids really need: frequent practice at unlearning, especially when it comes to the mechanisms of how cause and effect work. In the field of history education, there’s a growing movement to ask questions that don’t have a single right answer. In a curriculum developed at Stanford, high school students are encouraged to critically examine what really caused the Spanish-American War, whether the New Deal was a success, and why the Montgomery bus boycott was a watershed moment. Some teachers even send students out to interview people with whom they disagree. The focus is less on being right, and more on building the skills to consider different views and argue productively about them. That doesn’t mean all interpretations are accepted as valid. When the son of a Holocaust survivor came to her class, Erin McCarthy told her students that some people denied the existence of the Holocaust, and taught them to examine the evidence and reject those false claims. This is part of a broader movement to teach kids to think like fact-checkers: the guidelines include (1) “interrogate information instead of simply consuming it,” (2) “reject rank and popularity as a proxy for reliability,” and (3) “understand that the sender of information is often not its source.” These principles are valuable beyond the classroom. At our family dinner table, we sometimes hold myth-busting discussions. My wife and I have shared how we learned in school that Pluto was a planet (not true anymore) and Columbus discovered America (never true). Our kids have taught us that King Tut probably didn’t die in a chariot accident and gleefully explained that when sloths do their version of a fart, the gas comes not from their behinds but from their mouths. Rethinking needs to become a regular habit. Unfortunately, traditional methods of education don’t always allow students to form that habit. ","date":"2022-09-03","objectID":"/b90_think_again/:26:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE DUMBSTRUCK EFFECT It’s week twelve of physics class, and you get to attend a couple of sessions with a new, highly rated instructor to learn about static equilibrium and fluids. The first session is on statics; it’s a lecture. The second is on fluids, and it’s an active-learning session. One of your roommates has a different, equally popular instructor who does the opposite—using active learning for statics and lecturing on fluids. In both cases the content and the handouts are identical; the only difference is the delivery method. During the lecture the instructor presents slides, gives explanations, does demonstrations, and solves sample problems, and you take notes on the handouts. In the active-learning session, instead of doing the example problems himself, the instructor sends the class off to figure them out in small groups, wandering around to ask questions and offer tips before walking the class through the solution. At the end, you fill out a survey. In this experiment the topic doesn’t matter: the teaching method is what shapes your experience. I expected active learning to win the day, but the data suggest that you and your roommate will both enjoy the subject more when it’s delivered by lecture. You’ll also rate the instructor who lectures as more effective—and you’ll be more likely to say you wish all your physics courses were taught that way. Upon reflection, the appeal of dynamic lectures shouldn’t be surprising. For generations, people have admired the rhetorical eloquence of poets like Maya Angelou, politicians like John F. Kennedy Jr. and Ronald Reagan, preachers like Martin Luther King Jr., and teachers like Richard Feynman. Today we live in a golden age of spellbinding speaking, where great orators engage and educate from platforms with unprecedented reach. Creatives used to share their methods in small communities; now they can accumulate enough YouTube and Instagram subscribers to populate a small country. Pastors once gave sermons to hundreds at church; now they can reach hundreds of thousands over the internet in megachurches. Professors used to teach small enough classes that they could spend individual time with each student; now their lessons can be broadcast to millions through online courses. It’s clear that these lectures are entertaining and informative. The question is whether they’re the ideal method of teaching. In the physics experiment, the students took tests to gauge how much they had learned about statics and fluids. Despite enjoying the lectures more, they actually gained more knowledge and skill from the active-learning session. It required more mental effort, which made it less fun but led to deeper understanding. For a long time, I believed that we learn more when we’re having fun. This research convinced me I was wrong. It also reminded me of my favorite physics teacher, who got stellar reviews for letting us play Ping-Pong in class, but didn’t quite make the coefficient of friction stick. Active learning has impact far beyond physics. A meta-analysis compared the effects of lecturing and active learning on students’ mastery of the material, cumulating 225 studies with over 46,000 undergraduates in science, technology, engineering, and math (STEM). Active-learning methods included group problem solving, worksheets, and tutorials. On average, students scored half a letter grade worse under traditional lecturing than through active learning—and students were 1.55 times more likely to fail in classes with traditional lecturing. The researchers estimate that if the students who failed in lecture courses had participated in active learning, more than $3.5 million in tuition could have been saved. It’s not hard to see why a boring lecture would fail, but even captivating lectures can fall short for a less obvious, more concerning reason. Lectures aren’t designed to accommodate dialogue or disagreement; they turn students into passive receivers of information rather than active thinker","date":"2022-09-03","objectID":"/b90_think_again/:26:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE UNBEARABLE LIGHTNESS OF REPEATING There’s only one class I regret missing in college. It was taught by a philosopher named Robert Nozick. One of his ideas became famous thanks to the movie The Matrix: in the 1970s, Nozick introduced a thought experiment about whether people would choose to enter an “experience machine” that could provide infinite pleasure but remove them from real life.* In his classroom, Nozick created his own version of an experience machine: he insisted on teaching a new class every year. “I do my thinking through the courses I give,” he said. Nozick taught one course on truth; another on philosophy and neuroscience; a third on Socrates, Buddha, and Jesus; a fourth on thinking about thinking; and a fifth on the Russian Revolution. In four decades of teaching, he taught only one class a second time: it was on the good life. “Presenting a completely polished and worked-out view doesn’t give students a feel for what it’s like to do original work in philosophy and to see it happen, to catch on to doing it,” he explained. Sadly, before I could take one of his courses, he died of cancer. What I found so inspiring about Nozick’s approach was that he wasn’t content for students to learn from him. He wanted them to learn with him. Every time he tackled a new topic, he would have the opportunity to rethink his existing views on it. He was a remarkable role model for changing up our familiar methods of teaching—and learning. When I started teaching, I wanted to adopt some of his principles. I wasn’t prepared to inflict an entire semester of half-baked ideas on my students, so I set a benchmark: every year I would aim to throw out 20 percent of my class and replace it with new material. If I was doing new thinking every year, we could all start rethinking together. With the other 80 percent of the material, though, I found myself failing. I was teaching a semester-long class on organizational behavior for juniors and seniors. When I introduced evidence, I wasn’t giving them the space to rethink it. After years of wrestling with this problem, it dawned on me that I could create a new assignment to teach rethinking. I assigned students to work in small groups to record their own mini-podcasts or mini–TED talks. Their charge was to question a popular practice, to champion an idea that went against the grain of conventional wisdom, or to challenge principles covered in class. As they started working on the project, I noticed a surprising pattern. The students who struggled the most were the straight-A students—the perfectionists. It turns out that although perfectionists are more likely than their peers to ace school, they don’t perform any better than their colleagues at work. This tracks with evidence that, across a wide range of industries, grades are not a strong predictor of job performance. Achieving excellence in school often requires mastering old ways of thinking. Building an influential career demands new ways of thinking. In a classic study of highly accomplished architects, the most creative ones graduated with a B average. Their straight-A counterparts were so determined to be right that they often failed to take the risk of rethinking the orthodoxy. A similar pattern emerged in a study of students who graduated at the top of their class. “Valedictorians aren’t likely to be the future’s visionaries,” education researcher Karen Arnold explains. “They typically settle into the system instead of shaking it up.” That’s what I saw with my straight-A students: they were terrified of being wrong. To give them a strong incentive to take some risks, I made the assignment worth 20 percent of their final grade. I had changed the rules: now they were being rewarded for rethinking instead of regurgitating. I wasn’t sure if that incentive would work until I reviewed the work of a trio of straight-A students. They gave their mini–TED talk about the problems with TED talks, pointing out the risks of reinforcing short atte","date":"2022-09-03","objectID":"/b90_think_again/:26:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"JACK OF ROUGH DRAFTS, MASTER OF CRAFTS When I asked a handful of education pioneers to name the best teacher of rethinking they’ve ever encountered, I kept hearing the same name: Ron Berger. If you invited Ron over for dinner, he’s the kind of person who would notice that one of your chairs was broken, ask if you had some tools handy, and fix it on the spot. For most of his career, Ron was a public-elementary-school teacher in rural Massachusetts. His nurse, his plumber, and his local firefighters were all former students. During the summers and on weekends, he worked as a carpenter. Ron has devoted his life to teaching students an ethic of excellence. Mastering a craft, in his experience, is about constantly revising our thinking. Hands-on craftsmanship is the foundation for his classroom philosophy. Ron wanted his students to experience the joy of discovery, so he didn’t start by teaching them established knowledge. He began the school year by presenting them with “grapples”—problems to work through in phases. The approach was think-pair-share: the kids started individually, updated their ideas in small groups, and then presented their thoughts to the rest of the class, arriving at solutions together. Instead of introducing existing taxonomies of animals, for example, Ron had them develop their own categories first. Some students classified animals by whether they walked on land, swam in water, or flew through the air; others arranged them according to color, size, or diet. The lesson was that scientists always have many options, and their frameworks are useful in some ways but arbitrary in others. When students confront complex problems, they often feel confused. A teacher’s natural impulse is to rescue them as quickly as possible so they don’t feel lost or incompetent. Yet psychologists find that one of the hallmarks of an open mind is responding to confusion with curiosity and interest. One student put it eloquently: “I need time for my confusion.” Confusion can be a cue that there’s new territory to be explored or a fresh puzzle to be solved. Ron wasn’t content to deliver lessons that erased confusion. He wanted students to embrace confusion. His vision was for them to become leaders of their own learning, much like they would in “do it yourself” (DIY) craft projects. He started encouraging students to think like young scientists: they would identify problems, develop hypotheses, and design their own experiments to test them. His sixth graders went around the community to test local homes for radon gas. His third graders created their own maps of amphibian habitats. His first graders got their own group of snails to take care of, and went on to test which of over 140 foods they liked—and whether they preferred hot or cold, dark or light, and wet or dry environments. For architecture and engineering lessons, Ron had his students create blueprints for a house. When he required them to do at least four different drafts, other teachers warned him that younger students would become discouraged. Ron disagreed—he had already tested the concept with kindergarteners and first graders in art. Rather than asking them to simply draw a house, he announced, “We’ll be doing four different versions of a drawing of a house.” Some students didn’t stop there; many wound up deciding to do eight or ten drafts. The students had a support network of classmates cheering them on in their efforts. “Quality means rethinking, reworking, and polishing,” Ron reflects. “They need to feel they will be celebrated, not ridiculed, for going back to the drawing board. . . . They soon began complaining if I didn’t allow them to do more than one version.” Ron wanted to teach his students to revise their thinking based on input from others, so he turned the classroom into a challenge network. Every week—and sometimes every day—the entire class would do a critique session. One format was a gallery critique: Ron put everyone’s work on display, sent students aro","date":"2022-09-03","objectID":"/b90_think_again/:26:4","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 10 ","date":"2022-09-03","objectID":"/b90_think_again/:27:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"That’s Not the Way We’ve Always Done It ","date":"2022-09-03","objectID":"/b90_think_again/:28:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Building Cultures of Learning at Work If only it weren’t for the people . . . earth would be an engineer’s paradise. —kurt vonnegut As an avid scuba diver, Luca Parmitano was familiar with the risks of drowning. He just didn’t realize it could happen in outer space. Luca had just become the youngest astronaut ever to take a long trip to the International Space Station. In July 2013, the thirty-six-year-old Italian astronaut completed his first spacewalk, spending six hours running experiments, moving equipment, and setting up power and data cables. Now, a week later, Luca and another astronaut, Chris Cassidy, were heading out for a second walk to continue their work and do some maintenance. As they prepared to leave the airlock, they could see the Earth 250 miles below. After forty-four minutes in space, Luca felt something strange: the back of his head seemed to be wet. He wasn’t sure where the water was coming from. It wasn’t just a nuisance; it could cut off communication by shorting out his microphone or earphones. He reported the problem to Mission Control in Houston, and Chris asked if he was sweating. “I am sweating,” Luca said, “but it feels like a lot of water. It’s not going anywhere, it’s just in my Snoopy cap. Just FYI.” He went back to work. The officer in charge of spacewalks, Karina Eversley, knew something was wrong. That’s not normal, she thought, and quickly recruited a team of experts to compile questions for Luca. Was the amount of liquid increasing? Luca couldn’t tell. Was he sure it was water? When he stuck out his tongue to capture a few of the drops that were floating in his helmet, the taste was metallic. Mission Control made the call to terminate the spacewalk early. Luca and Chris had to split up to follow their tethers, which were routed in opposite directions. To get around an antenna, Luca flipped over. Suddenly, he couldn’t see clearly or breathe through his nose—globs of water were covering his eyes and filling his nostrils. The water was continuing to accumulate, and if it reached his mouth he could drown. His only hope was to navigate quickly back to the airlock. As the sun set, Luca was surrounded by darkness, with only a small headlight to guide him. Then his comms went down, too—he couldn’t hear himself or anyone else speak. Luca managed to find his way back to the outer hatch of the airlock, using his memory and the tension in his tether. He was still in grave danger: before he could remove his helmet, he would have to wait for Chris to close the hatch and repressurize the airlock. For several agonizing minutes of silence, it was unclear whether he would survive. When it was finally safe to remove his helmet, a quart and a half of water was in it, but Luca was alive. Months later, the incident would be called the “scariest wardrobe malfunction in NASA history.” The technical updates followed swiftly. The spacesuit engineers traced the leak to a fan/pump/separator, which they replaced moving forward. They also added a breathing tube that works like a snorkel and a pad to absorb water inside the helmet. Yet the biggest error wasn’t technical—it was human. When Luca had returned from his first spacewalk a week earlier, he had noticed some droplets of water in his helmet. He and Chris assumed they were the result of a leak in the bag that provided drinking water in his suit, and the crew in Houston agreed. Just to be safe, they replaced the bag, but that was the end of the discussion. The space station chief engineer, Chris Hansen, led the eventual investigation into what had gone wrong with Luca’s suit. “The occurrence of minor amounts of water in the helmet was normalized,” Chris told me. In the space station community, the “perception was that drink bags leak, which led to an acceptance that it was a likely explanation without digging deeper into it.” Luca’s scare wasn’t the first time that NASA’s failure at rethinking had proven disastrous. In 1986, the space shuttle Challenger exploded af","date":"2022-09-03","objectID":"/b90_think_again/:29:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"I ERR, THEREFORE I LEARN Years ago, an engineer turned management professor named Amy Edmondson became interested in preventing medical errors. She went into a hospital and surveyed its staff about the degree of psychological safety they experienced in their teams—could they take risks without the fear of being punished? Then she collected data on the number of medical errors each team made, tracking serious outcomes like potentially fatal doses of the wrong medication. She was surprised to find that the more psychological safety a team felt, the higher its error rates. It appeared that psychological safety could breed complacency. When trust runs deep in a team, people might not feel the need to question their colleagues or double-check their own work. But Edmondson soon recognized a major limitation of the data: the errors were all self-reported. To get an unbiased measure of mistakes, she sent a covert observer into the units. When she analyzed those data, the results flipped: psychologically safe teams reported more errors, but they actually made fewer errors. By freely admitting their mistakes, they were then able to learn what had caused them and eliminate them moving forward. In psychologically unsafe teams, people hid their mishaps to avoid penalties, which made it difficult for anyone to diagnose the root causes and prevent future problems. They kept repeating the same mistakes. Since then, research on psychological safety has flourished. When I was involved in a study at Google to identify the factors that distinguish teams with high performance and well-being, the most important differentiator wasn’t who was on the team or even how meaningful their work was. What mattered most was psychological safety. Over the past few years, psychological safety has become a buzzword in many workplaces. Although leaders might understand its significance, they often misunderstand exactly what it is and how to create it. Edmondson is quick to point out that psychological safety is not a matter of relaxing standards, making people comfortable, being nice and agreeable, or giving unconditional praise. It’s fostering a climate of respect, trust, and openness in which people can raise concerns and suggestions without fear of reprisal. It’s the foundation of a learning culture. In performance cultures, the emphasis on results often undermines psychological safety. When we see people get punished for failures and mistakes, we become worried about proving our competence and protecting our careers. We learn to engage in self-limiting behavior, biting our tongues rather than voicing questions and concerns. Sometimes that’s due to power distance: we’re afraid of challenging the big boss at the top. The pressure to conform to authority is real, and those who dare to deviate run the risk of backlash. In performance cultures, we also censor ourselves in the presence of experts who seem to know all the answers—especially if we lack confidence in our own expertise. A lack of psychological safety was a persistent problem at NASA. Before the Challenger launch, some engineers did raise red flags but were silenced by managers; others were ignored and ended up silencing themselves. After the Columbia launch, an engineer asked for clearer photographs to inspect the damage to the wing, but managers didn’t supply them. In a critical meeting to evaluate the condition of the shuttle after takeoff, the engineer didn’t speak up. About a month before that Columbia launch, Ellen Ochoa became the deputy director of flight crew operations. In 1993, Ellen had made history by becoming the first Latina in space. Now, the first flight she supported in a management role had ended in tragedy. After breaking the news to the space station crew and consoling the family members of the fallen astronauts, she was determined to figure out how she could personally help to prevent this kind of disaster from ever happening again. Ellen recognized that at NASA, the performance cu","date":"2022-09-03","objectID":"/b90_think_again/:29:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"SAFE AT HOME GATES When I first arrived at the Gates Foundation, people were whispering about the annual strategy reviews. It’s the time when program teams across the foundation meet with the cochairs—Bill and Melinda Gates—and the CEO to give progress reports on execution and collect feedback. Although the foundation employs some of the world’s leading experts in areas ranging from eradicating disease to promoting educational equity, these experts are often intimidated by Bill’s knowledge base, which seems impossibly broad and deep. What if he spots a fatal flaw in my work? Will it be the end of my career here? A few years ago, leaders at the Gates Foundation reached out to see if I could help them build psychological safety. They were worried that the pressure to present airtight analyses was discouraging people from taking risks. They often stuck to tried-and-true strategies that would make incremental progress rather than daring to undertake bold experiments that might make a bigger dent in some of the world’s most vexing problems. The existing evidence on creating psychological safety gave us some starting points. I knew that changing the culture of an entire organization is daunting, while changing the culture of a team is more feasible. It starts with modeling the values we want to promote, identifying and praising others who exemplify them, and building a coalition of colleagues who are committed to making the change. The standard advice for managers on building psychological safety is to model openness and inclusiveness. Ask for feedback on how you can improve, and people will feel safe to take risks. To test whether that recommendation would work, I launched an experiment with a doctoral student, Constantinos Coutifaris. In multiple companies, we randomly assigned some managers to ask their teams for constructive criticism. Over the following week, their teams reported higher psychological safety, but as we anticipated, it didn’t last. Some managers who asked for feedback didn’t like what they heard and got defensive. Others found the feedback useless or felt helpless to act on it, which discouraged them from continuing to seek feedback and their teams from continuing to offer it. Another group of managers took a different approach, one that had less immediate impact in the first week but led to sustainable gains in psychological safety a full year later. Instead of asking them to seek feedback, we had randomly assigned those managers to share their past experiences with receiving feedback and their future development goals. We advised them to tell their teams about a time when they benefited from constructive criticism and to identify the areas that they were working to improve now. By admitting some of their imperfections out loud, managers demonstrated that they could take it—and made a public commitment to remain open to feedback. They normalized vulnerability, making their teams more comfortable opening up about their own struggles. Their employees gave more useful feedback because they knew where their managers were working to grow. That motivated managers to create practices to keep the door open: they started holding “ask me anything” coffee chats, opening weekly one-on-one meetings by asking for constructive criticism, and setting up monthly team sessions where everyone shared their development goals and progress. Creating psychological safety can’t be an isolated episode or a task to check off on a to-do list. When discussing their weaknesses, many of the managers in our experiment felt awkward and anxious at first. Many of their team members were surprised by that vulnerability and unsure of how to respond. Some were skeptical: they thought their managers might be fishing for compliments or cherry-picking comments that made them look good. It was only over time—as managers repeatedly demonstrated humility and curiosity—that the dynamic changed. At the Gates Foundation, I wanted to go a step further. Instea","date":"2022-09-03","objectID":"/b90_think_again/:29:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"THE WORST THING ABOUT BEST PRACTICES In performance cultures, people often become attached to best practices. The risk is that once we’ve declared a routine the best, it becomes frozen in time. We preach about its virtues and stop questioning its vices, no longer curious about where it’s imperfect and where it could improve. Organizational learning should be an ongoing activity, but best practices imply it has reached an endpoint. We might be better off looking for better practices. At NASA, although teams routinely debriefed after both training simulations and significant operational events, what sometimes stood in the way of exploring better practices was a performance culture that held people accountable for outcomes. Every time they delayed a scheduled launch, they faced widespread public criticism and threats to funding. Each time they celebrated a flight that made it into orbit, they were encouraging their engineers to focus on the fact that the launch resulted in a success rather than on the faulty processes that could jeopardize future launches. That left NASA rewarding luck and repeating problematic practices, failing to rethink what qualified as an acceptable risk. It wasn’t for a lack of ability. After all, these were rocket scientists. As Ellen Ochoa observes, “When you are dealing with people’s lives hanging in the balance, you rely on following the procedures you already have. This can be the best approach in a time-critical situation, but it’s problematic if it prevents a thorough assessment in the aftermath.” Focusing on results might be good for short-term performance, but it can be an obstacle to long-term learning. Sure enough, social scientists find that when people are held accountable only for whether the outcome was a success or failure, they are more likely to continue with ill-fated courses of action. Exclusively praising and rewarding results is dangerous because it breeds overconfidence in poor strategies, incentivizing people to keep doing things the way they’ve always done them. It isn’t until a high-stakes decision goes horribly wrong that people pause to reexamine their practices. We shouldn’t have to wait until a space shuttle explodes or an astronaut nearly drowns to determine whether a decision was successful. Along with outcome accountability, we can create process accountability by evaluating how carefully different options are considered as people make decisions. A bad decision process is based on shallow thinking. A good process is grounded in deep thinking and rethinking, enabling people to form and express independent opinions. Research shows that when we have to explain the procedures behind our decisions in real time, we think more critically and process the possibilities more thoroughly. Process accountability might sound like the opposite of psychological safety, but they’re actually independent. Amy Edmondson finds that when psychological safety exists without accountability, people tend to stay within their comfort zone, and when there’s accountability but not safety, people tend to stay silent in an anxiety zone. When we combine the two, we create a learning zone. People feel free to experiment—and to poke holes in one another’s experiments in service of making them better. They become a challenge network. One of the most effective steps toward process accountability that I’ve seen is at Amazon, where important decisions aren’t made based on simple PowerPoint presentations. They’re informed by a six-page memo that lays out a problem, the different approaches that have been considered in the past, and how the proposed solutions serve the customer. At the start of the meeting, to avoid groupthink, everyone reads the memo silently. This isn’t practical in every situation, but it’s paramount when choices are both consequential and irreversible. Long before the results of the decision are known, the quality of the process can be evaluated based on the rigor and creativity of the author","date":"2022-09-03","objectID":"/b90_think_again/:29:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"CHAPTER 11 ","date":"2022-09-03","objectID":"/b90_think_again/:30:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Escaping Tunnel Vision ","date":"2022-09-03","objectID":"/b90_think_again/:31:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"Reconsidering Our Best-Laid Career and Life Plans A malaise set in within a couple hours of my arriving. I thought getting a job might help. It turns out I have a lot of relatives in Hell, and, using connections, I became the assistant to a demon who pulls people’s teeth out. It wasn’t actually a job, more of an internship. But I was eager. And at first it was kind of interesting. After a while, though, you start asking yourself: Is this what I came to Hell for, to hand different kinds of pliers to a demon? —jack handey W**hat do you want to be when you grow up? As a kid, that was my least favorite question. I dreaded conversations with adults because they always asked it—and no matter how I replied, they never liked my answer. When I said I wanted to be a superhero, they laughed. My next goal was to make the NBA, but despite countless hours of shooting hoops on my driveway, I was cut from middle school basketball tryouts three years in a row. I was clearly aiming too high. In high school, I became obsessed with springboard diving and decided I wanted to become a diving coach. Adults scoffed at that plan: they told me I was aiming too low. In my first semester of college, I decided to major in psychology, but that didn’t open any doors—it just gave me a few to close. I knew I didn’t want to be a therapist (not patient enough) or a psychiatrist (too squeamish for med school). I was still aimless, and I envied people who had a clear career plan. From the time he was in kindergarten, my cousin Ryan knew exactly what he wanted to be when he grew up. Becoming a doctor wasn’t just the American dream—it was the family dream. Our great-grandparents emigrated from Russia and barely scraped by. Our grandmother was a secretary, and our grandfather worked in a factory, but it wasn’t enough to support five children, so he worked a second job delivering milk. Before his kids were teenagers, he had taught them to drive the milk truck so they could finish their 4:00 a.m. delivery cycle before the school day and workday started. When none of their children went on to med school (or milk delivery), my grandparents hoped our generation would bring the prestige of a Dr. Grant to the family. The first seven grandchildren didn’t become doctors. I was the eighth, and I worked multiple jobs to pay for college and to keep my options open. They were proud when I ended up getting my doctorate in psychology, but they still hoped for a real doctor. For the ninth grandchild, Ryan, who arrived four years after me, an M.D. was practically preordained. Ryan checked all the right boxes: along with being precocious, he had a strong work ethic. He set his sights on becoming a neurosurgeon. He was passionate about the potential to help people and ready to persist in the face of whatever obstacles would come into his path. When Ryan was looking at colleges, he came to visit me. As we started talking about majors, he expressed a flicker of doubt about the premed track and asked if he should study economics instead. There’s a term in psychology that captures Ryan’s personality: blirtatiousness. Yep, that’s an actual research concept, derived from the combination of blurting and flirting. When “blirters” meet people, their responses tend to be fast and effusive. They typically score high in extraversion and impulsiveness—and low in shyness and neuroticism. Ryan could push himself to study for long hours, but it drained him. Drawn to something more active and social, he toyed with the idea of squeezing in an economics major along with premed, but abandoned that idea when he got to college. Gotta stay on track. Ryan sailed through the premed curriculum and became a teaching assistant for undergrads while he was still an undergrad himself. When he showed up at exam review sessions and saw how stressed the students were, he refused to start covering the material until they stood up and danced. When he was accepted to an Ivy League medical school, he asked me if he should","date":"2022-09-03","objectID":"/b90_think_again/:32:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"GOING INTO FORECLOSURE When we dedicate ourselves to a plan and it isn’t going as we hoped, our first instinct isn’t usually to rethink it. Instead, we tend to double down and sink more resources in the plan. This pattern is called escalation of commitment. Evidence shows that entrepreneurs persist with failing strategies when they should pivot, NBA general managers and coaches keep investing in new contracts and more playing time for draft busts, and politicians continue sending soldiers to wars that didn’t need to be fought in the first place. Sunk costs are a factor, but the most important causes appear to be psychological rather than economic. Escalation of commitment happens because we’re rationalizing creatures, constantly searching for self-justifications for our prior beliefs as a way to soothe our egos, shield our images, and validate our past decisions. Escalation of commitment is a major factor in preventable failures. Ironically, it can be fueled by one of the most celebrated engines of success: grit. Grit is the combination of passion and perseverance, and research shows that it can play an important role in motivating us to accomplish long-term goals. When it comes to rethinking, though, grit may have a dark side. Experiments show that gritty people are more likely to overplay their hands in roulette and more willing to stay the course in tasks at which they’re failing and success is impossible. Researchers have even suggested that gritty mountaineers are more likely to die on expeditions, because they’re determined to do whatever it takes to reach the summit. There’s a fine line between heroic persistence and foolish stubbornness. Sometimes the best kind of grit is gritting our teeth and turning around. Ryan escalated his commitment to medical training for sixteen years. If he had been less tenacious, he might have changed tracks sooner. Early on, he had fallen victim to what psychologists call identity foreclosure—when we settle prematurely on a sense of self without enough due diligence, and close our minds to alternative selves. In career choices, identity foreclosure often begins when adults ask kids: what do you want to be when you grow up? Pondering that question can foster a fixed mindset about work and self. “I think it’s one of the most useless questions an adult can ask a child,” Michelle Obama writes. “What do you want to be when you grow up? As if growing up is finite. As if at some point you become something and that’s the end.”* Some kids dream too small. They foreclose on following in family footsteps and never really consider alternatives. You probably know some people who faced the opposite problem. They dreamed too big, becoming attached to a lofty vision that wasn’t realistic. Sometimes we lack the talent to pursue our callings professionally, leaving them unanswered; other times there’s little hope that our passions can pay the bills. “You can be anything you wanna be?!” the comedian Chris Rock quipped. “Tell the kids the truth. . . . You can be anything you’re good at . . . as long as they’re hiring.” Even if kids get excited about a career path that does prove realistic, what they thought was their dream job can turn out to be a nightmare. Kids might be better off learning about careers as actions to take rather than as identities to claim. When they see work as what they do rather than who they are, they become more open to exploring different possibilities. Although children are often fascinated by science from a young age, over the course of elementary school, they tend to lose interest and confidence in their potential to be scientists. Recent studies show that it’s possible to maintain their enthusiasm by introducing them to science differently. When second and third graders learned about “doing science” rather than “being scientists,” they were more excited about pursuing science. Becoming a scientist might seem out of reach, but the act of experimenting is something we can all try ou","date":"2022-09-03","objectID":"/b90_think_again/:32:1","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"TIME FOR A CHECKUP We foreclose on all kinds of life plans. Once you’ve committed to one, it becomes part of your identity, making it difficult to de-escalate. Declaring an English major because you love to read, only to discover that you don’t enjoy the process of writing. Deciding to start college during a pandemic, only to conclude later that you should have considered a gap year. Gotta stay on track. Ending a romantic relationship because you don’t want kids, only to realize years down the road that you might after all. Identity foreclosure can stop us from evolving. In a study of amateur musicians, those who had settled on music as a professional calling were more likely to ignore career advice from a trusted adviser over the course of the following seven years. They listened to their hearts and tuned out their mentors. In some ways, identity foreclosure is the opposite of an identity crisis: instead of accepting uncertainty about who we want to become, we develop compensatory conviction and plunge head over heels into a career path. I’ve noticed that the students who are the most certain about their career plans at twenty are often the ones with the deepest regrets by thirty. They haven’t done enough rethinking along the way.* Sometimes it’s because they’re thinking too much like politicians, eager to earn the approval of parents and peers. They become seduced by status, failing to see that no matter how much an accomplishment or affiliation impresses someone else, it’s still a poor choice if it depresses them. In other cases it’s because they’re stuck in preacher mode, and they’ve come to see a job as a sacred cause. And occasionally they pick careers in prosecutor mode, where they charge classmates with selling their souls to capitalism and hurl themselves into nonprofits in the hopes of saving the world. Sadly, they often know too little about the job—and too little about their evolving selves—to make a lifelong commitment. They get trapped in an overconfidence cycle, taking pride in pursuing a career identity and surrounding themselves with people who validate their conviction. By the time they discover it was the wrong fit, they feel it’s too late to think again. The stakes seem too high to walk away; the sacrifices of salary, status, skill, and time seem too great. For the record, I think it’s better to lose the past two years of progress than to waste the next twenty. In hindsight, identity foreclosure is a Band-Aid: it covers up an identity crisis, but fails to cure it. My advice to students is to take a cue from health-care professions. Just as they make appointments with the doctor and the dentist even when nothing is wrong, they should schedule checkups on their careers. I encourage them to put a reminder in their calendars to ask some key questions twice a year. When did you form the aspirations you’re currently pursuing, and how have you changed since then? Have you reached a learning plateau in your role or your workplace, and is it time to consider a pivot? Answering these career checkup questions is a way to periodically activate rethinking cycles. It helps students maintain humility about their ability to predict the future, contemplate doubts about their plans, and stay curious enough to discover new possibilities or reconsider previously discarded ones. I had one student, Marissa Shandell, who scored a coveted job at a prestigious consulting firm and planned on climbing up the ladder. She kept getting promoted early but found herself working around the clock. Instead of continuing to just grit and bear it, she and her husband had a career checkup conversation every six months, talking not just about the growth trajectory of their companies but also about the growth trajectory of their jobs. After being promoted to associate partner well ahead of schedule, Marissa realized she had reached a learning plateau (and a lifestyle plateau) and decided to pursue a doctorate in management.* Deciding to leave a c","date":"2022-09-03","objectID":"/b90_think_again/:32:2","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"WHEN CHASING HAPPINESS CHASES IT AWAY When we think about how to plan our lives, there are few things that take priority over happiness. The kingdom of Bhutan has a Gross National Happiness index. In the United States, the pursuit of happiness is so prized that it’s one of the three unalienable rights in our Declaration of Independence. If we’re not careful, though, the pursuit of happiness can become a recipe for misery. Psychologists find that the more people value happiness, the less happy they often become with their lives. It’s true for people who naturally care about happiness and for people who are randomly assigned to reflect on why happiness matters. There’s even evidence that placing a great deal of importance on happiness is a risk factor for depression. Why? One possibility is that when we’re searching for happiness, we get too busy evaluating life to actually experience it. Instead of savoring our moments of joy, we ruminate about why our lives aren’t more joyful. A second likely culprit is that we spend too much time striving for peak happiness, overlooking the fact that happiness depends more on the frequency of positive emotions than their intensity. A third potential factor is that when we hunt for happiness, we overemphasize pleasure at the expense of purpose. This theory is consistent with data suggesting that meaning is healthier than happiness, and that people who look for purpose in their work are more successful in pursuing their passions—and less likely to quit their jobs—than those who look for joy. While enjoyment waxes and wanes, meaning tends to last. A fourth explanation is that Western conceptions of happiness as an individual state leave us feeling lonely. In more collectivistic Eastern cultures, that pattern is reversed: pursuing happiness predicts higher well-being, because people prioritize social engagement over independent activities. Last fall a student stopped by my office hours for some advice. She explained that when she chose Wharton, she had focused too much on getting into the best school and too little on finding the best fit. She wished she had picked a college with a more carefree culture and a stronger sense of community. Now that she was clear on her values, she was considering a transfer to a school that would make her happier. A few weeks later she told me that a moment in class had helped her rethink her plan. It wasn’t the research on happiness that we discussed, the values survey she took, or the decision-making activity we did. It was a comedy sketch I showed from Saturday Night Live. The scene stars Adam Sandler as a tour guide. In a mock commercial advertising his company’s Italian tours, he mentions that customer reviews sometimes express disappointment. He takes the opportunity to remind customers about what a vacation can and can’t do for them: There’s a lot a vacation can do: help you unwind, see some different-looking squirrels, but it cannot fix deeper issues, like how you behave in group settings. We can take you on a hike. We cannot turn you into someone who likes hiking. Remember, you’re still gonna be you on vacation. If you are sad where you are, and then you get on a plane to Italy, the you in Italy will be the same sad you from before, just in a new place. © Saturday Night Live/NBC When we pursue happiness, we often start by changing our surroundings. We expect to find bliss in a warmer climate or a friendlier dorm, but any joy that those choices bring about is typically temporary. In a series of studies, students who changed their environments by adjusting their living arrangements or course schedules quickly returned to their baseline levels of happiness. As Ernest Hemingway wrote, “You can’t get away from yourself by moving from one place to another.” Meanwhile, students who changed their actions by joining a new club, adjusting their study habits, or starting a new project experienced lasting gains in happiness. Our happiness often depends more on what we","date":"2022-09-03","objectID":"/b90_think_again/:32:3","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"LIFE, LIBERTY, AND THE PURSUIT OF MEANING To be clear, I wouldn’t encourage anyone to stay in a role, relationship, or place they hated unless they had no other alternatives. Still, when it comes to careers, instead of searching for the job where we’ll be happiest, we might be better off pursuing the job where we expect to learn and contribute the most. Psychologists find that passions are often developed, not discovered. In a study of entrepreneurs, the more effort they put into their startups, the more their enthusiasm about their businesses climbed each week. Their passion grew as they gained momentum and mastery. Interest doesn’t always lead to effort and skill; sometimes it follows them. By investing in learning and problem solving, we can develop our passions—and build the skills necessary to do the work and lead the lives we find worthwhile. As we get older, we become more focused on searching for meaning—and we’re most likely to find it in actions that benefit others. My favorite test of meaningful work is to ask: if this job didn’t exist, how much worse off would people be? It’s near midlife that this question often begins to loom large. At around this time, in both work and life, we feel we have more to give (and less to lose), and we’re especially keen to share our knowledge and skills with the next generation. When my students talk about the evolution of self-esteem in their careers, the progression often goes something like this: Phase 1: I’m not important Phase 2: I’m important Phase 3: I want to contribute to something important I’ve noticed that the sooner they get to phase 3, the more impact they have and the more happiness they experience. It’s left me thinking about happiness less as a goal and more as a by-product of mastery and meaning. “Those only are happy,” philosopher John Stuart Mill wrote, “who have their minds fixed on some object other than their own happiness; on the happiness of others, on the improvement of mankind, even on some art or pursuit, followed not as a means, but as itself an ideal end. Aiming thus at something else, they find happiness by the way.” Careers, relationships, and communities are examples of what scientists call open systems—they’re constantly in flux because they’re not closed off from the environments around them. We know that open systems are governed by at least two key principles: there are always multiple paths to the same end (equifinality), and the same starting point can be a path to many different ends (multifinality). We should be careful to avoid getting too attached to a particular route or even a particular destination. There isn’t one definition of success or one track to happiness. My cousin Ryan finally wound up rethinking his career arc. Five years into his neurosurgery residency, he did his own version of a career checkup and decided to scratch his entrepreneurial itch. He cofounded a fast-growing, venture-backed startup called Nomad Health, which creates a marketplace to match clinicians with medical facilities. He also advised several medical device startups, filed medical device patents, and is now working on multiple startups to improve health care. Looking back, he still regrets that he foreclosed so early on an identity as a neurosurgeon and escalated his commitment to that career. At work and in life, the best we can do is plan for what we want to learn and contribute over the next year or two, and stay open to what might come next. To adapt an analogy from E. L. Doctorow, writing out a plan for your life “is like driving at night in the fog. You can only see as far as your headlights, but you can make the whole trip that way.” we don’t have to upend our entire paths to rethink some of our plans. Some people are perfectly content with their fields of work but dissatisfied with their current roles. Others may be too risk averse to make a geographic move for a job or a partner. And many don’t have the luxury of making a pivot: being economically dep","date":"2022-09-03","objectID":"/b90_think_again/:32:4","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"I. INDIVIDUAL RETHINKING A. Develop the Habit of Thinking Again \\1. Think like a scientist. When you start forming an opinion, resist the temptation to preach, prosecute, or politick. Treat your emerging view as a hunch or a hypothesis and test it with data. Like the entrepreneurs who learned to approach their business strategies as experiments, you’ll maintain the agility to pivot. \\2. Define your identity in terms of values, not opinions. It’s easier to avoid getting stuck to your past beliefs if you don’t become attached to them as part of your present self-concept. See yourself as someone who values curiosity, learning, mental flexibility, and searching for knowledge. As you form opinions, keep a list of factors that would change your mind. \\3. Seek out information that goes against your views. You can fight confirmation bias, burst filter bubbles, and escape echo chambers by actively engaging with ideas that challenge your assumptions. An easy place to start is to follow people who make you think—even if you usually disagree with what they think. B. Calibrate Your Confidence \\4. Beware of getting stranded at the summit of Mount Stupid. Don’t confuse confidence with competence. The Dunning-Kruger effect is a good reminder that the better you think you are, the greater the risk that you’re overestimating yourself—and the greater the odds that you’ll stop improving. To prevent overconfidence in your knowledge, reflect on how well you can explain a given subject. \\5. Harness the benefits of doubt. When you find yourself doubting your ability, reframe the situation as an opportunity for growth. You can have confidence in your capacity to learn while questioning your current solution to a problem. Knowing what you don’t know is often the first step toward developing expertise. \\6. Embrace the joy of being wrong. When you find out you’ve made a mistake, take it as a sign that you’ve just discovered something new. Don’t be afraid to laugh at yourself. It helps you focus less on proving yourself—and more on improving yourself. C. Invite Others to Question Your Thinking \\7. Learn something new from each person you meet. Everyone knows more than you about something. Ask people what they’ve been rethinking lately, or start a conversation about times you’ve changed your mind in the past year. \\8. Build a challenge network, not just a support network. It’s helpful to have cheerleaders encouraging you, but you also need critics to challenge you. Who are your most thoughtful critics? Once you’ve identified them, invite them to question your thinking. To make sure they know you’re open to dissenting views, tell them why you respect their pushback—and where they usually add the most value. \\9. Don’t shy away from constructive conflict. Disagreements don’t have to be disagreeable. Although relationship conflict is usually counterproductive, task conflict can help you think again. Try framing disagreement as a debate: people are more likely to approach it intellectually and less likely to take it personally. ","date":"2022-09-03","objectID":"/b90_think_again/:33:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"II. INTERPERSONAL RETHINKING A. Ask Better Questions \\10. Practice the art of persuasive listening. When we’re trying to open other people’s minds, we can frequently accomplish more by listening than by talking. How can you show an interest in helping people crystallize their own views and uncover their own reasons for change? A good way to start is to increase your question-to-statement ratio. \\11. Question how rather than why. When people describe why they hold extreme views, they often intensify their commitment and double down. When they try to explain how they would make their views a reality, they often realize the limits of their understanding and start to temper some of their opinions. \\12. Ask “What evidence would change your mind?” You can’t bully someone into agreeing with you. It’s often more effective to inquire about what would open their minds, and then see if you can convince them on their own terms. \\13. Ask how people originally formed an opinion. Many of our opinions, like our stereotypes, are arbitrary; we’ve developed them without rigorous data or deep reflection. To help people reevaluate, prompt them to consider how they’d believe different things if they’d been born at a different time or in a different place. B. Approach Disagreements as Dances, Not Battles \\14. Acknowledge common ground. A debate is like a dance, not a war. Admitting points of convergence doesn’t make you weaker—it shows that you’re willing to negotiate about what’s true, and it motivates the other side to consider your point of view. \\15. Remember that less is often more. If you pile on too many different reasons to support your case, it can make your audiences defensive—and cause them to reject your entire argument based on its least compelling points. Instead of diluting your argument, lead with a few of your strongest points. \\16. Reinforce freedom of choice. Sometimes people resist not because they’re dismissing the argument but because they’re rejecting the feeling of their behavior being controlled. It helps to respect their autonomy by reminding them that it’s up to them to choose what they believe. \\17. Have a conversation about the conversation. If emotions are running hot, try redirecting the discussion to the process. Like the expert negotiators who comment on their feelings and test their understanding of the other side’s feelings, you can sometimes make progress by expressing your disappointment or frustration and asking people if they share it. ","date":"2022-09-03","objectID":"/b90_think_again/:34:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"},{"categories":["think"],"content":"III. COLLECTIVE RETHINKING A. Have More Nuanced Conversations \\18. Complexify contentious topics. There are more than two sides to every story. Instead of treating polarizing issues like two sides of a coin, look at them through the many lenses of a prism. Seeing the shades of gray can make us more open. \\19. Don’t shy away from caveats and contingencies. Acknowledging competing claims and conflicting results doesn’t sacrifice interest or credibility. It’s an effective way to engage audiences while encouraging them to stay curious. \\20. Expand your emotional range. You don’t have to eliminate frustration or even indignation to have a productive conversation. You just need to mix in a broader set of emotions along with them—you might try showing some curiosity or even admitting confusion or ambivalence. B. Teach Kids to Think Again \\21. Have a weekly myth-busting discussion at dinner. It’s easier to debunk false beliefs at an early age, and it’s a great way to teach kids to become comfortable with rethinking. Pick a different topic each week—one day it might be dinosaurs, the next it could be outer space—and rotate responsibility around the family for bringing a myth for discussion. \\22. Invite kids to do multiple drafts and seek feedback from others. Creating different versions of a drawing or a story can encourage kids to learn the value of revising their ideas. Getting input from others can also help them to continue evolving their standards. They might learn to embrace confusion—and to stop expecting perfection on the first try. \\23. Stop asking kids what they want to be when they grow up. They don’t have to define themselves in terms of a career. A single identity can close the door to alternatives. Instead of trying to narrow their options, help them broaden their possibilities. They don’t have to be one thing—they can do many things. C. Create Learning Organizations \\24. Abandon best practices. Best practices suggest that the ideal routines are already in place. If we want people to keep rethinking the way they work, we might be better off adopting process accountability and continually striving for better practices. \\25. Establish psychological safety. In learning cultures, people feel confident that they can question and challenge the status quo without being punished. Psychological safety often starts with leaders role-modeling humility. \\26. Keep a rethinking scorecard. Don’t evaluate decisions based only on the results; track how thoroughly different options are considered in the process. A bad process with a good outcome is luck. A good process with a bad outcome might be a smart experiment. D. Stay Open to Rethinking Your Future \\27. Throw out the ten-year plan. What interested you last year might bore you this year—and what confused you yesterday might become exciting tomorrow. Passions are developed, not just discovered. Planning just one step ahead can keep you open to rethinking. \\28. Rethink your actions, not just your surroundings. Chasing happiness can chase it away. Trading one set of circumstances for another isn’t always enough. Joy can wax and wane, but meaning is more likely to last. Building a sense of purpose often starts with taking actions to enhance your learning or your contribution to others. \\29. Schedule a life checkup. It’s easy to get caught in escalation of commitment to an unfulfilling path. Just as you schedule health checkups with your doctor, it’s worth having a life checkup on your calendar once or twice a year. It’s a way to assess how much you’re learning, how your beliefs and goals are evolving, and whether your next steps warrant some rethinking. \\30. Make time to think again. When I looked at my calendar, I noticed that it was mostly full of doing. I set a goal of spending an hour a day thinking and learning. Now I’ve decided to go further: I’m scheduling a weekly time for rethinking and unlearning. I reach out to my challenge network and ask what ideas and opinions they think I s","date":"2022-09-03","objectID":"/b90_think_again/:35:0","tags":["think"],"title":"Think Again","uri":"/b90_think_again/"}]